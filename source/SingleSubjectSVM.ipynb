{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yecatstevir/teambrainiac/blob/main/source/SingleSubjectSVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_wDm122NTXY"
      },
      "source": [
        "# Whole Brain Support Vector Machine Training\n",
        "- Go to 'Runtime' in Colab browser bar, select 'Change Runtime Type', select 'High-RAM' from 'Runtime Shape'. \n",
        "- load local pickle file containing all masked, normalized Whole Brain subject data in numpy matrix format\n",
        "- SVM training per subject"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c284FIeHNcR2"
      },
      "source": [
        "### Mount Google Drive and clone repository\n",
        "- open to source directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIfcjimGzIZo",
        "outputId": "e4ffb2d2-37b1-4945-f074-54e76689b8ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')#, force_remount = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5eq9qyKMTp_",
        "outputId": "b5fd8451-c051-43c7-c888-f010b96fe2c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lh0DqCU10r3n",
        "outputId": "26bbdcbc-ead4-4ba2-b4ec-d90a1cbb40a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'teambrainiac'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 1387, done.\u001b[K\n",
            "remote: Counting objects: 100% (1387/1387), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1089/1089), done.\u001b[K\n",
            "remote: Total 1387 (delta 895), reused 552 (delta 281), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1387/1387), 84.69 MiB | 11.74 MiB/s, done.\n",
            "Resolving deltas: 100% (895/895), done.\n",
            "/content/teambrainiac/source\n",
            "access_data.py\t\t\t  models\n",
            "AccuracyMeasures.ipynb\t\t  process.py\n",
            "data\t\t\t\t  single_subject.py\n",
            "DataExploration_SingleSubj.ipynb  SingleSubjectSVM.ipynb\n",
            "DataExplorationVisuals.ipynb\t  streamlit\n",
            "DL\t\t\t\t  SubjectVisualization_Models_ZNORM.ipynb\n",
            "group_svm\t\t\t  TestMask.ipynb\n",
            "helper\t\t\t\t  utils.py\n",
            "images\t\t\t\t  VisualizationPlayground.ipynb\n",
            "__init__.py\n"
          ]
        }
      ],
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/yecatstevir/teambrainiac.git\n",
        "# Change directory into cloned repo\n",
        "%cd teambrainiac/source\n",
        "!ls\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zadD_dOPPjQm"
      },
      "source": [
        "### Load path_config.py \n",
        "- we are already in source so we can just load this file without changing directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7CgpmdW5jdGlvbiBfdXBsb2FkRmlsZXMoaW5wdXRJZCwgb3V0cHV0SWQpIHsKICBjb25zdCBzdGVwcyA9IHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCk7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICAvLyBDYWNoZSBzdGVwcyBvbiB0aGUgb3V0cHV0RWxlbWVudCB0byBtYWtlIGl0IGF2YWlsYWJsZSBmb3IgdGhlIG5leHQgY2FsbAogIC8vIHRvIHVwbG9hZEZpbGVzQ29udGludWUgZnJvbSBQeXRob24uCiAgb3V0cHV0RWxlbWVudC5zdGVwcyA9IHN0ZXBzOwoKICByZXR1cm4gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpOwp9CgovLyBUaGlzIGlzIHJvdWdobHkgYW4gYXN5bmMgZ2VuZXJhdG9yIChub3Qgc3VwcG9ydGVkIGluIHRoZSBicm93c2VyIHlldCksCi8vIHdoZXJlIHRoZXJlIGFyZSBtdWx0aXBsZSBhc3luY2hyb25vdXMgc3RlcHMgYW5kIHRoZSBQeXRob24gc2lkZSBpcyBnb2luZwovLyB0byBwb2xsIGZvciBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcC4KLy8gVGhpcyB1c2VzIGEgUHJvbWlzZSB0byBibG9jayB0aGUgcHl0aG9uIHNpZGUgb24gY29tcGxldGlvbiBvZiBlYWNoIHN0ZXAsCi8vIHRoZW4gcGFzc2VzIHRoZSByZXN1bHQgb2YgdGhlIHByZXZpb3VzIHN0ZXAgYXMgdGhlIGlucHV0IHRvIHRoZSBuZXh0IHN0ZXAuCmZ1bmN0aW9uIF91cGxvYWRGaWxlc0NvbnRpbnVlKG91dHB1dElkKSB7CiAgY29uc3Qgb3V0cHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKG91dHB1dElkKTsKICBjb25zdCBzdGVwcyA9IG91dHB1dEVsZW1lbnQuc3RlcHM7CgogIGNvbnN0IG5leHQgPSBzdGVwcy5uZXh0KG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSk7CiAgcmV0dXJuIFByb21pc2UucmVzb2x2ZShuZXh0LnZhbHVlLnByb21pc2UpLnRoZW4oKHZhbHVlKSA9PiB7CiAgICAvLyBDYWNoZSB0aGUgbGFzdCBwcm9taXNlIHZhbHVlIHRvIG1ha2UgaXQgYXZhaWxhYmxlIHRvIHRoZSBuZXh0CiAgICAvLyBzdGVwIG9mIHRoZSBnZW5lcmF0b3IuCiAgICBvdXRwdXRFbGVtZW50Lmxhc3RQcm9taXNlVmFsdWUgPSB2YWx1ZTsKICAgIHJldHVybiBuZXh0LnZhbHVlLnJlc3BvbnNlOwogIH0pOwp9CgovKioKICogR2VuZXJhdG9yIGZ1bmN0aW9uIHdoaWNoIGlzIGNhbGxlZCBiZXR3ZWVuIGVhY2ggYXN5bmMgc3RlcCBvZiB0aGUgdXBsb2FkCiAqIHByb2Nlc3MuCiAqIEBwYXJhbSB7c3RyaW5nfSBpbnB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIGlucHV0IGZpbGUgcGlja2VyIGVsZW1lbnQuCiAqIEBwYXJhbSB7c3RyaW5nfSBvdXRwdXRJZCBFbGVtZW50IElEIG9mIHRoZSBvdXRwdXQgZGlzcGxheS4KICogQHJldHVybiB7IUl0ZXJhYmxlPCFPYmplY3Q+fSBJdGVyYWJsZSBvZiBuZXh0IHN0ZXBzLgogKi8KZnVuY3Rpb24qIHVwbG9hZEZpbGVzU3RlcChpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IGlucHV0RWxlbWVudCA9IGRvY3VtZW50LmdldEVsZW1lbnRCeUlkKGlucHV0SWQpOwogIGlucHV0RWxlbWVudC5kaXNhYmxlZCA9IGZhbHNlOwoKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIG91dHB1dEVsZW1lbnQuaW5uZXJIVE1MID0gJyc7CgogIGNvbnN0IHBpY2tlZFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgaW5wdXRFbGVtZW50LmFkZEV2ZW50TGlzdGVuZXIoJ2NoYW5nZScsIChlKSA9PiB7CiAgICAgIHJlc29sdmUoZS50YXJnZXQuZmlsZXMpOwogICAgfSk7CiAgfSk7CgogIGNvbnN0IGNhbmNlbCA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2J1dHRvbicpOwogIGlucHV0RWxlbWVudC5wYXJlbnRFbGVtZW50LmFwcGVuZENoaWxkKGNhbmNlbCk7CiAgY2FuY2VsLnRleHRDb250ZW50ID0gJ0NhbmNlbCB1cGxvYWQnOwogIGNvbnN0IGNhbmNlbFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgY2FuY2VsLm9uY2xpY2sgPSAoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9OwogIH0pOwoKICAvLyBXYWl0IGZvciB0aGUgdXNlciB0byBwaWNrIHRoZSBmaWxlcy4KICBjb25zdCBmaWxlcyA9IHlpZWxkIHsKICAgIHByb21pc2U6IFByb21pc2UucmFjZShbcGlja2VkUHJvbWlzZSwgY2FuY2VsUHJvbWlzZV0pLAogICAgcmVzcG9uc2U6IHsKICAgICAgYWN0aW9uOiAnc3RhcnRpbmcnLAogICAgfQogIH07CgogIGNhbmNlbC5yZW1vdmUoKTsKCiAgLy8gRGlzYWJsZSB0aGUgaW5wdXQgZWxlbWVudCBzaW5jZSBmdXJ0aGVyIHBpY2tzIGFyZSBub3QgYWxsb3dlZC4KICBpbnB1dEVsZW1lbnQuZGlzYWJsZWQgPSB0cnVlOwoKICBpZiAoIWZpbGVzKSB7CiAgICByZXR1cm4gewogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgICAgfQogICAgfTsKICB9CgogIGZvciAoY29uc3QgZmlsZSBvZiBmaWxlcykgewogICAgY29uc3QgbGkgPSBkb2N1bWVudC5jcmVhdGVFbGVtZW50KCdsaScpOwogICAgbGkuYXBwZW5kKHNwYW4oZmlsZS5uYW1lLCB7Zm9udFdlaWdodDogJ2JvbGQnfSkpOwogICAgbGkuYXBwZW5kKHNwYW4oCiAgICAgICAgYCgke2ZpbGUudHlwZSB8fCAnbi9hJ30pIC0gJHtmaWxlLnNpemV9IGJ5dGVzLCBgICsKICAgICAgICBgbGFzdCBtb2RpZmllZDogJHsKICAgICAgICAgICAgZmlsZS5sYXN0TW9kaWZpZWREYXRlID8gZmlsZS5sYXN0TW9kaWZpZWREYXRlLnRvTG9jYWxlRGF0ZVN0cmluZygpIDoKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgJ24vYSd9IC0gYCkpOwogICAgY29uc3QgcGVyY2VudCA9IHNwYW4oJzAlIGRvbmUnKTsKICAgIGxpLmFwcGVuZENoaWxkKHBlcmNlbnQpOwoKICAgIG91dHB1dEVsZW1lbnQuYXBwZW5kQ2hpbGQobGkpOwoKICAgIGNvbnN0IGZpbGVEYXRhUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICAgIGNvbnN0IHJlYWRlciA9IG5ldyBGaWxlUmVhZGVyKCk7CiAgICAgIHJlYWRlci5vbmxvYWQgPSAoZSkgPT4gewogICAgICAgIHJlc29sdmUoZS50YXJnZXQucmVzdWx0KTsKICAgICAgfTsKICAgICAgcmVhZGVyLnJlYWRBc0FycmF5QnVmZmVyKGZpbGUpOwogICAgfSk7CiAgICAvLyBXYWl0IGZvciB0aGUgZGF0YSB0byBiZSByZWFkeS4KICAgIGxldCBmaWxlRGF0YSA9IHlpZWxkIHsKICAgICAgcHJvbWlzZTogZmlsZURhdGFQcm9taXNlLAogICAgICByZXNwb25zZTogewogICAgICAgIGFjdGlvbjogJ2NvbnRpbnVlJywKICAgICAgfQogICAgfTsKCiAgICAvLyBVc2UgYSBjaHVua2VkIHNlbmRpbmcgdG8gYXZvaWQgbWVzc2FnZSBzaXplIGxpbWl0cy4gU2VlIGIvNjIxMTU2NjAuCiAgICBsZXQgcG9zaXRpb24gPSAwOwogICAgZG8gewogICAgICBjb25zdCBsZW5ndGggPSBNYXRoLm1pbihmaWxlRGF0YS5ieXRlTGVuZ3RoIC0gcG9zaXRpb24sIE1BWF9QQVlMT0FEX1NJWkUpOwogICAgICBjb25zdCBjaHVuayA9IG5ldyBVaW50OEFycmF5KGZpbGVEYXRhLCBwb3NpdGlvbiwgbGVuZ3RoKTsKICAgICAgcG9zaXRpb24gKz0gbGVuZ3RoOwoKICAgICAgY29uc3QgYmFzZTY0ID0gYnRvYShTdHJpbmcuZnJvbUNoYXJDb2RlLmFwcGx5KG51bGwsIGNodW5rKSk7CiAgICAgIHlpZWxkIHsKICAgICAgICByZXNwb25zZTogewogICAgICAgICAgYWN0aW9uOiAnYXBwZW5kJywKICAgICAgICAgIGZpbGU6IGZpbGUubmFtZSwKICAgICAgICAgIGRhdGE6IGJhc2U2NCwKICAgICAgICB9LAogICAgICB9OwoKICAgICAgbGV0IHBlcmNlbnREb25lID0gZmlsZURhdGEuYnl0ZUxlbmd0aCA9PT0gMCA/CiAgICAgICAgICAxMDAgOgogICAgICAgICAgTWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCk7CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPSBgJHtwZXJjZW50RG9uZX0lIGRvbmVgOwoKICAgIH0gd2hpbGUgKHBvc2l0aW9uIDwgZmlsZURhdGEuYnl0ZUxlbmd0aCk7CiAgfQoKICAvLyBBbGwgZG9uZS4KICB5aWVsZCB7CiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICB9CiAgfTsKfQoKc2NvcGUuZ29vZ2xlID0gc2NvcGUuZ29vZ2xlIHx8IHt9OwpzY29wZS5nb29nbGUuY29sYWIgPSBzY29wZS5nb29nbGUuY29sYWIgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYi5fZmlsZXMgPSB7CiAgX3VwbG9hZEZpbGVzLAogIF91cGxvYWRGaWxlc0NvbnRpbnVlLAp9Owp9KShzZWxmKTsK",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "u_2qZqNJPLkn",
        "outputId": "ae23c41f-0c99-47d6-9585-5b6211e51da5"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-848b1f02-aaf5-4a1e-8845-60ad37575e6d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-848b1f02-aaf5-4a1e-8845-60ad37575e6d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving path_config.py to path_config.py\n",
            "User uploaded file \"path_config.py\" with length 228 bytes\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cmYZ22BNj8h"
      },
      "source": [
        "### Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abHr7oUl06ED",
        "outputId": "8fcb3a47-1ad6-4461-fd98-0576e9aca03e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.21.42-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting nilearn\n",
            "  Downloading nilearn-0.9.1-py3-none-any.whl (9.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.6 MB 17.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nibabel in /usr/local/lib/python3.7/dist-packages (3.0.2)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.9 MB/s \n",
            "\u001b[?25hCollecting botocore<1.25.0,>=1.24.42\n",
            "  Downloading botocore-1.24.42-py3-none-any.whl (8.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.7 MB 54.7 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.25.0,>=1.24.42->boto3) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 74.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.42->boto3) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.21.5)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.3.5)\n",
            "Requirement already satisfied: joblib>=0.15 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.1.0)\n",
            "Collecting scipy>=1.5\n",
            "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.1 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from nilearn) (4.2.6)\n",
            "Requirement already satisfied: requests>=2 in /usr/local/lib/python3.7/dist-packages (from nilearn) (2.23.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.0.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->nilearn) (2018.9)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 65.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->nilearn) (3.1.0)\n",
            "Installing collected packages: urllib3, jmespath, scipy, botocore, s3transfer, nilearn, boto3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.21.42 botocore-1.24.42 jmespath-1.0.0 nilearn-0.9.1 s3transfer-0.5.2 scipy-1.7.3 urllib3-1.25.11\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Import libraries\n",
        "!pip install boto3 nilearn nibabel #for saving data and image visualizations\n",
        "import pickle\n",
        "#sklearn packages needed\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, auc, recall_score, precision_score,roc_curve,f1_score\n",
        "#important utility functions for loading,masking,saving data\n",
        "#from utils import *\n",
        "from access_data import *\n",
        "from process import *\n",
        "#normal python packages we use\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import signal\n",
        "from nilearn.signal import clean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZVh3WQeMk1c"
      },
      "source": [
        "### Get paths to subject data and grab labels for SVM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## load and open the pickle file that contains paths to all data.\n",
        "path = \"data/data_path_dictionary.pkl\"\n",
        "data_path_dict = open_pickle(path)\n",
        "\n",
        "##get mask_dictionary\n"
      ],
      "metadata": {
        "id": "l4wpPDZh1U1X"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Functions to get information about data to run our SVM"
      ],
      "metadata": {
        "id": "B4cu86EDfpre"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YwJQ-eCGifbH"
      },
      "outputs": [],
      "source": [
        "def get_data_dict(path):\n",
        "  \"\"\"\n",
        "    Function to get data path dict\n",
        "      params:\n",
        "        path : str: path to data path dictionary\n",
        "      returns: dictionary of data paths\n",
        "  \"\"\"\n",
        "  data_path_dict = open_pickle(path)\n",
        "  return data_path_dict\n",
        "\n",
        "def get_subj_information(data_path_dict):\n",
        "  \"\"\"\n",
        "    Function to get subject information.\n",
        "    data_path_dict  : dictionary containing paths to all data stored on AWS\n",
        "    returns:  subject_ids(list of subjects to run),subj_paths(paths to subject raw data)\n",
        "  \"\"\"\n",
        "  subject_ids = data_path_dict['subject_ID'] #subject_ids\n",
        "  subj_paths = data_path_dict['subject_data'] #subject_paths\n",
        "  return subject_ids,subj_paths\n",
        "\n",
        "def get_labels(data_path_dict):\n",
        "  \"\"\"\n",
        "    Function to get the labels for our data.\n",
        "    data_path_dict  : dictionary containing paths to all data stored on AWS\n",
        "    returns: mask_labels_indices(timepoints we want masked out),binary_labels(labels for our for our two brain states)\n",
        "             and label_type\n",
        "  \"\"\"\n",
        "  \n",
        "  label_data_path = data_path_dict['labels'][0] #get labels\n",
        "  label_type = 'rt_labels' #tell the function what labels we want\n",
        "  mask_labels_indices, binary_labels = labels_mask_binary(label_data_path, label_type) #grab indices and labels\n",
        "  return mask_labels_indices, binary_labels,label_type\n",
        "\n",
        "def get_mask_data(data_path_dict,mask_ind):\n",
        "  \"\"\"\n",
        "    Function to return the mask of what brain voxels we want to include in analysis\n",
        "    Params:\n",
        "      data_path_dict  : dictionary: containing paths to data\n",
        "      mask_ind: int: index of where the path to the masks are 0: full brain mask plus masks that subtract region\n",
        "                1: Regions of interest(ROIs) mask out full brain except structure we care about\n",
        "    returns: dictionary: contains mask data\n",
        "    \n",
        "  \"\"\"\n",
        "  mask_data_filepath = data_path_dict['mask_data'][mask_ind] #path to masked data     \n",
        "  mask_type_dict = access_load_data(mask_data_filepath, True) #get the mask data dictionary\n",
        "  \n",
        "  return mask_type_dict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_mask(np_array_mask):\n",
        "  \"\"\"\n",
        "    Function to create boolean mask to mask out voxels we don't want\n",
        "    Params:\n",
        "      mask_type: string: which mask to grab to get boolean array\n",
        "    returns: boolean array of voxels to include\n",
        "  \"\"\"\n",
        "  #np_array_mask = mask_data[mask_type] #get the mask array\n",
        "  #create a 1-D array for the mask. Important to use Fourier Transformation as we are working in brain space!\n",
        "  mask = np.ma.make_mask(np_array_mask).reshape(79*95*79,order='F')\n",
        "  return mask"
      ],
      "metadata": {
        "id": "yA3Dbi_k7uaJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-ulIQfZNOar"
      },
      "source": [
        "## Set up SVM Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_subject_data(data,mask,mask_labels_indices):\n",
        "  \"\"\"\n",
        "    Function to mask user data to mask out voxels we don't want\n",
        "    Params:\n",
        "      data: dictionary: subject data dictionary contain 4 runs of unmasked data\n",
        "      mask: nd.array: 1-d array boolean values used to only include voxels we want.\n",
        "      mask_labels_indices: indices of rows we want in to include in our model\n",
        "    returns: dictionary: includes 4 runs of masked data\n",
        "  \"\"\"\n",
        "  user_data_dict = {} #create empty dict\n",
        "  arr = []\n",
        "  for i in tqdm.tqdm(range(4)):\n",
        "      user_key = 'run_0' + str(i+1) + '_vec'\n",
        "      array = data[user_key]\n",
        "      array_masked = array[:, mask]\n",
        "      array_masked = array_masked[mask_labels_indices]  \n",
        "      arr.append(array_masked)\n",
        "  user_data_dict['data'] = arr\n",
        "  return user_data_dict"
      ],
      "metadata": {
        "id": "qw5j60FmKluK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "k5fhMjF0AKD0"
      },
      "outputs": [],
      "source": [
        "def scale_data_single_subj(sub_data,train_runs,test_runs,norm='none'):\n",
        "  \"\"\"\n",
        "    Function to scale data.\n",
        "    Params:\n",
        "      sub_data     : (1 subject data, keys as subject ID for frmi data or labels)\n",
        "      sub_id       : subject id  of subject we are normalizing for\n",
        "      runs_test    : tuple, (which run are we using for the test data)\n",
        "      norm         : list, (\"RUNS\": normalizing separately on each run;\n",
        "                              \"SUBJECT\": Normalizing separately by each subject)\n",
        "    returns      : dictionary of nd.arrays, Concatenated X data of (time points, x*y*z) x = 79, y = 95, z = 75\n",
        "                    and Concatenated y labels of (time points,)\n",
        "    \"\"\"\n",
        "  ##run standardization\n",
        "  ##initialize empty dictionary\n",
        "  normalized_runs = {}\n",
        "  for run in runs_list:\n",
        "    run_name = user_key = 'run_0' + str(run) \n",
        "    run_data = sub_data['data'][run-1]\n",
        "    if norm=='none':\n",
        "      normalized_runs[run_name] = clean(run_data,detrend=True,standardize=False,filter=False,standardize_confounds=False)\n",
        "    else:\n",
        "      normalized_runs[run_name] = clean(run_data,detrend=True,standardize=norm,filter=False,standardize_confounds=False)\n",
        "  return normalized_runs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy_scores(clf,data,X_train,y_train,runs_test,y_labels):\n",
        "  \"\"\"\n",
        "    Function to get accuracy scores for subject models.\n",
        "    Params:\n",
        "      model_dict: contains subject model and training/test/val/data\n",
        "      subj: subject name \n",
        "      normalization_type: options: 'PSC','ZNORM','none' what type of normalization\n",
        "    returns: subj_list, list of subject metrics\n",
        "  \"\"\"\n",
        "  accuracy_list = []\n",
        "  df_columns = ['train_acc']\n",
        "  y_predicts = clf.predict(X_train)\n",
        "  accuracy_list.append(accuracy_score(y_train,y_predicts))\n",
        "  for run in runs_test:\n",
        "    y_predicts = clf.predict(data[run])\n",
        "    df_columns.append(run + '_acc')\n",
        "    accuracy_list.append(accuracy_score(y_labels,y_predicts))\n",
        "    df_columns.append(run+'_f1_score')\n",
        "    accuracy_list.append(f1_score(y_labels,y_predicts))\n",
        "    \n",
        "    \n",
        "  return accuracy_list,df_columns"
      ],
      "metadata": {
        "id": "Yg7u_m7z-4BQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predicts(clf,data,runs_test):\n",
        "  \"\"\"\n",
        "    Function to get accuracy scores for subject models.\n",
        "    Params:\n",
        "      model_dict: contains subject model and training/testdata\n",
        "      subj: subject name \n",
        "    returns: y_val_predicts(if validation run),y_test_predicts\n",
        "  \"\"\"\n",
        "  predictions_dict = {}\n",
        "  for runs in runs_test:\n",
        "    predictions_dict[runs] = {}\n",
        "    predictions_dict[runs]['predicts'] = clf.predict(data[runs])\n",
        "    predictions_dict[runs]['proba'] = clf.predict_proba(data[runs])\n",
        "    predictions_dict[runs]['decision_function'] = clf.decision_function(data[runs])\n",
        "  \n",
        "                                                  \n",
        "  return predictions_dict"
      ],
      "metadata": {
        "id": "o02tDjJDi2ts"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "OAtPV7sKNyhc"
      },
      "outputs": [],
      "source": [
        "def run_single_subject_svm(data,runs_train,train_labels,svc_kernel='rbf',svc_c=1,do_cv=False,params={}):\n",
        "  \"\"\"\n",
        "    Function to run cross-validation or single subject SVM\n",
        "    Params:\n",
        "      tuple: contains\n",
        "        X_train      : 2-d array of training data\n",
        "        y_train   : sub_labels to indicate which row of the sub_data belongs to increase/decrease state\n",
        "        svc_kernel : kernel for svc\n",
        "        svc_c: c value for svc\n",
        "      optionals:\n",
        "        do_cv: boolean: to decide if cross-validation gridsearch is requested: default=False\n",
        "        params: dictionary: dictionary containing params to grid search: default=empty dictionary\n",
        "    returns      : subject individual model\n",
        "  \"\"\" \n",
        "  #run cv if do_cv = True, else run individual model SVM\n",
        "  X_train = []\n",
        "  y_train = []\n",
        "  if len(runs_train)>1:\n",
        "        for run in runs_train:\n",
        "          X_train.append(data[run])\n",
        "          y_train.append(train_labels)    \n",
        "        X_train = np.concatenate(np.array(X_train))\n",
        "        y_train = np.concatenate(np.array(y_train))\n",
        "  else:\n",
        "    X_train = data[runs_train[0]]\n",
        "    y_train = train_labels\n",
        "  if do_cv:\n",
        "    #cv_params = {'C':[0.7, 1, 5, 10],'kernel':['linear', 'rbf']}\n",
        "    svc = SVC()\n",
        "    clf = GridSearchCV(svc, params)\n",
        "    clf.fit(X_train,y_train)\n",
        "    return clf\n",
        "  else:\n",
        "    clf = SVC(C=svc_c,kernel=svc_kernel,probability=True)\n",
        "    clf.fit(X_train,y_train)\n",
        "  return clf,X_train,y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cUIIWq3GsvNb"
      },
      "outputs": [],
      "source": [
        "\n",
        "def run_subject_model(data_path_dict,path,file_name,runs_train,runs_test,mask_list,mask_ind_list,norm_type='NONORM'):\n",
        "  \"\"\"\n",
        "    Function loops over subjects to grap data,scale,data,and run the SVM.\n",
        "    data_path_dict  : dictionary containing paths to all data stored on AWS\n",
        "    path : path to save pickle files. NOTE: If run on all subjects, this will take up about 20 GB\n",
        "    file_name : name of file to prepend to subject id\n",
        "    runs_train: what runs do we want to train on\n",
        "    runs_test: what runs do we want to test on\n",
        "    mask_type: which type of brain mask do we want to apply\n",
        "    mask_ind: to distinguish between ROI regions of masking(1) and full brain(or full_brain minus ROIs)(0)\n",
        "    do_psc: do we want to apply Percent Signal Change normalization. Default = True,\n",
        "    norm: Takes three parameters: none(no normalization will be applied),\"SUBJECT\"(apply normalization per subject)\n",
        "          \"RUNS\"(apply normalization per run separately)\n",
        "  \"\"\"\n",
        "  model_dict = {}\n",
        "  #get subject information\n",
        "  subject_ids,subj_paths = get_subj_information(data_path_dict)\n",
        "  \n",
        "  #get mask labels to only retrieve time series we care about\n",
        "  mask_labels_indices,binary_labels,label_type = get_labels(data_path_dict)\n",
        "  #get mask data\n",
        "  \n",
        "  #loop over subjects\n",
        "  for idx in range(len(subject_ids)):\n",
        "    sub_id = subject_ids[idx] #get subject id\n",
        "    subj_path = subj_paths[idx] #get subject path\n",
        "    #get subject raw data\n",
        "    #call function to load data and return a dictionary. We are loading .mat files so need to set second param to True\n",
        "    sub_data_unmasked = access_load_data(mat_path,True)\n",
        "    #loop over masks indices to reach masks or rois\n",
        "    for mind in mask_ind_list:   \n",
        "      mask_data_dict = get_mask_data(data_path_dict,mask_ind_list[mind])\n",
        "      #loop over masks we want to run\n",
        "      for mask_type in mask_list:\n",
        "       \n",
        "        mask = create_mask(mask_type)\n",
        "        #mask_data\n",
        "        sub_data_masked = mask_data(sub_data_unmasked,mask_labels_indices,binary_labels)\n",
        "        sub_data = sub_data_masked[sub_id]\n",
        "        #normalize data if indicated and return train data/labels,val data/labels, test data/labels\n",
        "        normalized_runs = scale_data_single_subj(sub_data,binary_labels,runs_list,norm_type=norm_type)\n",
        "        #run svm, add clf to dictionary\n",
        "        clf = run_single_subject_svm(train_data,svc_kernel,svc_c,do_cv=False,params={})\n",
        "        #run get predicts to get predictions and add to subject_model_dictionary\n",
        "        final_model_dict = get_predicts(model_dict['model'],val_test_data_labels)\n",
        "        model_dict[sub_id]['y_test_predicts'] = y_test_predicts\n",
        "        model_dict[sub_id]['y_test_proba'] = y_test_proba\n",
        "        model_dict[sub_id]['y_test_dec_func'] = y_test_dec_func\n",
        "    if len(y_val_predicts)>0:\n",
        "      model_dict[sub_id]['y_val_predicts'] = y_val_predicts\n",
        "      model_dict[sub_id]['y_val_proba'] = y_val_proba\n",
        "      model_dict[sub_id]['y_val_dec_func'] = y_val_dec_func\n",
        "    destination_path = f'{path}{file_name}{sub_id}.pkl'     \n",
        "    single_model_path[sub_id] = destination_path\n",
        "    # filehandler = open(destination_path,\"wb\") #comment out this\n",
        "    # pickle.dump(model_dict,filehandler) #comment out this\n",
        "    # filehandler.close() #comment out this\n",
        "    object_name = destination_path\n",
        "    s3_upload(model_dict,object_name,\"pickle\")\n",
        "  object_name = f'{path}{mask_type}_data_path_dict.pkl'\n",
        "  s3_upload(single_model_path,object_name,\"pickle\")\n",
        "  # filehandler = open(object_name,\"wb\") #comment out this\n",
        "  # pickle.dump(single_model_path,filehandler) #comment out this\n",
        "  # filehandler.close() #comment out this\n",
        "  return model_dict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###running model with best params across all masks\n",
        "##what runs do you want to normalize on\n",
        "runs_train=['run_02'] #runs we want to train on\n",
        "runs_test=['run_03','run_04'] #runs we want to test on\n",
        "runs_list=[2,3,4]\n",
        "svc_kernel='rbf' #specify kernel \n",
        "svc_c = 1 #specify c parameter\n",
        "save_data_path = f'/content/drive/My Drive/data/singlesubjectmodels/'\n",
        "mask_list = ['mask']\n",
        "mask_indices = [0]\n",
        "#get subject information\n",
        "subjs_id, subjs_paths = get_subj_information(data_path_dict)\n",
        "#get mask labels to only retrieve time series we care about\n",
        "mask_labels_indices,binary_labels,label_type = get_labels(data_path_dict)\n",
        "subj_mask_model = {}\n",
        "for idx in range(len(subjs_id)):\n",
        "  subj_id = subjs_id[idx]\n",
        "  subj_path = subjs_paths[idx]\n",
        "  subj_data = access_load_data(subj_path,True)\n",
        "  for midx in mask_indices:\n",
        "    mask_dict = get_mask_data(data_path_dict,midx)\n",
        "    for mask_type in mask_list:\n",
        "      subj_mask_model[mask_type] = {}\n",
        "      subj_mask_model[mask_type][subj_id] = {}\n",
        "      mask = make_mask(mask_dict[mask_type])\n",
        "      masked_data = mask_subject_data(subj_data,mask,mask_labels_indices)\n",
        "      scaled_data = scale_data_single_subj(masked_data,runs_train,runs_test,norm='zscore')\n",
        "      clf,X_train,y_train = run_single_subject_svm(scaled_data,runs_train,binary_labels,svc_kernel,svc_c)\n",
        "      subj_mask_model[mask_type][subj_id]['model'] = clf\n",
        "      subj_mask_model[mask_type][subj_id]['X_train'] = X_train\n",
        "      subj_mask_model[mask_type][subj_id]['y_train'] = y_train\n",
        "      subj_mask_model[mask_type][subj_id]['predicts'] = get_predicts(clf,scaled_data,runs_test)\n",
        "      full_path_name = f'{save_data_path}{mask_type}_subject_models.pkl'\n",
        "      filehandler = open(full_path_name,\"wb\")\n",
        "      pickle.dump(subj_mask_model,filehandler)\n",
        "      filehandler.close()\n",
        "  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "id": "841pE0J14LuB",
        "outputId": "bb1ce220-99ff-4e84-cde5-48ac9a60ed0f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00,  4.76it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-024b1f68bf31>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m       \u001b[0msubj_mask_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubj_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X_train'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m       \u001b[0msubj_mask_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubj_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_train'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m       \u001b[0msubj_mask_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubj_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predicts'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_predicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscaled_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mruns_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m       \u001b[0mfull_path_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{save_data_path}{mask_type}_subject_models.pkl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m       \u001b[0mfilehandler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_path_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-8e6c0b6286af>\u001b[0m in \u001b[0;36mget_predicts\u001b[0;34m(clf, data, runs_test)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpredictions_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mruns\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mpredictions_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mruns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'predicts'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mruns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mpredictions_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mruns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'proba'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mruns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mpredictions_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mruns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'decision_function'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mruns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'SVC' object has no attribute 'proba'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###data exploration for different normalization strategies\n",
        "##what runs do you want to normalize on\n",
        "runs_train=['run_02'] #runs we want to train on\n",
        "runs_test=['run_03','run_04'] #runs we want to test on\n",
        "runs_list=[2,3,4]\n",
        "##for fMRI, we always want to detrend the data\n",
        "norm_list = ['psc','zscore','none'] #list of normalization strategies you want to test\n",
        "save_subject_data =  ['10047_09030','30017_09567'] #specify subjects you want to save for normalization visualizations\n",
        "svc_kernel='rbf' #specify kernel \n",
        "svc_c = 1 #specify c parameter\n",
        "mask_dict = get_mask_data(data_path_dict,0) #get mask_dict\n",
        "mask = make_mask(mask_dict['mask']) #create mask\n",
        "#get subject information\n",
        "subjs_id, subjs_paths = get_subj_information(data_path_dict)\n",
        "#get mask labels to only retrieve time series we care about\n",
        "mask_labels_indices,binary_labels,label_type = get_labels(data_path_dict)\n",
        "###initialize variables to hold results\n",
        "model_dict = {}\n",
        "results = []\n",
        "df_columns = ['subject_id','norm_type']\n",
        "#loop over norm_type to get initialize model_dict norm_type dictionary\n",
        "for norm_type in norm_list:\n",
        "  model_dict[norm_type] = {}\n",
        "#for loop to loop over subjects\n",
        "for idx in range(len(subjs_id)):\n",
        "  subj_id = subjs_id[idx] #get subj_id\n",
        "  subj_path = subjs_paths[idx] #get subj_path\n",
        "  subj_data = access_load_data(subj_path,True) #get subj_data\n",
        "  masked_data = mask_subject_data(subj_data,mask,mask_labels_indices) #mask subject data\n",
        "  #loop over normalizations to do\n",
        "  for norm_type in norm_list:\n",
        "    norm_results = [subj_id,norm_type] #store subject id and normalization strategy in results list\n",
        "    scaled_data = scale_data_single_subj(masked_data,runs_train,runs_test,norm=norm_type) #scale the data for runs listed in runs_lit\n",
        "    clf,X_train,y_train = run_single_subject_svm(scaled_data,runs_train,binary_labels,svc_kernel,svc_c) #run the model\n",
        "    if subj_id in save_subject_data:    #if subject in list of subjects to save\n",
        "      model_dict[norm_type][subj_id] = {} #initialize empty dictionary for the subject data\n",
        "      #store subject variables\n",
        "      model_dict[norm_type][subj_id]['model'] = clf\n",
        "      model_dict[norm_type][subj_id]['X_train'] = X_train\n",
        "      model_dict[norm_type][subj_id]['y_train'] = y_train\n",
        "      model_dict[norm_type][subj_id]['data'] = scaled_data\n",
        "    sub_scores,cols = get_accuracy_scores(clf,scaled_data,X_train,y_train,runs_test,binary_labels) #get accuracy scores for analysis\n",
        "    norm_results.extend(sub_scores) #append to list\n",
        "    results.append(norm_results) #append to results\n",
        "df_columns.extend(cols) #extend data frame columns\n",
        "results_df = pd.DataFrame(results,columns=df_columns) #create df\n",
        "#save results for analysis\n",
        "save_data_path = f'/content/drive/My Drive/data/dataexploration/{svc_kernel}_exploration/accuracy_results.csv'\n",
        "results_df.to_csv(save_data_path)\n",
        "#save models for visualization\n",
        "save_models_path = f'/content/drive/My Drive/data/dataexploration/{svc_kernel}_exploration/norm_models.pkl'\n",
        "filehandler = open(save_models_path,\"wb\")\n",
        "pickle.dump(model_dict,filehandler)\n",
        "filehandler.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "Uzmswf5uu5uz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m41fuwsALmlH"
      },
      "outputs": [],
      "source": [
        "def run_subject_model_cv(data_path_dict, destination_path, runs_train, runs_list, mask_type, mask_ind, norm_type='none'):\n",
        "  \"\"\"\n",
        "    Function to run cross validation for data. Note: takes 2 hours to run on all subjects.\n",
        "    Params:\n",
        "      data_path_dict  : dictionary containing paths to all data stored on AWS\n",
        "      path : path to save pickle files. NOTE: If run on all subjects, this will take up about 20 GB\n",
        "      file_name : name of file to prepend to subject id\n",
        "      runs_train: what runs do we want to train on\n",
        "      runs_test: what runs do we want to test on\n",
        "      mask_type: which type of brain mask do we want to apply\n",
        "      mask_ind: to distinguish between ROI regions of masking(1) and full brain(or full_brain minus ROIs)(0)\n",
        "      do_psc: do we want to apply Percent Signal Change normalization. Default = True,\n",
        "      norm: Takes three parameters: none(no normalization will be applied),\"SUBJECT\"(apply normalization per subject)\n",
        "            \"RUNS\"(apply normalization per run separately)\n",
        "  Returns: cv_dict, a dictionary that holds cv results\n",
        "  \"\"\"\n",
        "\n",
        "  cv_dict = {}\n",
        "  subject_ids,subj_paths = get_subj_information(data_path_dict)\n",
        "  mask_labels_indices,binary_labels,label_type = get_labels(data_path_dict)\n",
        "  mask_data = get_mask_data(data_path_dict,mask_ind)\n",
        "  mask = make_mask(mask_data[mask_type])\n",
        "  for idx in range(len(subject_ids)):\n",
        "    subj_id = subjs_id[idx]\n",
        "    subj_path = subjs_paths[idx]\n",
        "    subj_data = access_load_data(subj_path,True) \n",
        "    masked_data = mask_data(subj_data,mask,mask_labels_indices)\n",
        "     \n",
        "    sub_data = user_data_dict[sub_id]\n",
        "    sub_labels = user_data_dict[f\"{sub_id}_rt_labels\"]\n",
        "    clf = run_single_subject_svm(sub_data,sub_labels,runs_train,runs_test,norm_type,do_cv=True)\n",
        "    cv_dict[sub_id] = {}\n",
        "    cv_dict[sub_id]['model'] = clf\n",
        "  filehandler = open(destination_path,\"wb\")\n",
        "  pickle.dump(cv_dict,filehandler)\n",
        "  filehandler.close() \n",
        "  return cv_dict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv_dict = {} #initialize the results dictionary\n",
        "destination_path = '/content/drive/My Drive/data/dataexploration/cross_validation_results/cv_results.pkl' #where to save data\n",
        "runs_train=['run_02'] #runs we want to train on\n",
        "runs_list=[2] #cv results\n",
        "cv_params = {'C':[1, 5, 10,1000],'kernel':['linear', 'rbf'],'gamma': [.0001, .01, 'auto','scale']}\n",
        "subject_ids,subj_paths = get_subj_information(data_path_dict)\n",
        "mask_labels_indices,binary_labels,label_type = get_labels(data_path_dict)\n",
        "mask_dict = get_mask_data(data_path_dict,0)\n",
        "mask = make_mask(mask_dict['mask'])\n",
        "for idx in range(len(subject_ids)):\n",
        "  subj_id = subjs_id[idx]\n",
        "  subj_path = subjs_paths[idx]\n",
        "  subj_data = access_load_data(subj_path,True) \n",
        "  masked_data = mask_subject_data(subj_data,mask,mask_labels_indices)\n",
        "  scaled_data = scale_data_single_subj(masked_data,runs_train,runs_test,norm='zscore')\n",
        "  clf = run_single_subject_svm(scaled_data,runs_train,binary_labels,do_cv=True,params=cv_params)\n",
        "  cv_dict[subj_id] = {}\n",
        "  cv_dict[subj_id]['model'] = clf\n",
        "filehandler = open(destination_path,\"wb\")\n",
        "pickle.dump(cv_dict,filehandler)\n",
        "filehandler.close() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddU2mgT1DDhQ",
        "outputId": "9634977a-c9a5-4352-cdb5-bd7d2466679e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:01<00:00,  3.10it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  2.77it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  2.92it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.34it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.49it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.42it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.52it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.45it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.38it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.53it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.40it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.37it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.47it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.64it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.15it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  2.97it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.06it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.31it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.63it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.25it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.60it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.29it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.57it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.00it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  2.91it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.25it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.30it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.05it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.11it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.28it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.14it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  2.88it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  2.79it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  2.97it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.14it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.11it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.15it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.15it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.29it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.22it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.60it/s]\n",
            "100%|██████████| 4/4 [00:00<00:00,  4.51it/s]\n",
            "100%|██████████| 4/4 [00:00<00:00,  4.00it/s]\n",
            "100%|██████████| 4/4 [00:00<00:00,  4.17it/s]\n",
            "100%|██████████| 4/4 [00:00<00:00,  4.16it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.78it/s]\n",
            "100%|██████████| 4/4 [00:01<00:00,  3.69it/s]\n",
            "100%|██████████| 4/4 [00:00<00:00,  4.10it/s]\n",
            "100%|██████████| 4/4 [00:00<00:00,  4.44it/s]\n",
            "100%|██████████| 4/4 [00:00<00:00,  4.38it/s]\n",
            "100%|██████████| 4/4 [00:00<00:00,  4.55it/s]\n",
            "100%|██████████| 4/4 [00:00<00:00,  4.40it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask_labels_indices,binary_labels,label_type = get_labels(data_path_dict)\n",
        "mask_dict = get_mask_data(data_path_dict,0)\n",
        "mask = make_mask(mask_dict['mask'])\n",
        "mask_labels_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fX5BjLOBwvch",
        "outputId": "a90b7047-358b-4ee7-8de7-a93b3ad04133"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  3,   4,   5,   6,   7,   8,   9,  15,  16,  17,  18,  19,  20,\n",
              "        21,  27,  28,  29,  30,  31,  32,  33,  39,  40,  41,  42,  43,\n",
              "        44,  45,  51,  52,  53,  54,  55,  56,  57,  63,  64,  65,  66,\n",
              "        67,  68,  69,  75,  76,  77,  78,  79,  80,  81,  87,  88,  89,\n",
              "        90,  91,  92,  93,  99, 100, 101, 102, 103, 104, 105, 111, 112,\n",
              "       113, 114, 115, 116, 117, 123, 124, 125, 126, 127, 128, 129, 135,\n",
              "       136, 137, 138, 139, 140, 141])"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##what runs do you want to normalize on\n",
        "destination_path = '/content/drive/My Drive/data/dataexploration/cross_validation_results/cv_results.pkl'\n",
        "runs_train=['run_02'] #runs we want to train on\n",
        "runs_test=['run_03','run_04'] #runs we want to test on\n",
        "runs_list=[2]\n",
        "mask_type = 'mask'\n",
        "mask_ind = 0\n",
        "cv_results = run_subject_model_cv(data_path_dict,destination_path,runs_train,runs_list,mask_type,mask_ind,norm='zscore')"
      ],
      "metadata": {
        "id": "8mTSbgo41nl0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "SingleSubjectSVM",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}