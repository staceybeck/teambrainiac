{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yecatstevir/teambrainiac/blob/main/source/SingleSubjectSVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_wDm122NTXY"
      },
      "source": [
        "# Whole Brain Support Vector Machine Training\n",
        "- Go to 'Runtime' in Colab browser bar, select 'Change Runtime Type', select 'High-RAM' from 'Runtime Shape'. \n",
        "- load local pickle file containing all masked, normalized Whole Brain subject data in numpy matrix format\n",
        "- SVM training per subject"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c284FIeHNcR2"
      },
      "source": [
        "### Mount Google Drive and clone repository\n",
        "- open to source directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIfcjimGzIZo",
        "outputId": "e4ffb2d2-37b1-4945-f074-54e76689b8ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')#, force_remount = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5eq9qyKMTp_",
        "outputId": "b5fd8451-c051-43c7-c888-f010b96fe2c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lh0DqCU10r3n"
      },
      "outputs": [],
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/yecatstevir/teambrainiac.git\n",
        "# Change directory into cloned repo\n",
        "%cd teambrainiac/source\n",
        "!ls\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zadD_dOPPjQm"
      },
      "source": [
        "### Load path_config.py \n",
        "- we are already in source so we can just load this file without changing directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_2qZqNJPLkn"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cmYZ22BNj8h"
      },
      "source": [
        "### Import libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "abHr7oUl06ED"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Import libraries\n",
        "!pip install boto3 nilearn nibabel #for saving data and image visualizations\n",
        "import pickle\n",
        "#sklearn packages needed\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, auc, recall_score, precision_score,roc_curve,f1_score\n",
        "#important utility functions for loading,masking,saving data\n",
        "#from utils import *\n",
        "from access_data import *\n",
        "from process import *\n",
        "#normal python packages we use\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import signal\n",
        "from nilearn.signal import clean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZVh3WQeMk1c"
      },
      "source": [
        "### Get paths to subject data and grab labels for SVM"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## load and open the pickle file that contains paths to all data.\n",
        "path = \"data/data_path_dictionary.pkl\"\n",
        "data_path_dict = open_pickle(path)\n",
        "\n",
        "##get mask_dictionary\n"
      ],
      "metadata": {
        "id": "l4wpPDZh1U1X"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Functions to get information about data to run our SVM"
      ],
      "metadata": {
        "id": "B4cu86EDfpre"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "YwJQ-eCGifbH"
      },
      "outputs": [],
      "source": [
        "def get_data_dict(path):\n",
        "  \"\"\"\n",
        "    Function to get data path dict\n",
        "      params:\n",
        "        path : str: path to data path dictionary\n",
        "      returns: dictionary of data paths\n",
        "  \"\"\"\n",
        "  data_path_dict = open_pickle(path)\n",
        "  return data_path_dict\n",
        "\n",
        "def get_subj_information(data_path_dict):\n",
        "  \"\"\"\n",
        "    Function to get subject information.\n",
        "    data_path_dict  : dictionary containing paths to all data stored on AWS\n",
        "    returns:  subject_ids(list of subjects to run),subj_paths(paths to subject raw data)\n",
        "  \"\"\"\n",
        "  subject_ids = data_path_dict['subject_ID'] #subject_ids\n",
        "  subj_paths = data_path_dict['subject_data'] #subject_paths\n",
        "  return subject_ids,subj_paths\n",
        "\n",
        "def get_labels(data_path_dict):\n",
        "  \"\"\"\n",
        "    Function to get the labels for our data.\n",
        "    data_path_dict  : dictionary containing paths to all data stored on AWS\n",
        "    returns: mask_labels_indices(timepoints we want masked out),binary_labels(labels for our for our two brain states)\n",
        "             and label_type\n",
        "  \"\"\"\n",
        "  \n",
        "  label_data_path = data_path_dict['labels'][0] #get labels\n",
        "  label_type = 'rt_labels' #tell the function what labels we want\n",
        "  mask_labels_indices, binary_labels = labels_mask_binary(label_data_path, label_type) #grab indices and labels\n",
        "  return mask_labels_indices, binary_labels,label_type\n",
        "\n",
        "def get_mask_data(data_path_dict,mask_ind):\n",
        "  \"\"\"\n",
        "    Function to return the mask of what brain voxels we want to include in analysis\n",
        "    Params:\n",
        "      data_path_dict  : dictionary: containing paths to data\n",
        "      mask_ind: int: index of where the path to the masks are 0: full brain mask plus masks that subtract region\n",
        "                1: Regions of interest(ROIs) mask out full brain except structure we care about\n",
        "    returns: dictionary: contains mask data\n",
        "    \n",
        "  \"\"\"\n",
        "  mask_data_filepath = data_path_dict['mask_data'][mask_ind] #path to masked data     \n",
        "  mask_type_dict = access_load_data(mask_data_filepath, True) #get the mask data dictionary\n",
        "  \n",
        "  return mask_type_dict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_mask(np_array_mask):\n",
        "  \"\"\"\n",
        "    Function to create boolean mask to mask out voxels we don't want\n",
        "    Params:\n",
        "      mask_type: string: which mask to grab to get boolean array\n",
        "    returns: boolean array of voxels to include\n",
        "  \"\"\"\n",
        "  #np_array_mask = mask_data[mask_type] #get the mask array\n",
        "  #create a 1-D array for the mask. Important to use Fourier Transformation as we are working in brain space!\n",
        "  mask = np.ma.make_mask(np_array_mask).reshape(79*95*79,order='F')\n",
        "  return mask"
      ],
      "metadata": {
        "id": "yA3Dbi_k7uaJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-ulIQfZNOar"
      },
      "source": [
        "## Set up SVM Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_subject_data(data,mask,mask_labels_indices):\n",
        "  \"\"\"\n",
        "    Function to mask user data to mask out voxels we don't want\n",
        "    Params:\n",
        "      data: dictionary: subject data dictionary contain 4 runs of unmasked data\n",
        "      mask: nd.array: 1-d array boolean values used to only include voxels we want.\n",
        "      mask_labels_indices: indices of rows we want in to include in our model\n",
        "    returns: dictionary: includes 4 runs of masked data\n",
        "  \"\"\"\n",
        "  user_data_dict = {} #create empty dict\n",
        "  arr = []\n",
        "  for i in tqdm.tqdm(range(4)):\n",
        "      user_key = 'run_0' + str(i+1) + '_vec'\n",
        "      array = data[user_key]\n",
        "      array_masked = array[:, mask]\n",
        "      array_masked = array_masked[mask_labels_indices]  \n",
        "      arr.append(array_masked)\n",
        "  user_data_dict['data'] = arr\n",
        "  return user_data_dict"
      ],
      "metadata": {
        "id": "qw5j60FmKluK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "k5fhMjF0AKD0"
      },
      "outputs": [],
      "source": [
        "def scale_data_single_subj(sub_data,train_runs,test_runs,norm='none'):\n",
        "  \"\"\"\n",
        "    Function to scale data.\n",
        "    Params:\n",
        "      sub_data     : (1 subject data, keys as subject ID for frmi data or labels)\n",
        "      sub_id       : subject id  of subject we are normalizing for\n",
        "      runs_test    : tuple, (which run are we using for the test data)\n",
        "      norm         : list, (\"RUNS\": normalizing separately on each run;\n",
        "                              \"SUBJECT\": Normalizing separately by each subject)\n",
        "    returns      : dictionary of nd.arrays, Concatenated X data of (time points, x*y*z) x = 79, y = 95, z = 75\n",
        "                    and Concatenated y labels of (time points,)\n",
        "    \"\"\"\n",
        "  ##run standardization\n",
        "  ##initialize empty dictionary\n",
        "  normalized_runs = {}\n",
        "  for run in runs_list:\n",
        "    run_name = user_key = 'run_0' + str(run) \n",
        "    run_data = sub_data['data'][run-1]\n",
        "    if norm=='none':\n",
        "      normalized_runs[run_name] = clean(run_data,detrend=True,standardize=False,filter=False,standardize_confounds=False)\n",
        "    else:\n",
        "      normalized_runs[run_name] = clean(run_data,detrend=True,standardize=norm,filter=False,standardize_confounds=False)\n",
        "  return normalized_runs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_accuracy_scores(clf,data,X_train,y_train,runs_test,y_labels):\n",
        "  \"\"\"\n",
        "    Function to get accuracy scores for subject models.\n",
        "    Params:\n",
        "      model_dict: contains subject model and training/test/val/data\n",
        "      subj: subject name \n",
        "      normalization_type: options: 'PSC','ZNORM','none' what type of normalization\n",
        "    returns: subj_list, list of subject metrics\n",
        "  \"\"\"\n",
        "  accuracy_list = []\n",
        "  df_columns = ['train_acc']\n",
        "  y_predicts = clf.predict(X_train)\n",
        "  accuracy_list.append(accuracy_score(y_train,y_predicts))\n",
        "  for run in runs_test:\n",
        "    y_predicts = clf.predict(data[run])\n",
        "    df_columns.append(run + '_acc')\n",
        "    accuracy_list.append(accuracy_score(y_labels,y_predicts))\n",
        "    df_columns.append(run+'_f1_score')\n",
        "    accuracy_list.append(f1_score(y_labels,y_predicts))\n",
        "    \n",
        "    \n",
        "  return accuracy_list,df_columns"
      ],
      "metadata": {
        "id": "Yg7u_m7z-4BQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_predicts(clf,data,runs_test):\n",
        "  \"\"\"\n",
        "    Function to get accuracy scores for subject models.\n",
        "    Params:\n",
        "      model_dict: contains subject model and training/testdata\n",
        "      subj: subject name \n",
        "    returns: y_val_predicts(if validation run),y_test_predicts\n",
        "  \"\"\"\n",
        "  predictions_dict = {}\n",
        "  for runs in runs_test:\n",
        "    predictions_dict[runs] = {}\n",
        "    predictions_dict[runs]['predicts'] = clf.predict(data[runs])\n",
        "    predictions_dict[runs]['proba'] = clf.predict_proba(data[runs])\n",
        "    predictions_dict[runs]['decision_function'] = clf.decision_function(data[runs])\n",
        "  \n",
        "                                                  \n",
        "  return predictions_dict"
      ],
      "metadata": {
        "id": "o02tDjJDi2ts"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "OAtPV7sKNyhc"
      },
      "outputs": [],
      "source": [
        "def run_single_subject_svm(data,runs_train,train_labels,svc_kernel='rbf',svc_c=1,do_cv=False,params={}):\n",
        "  \"\"\"\n",
        "    Function to run cross-validation or single subject SVM\n",
        "    Params:\n",
        "      tuple: contains\n",
        "        X_train      : 2-d array of training data\n",
        "        y_train   : sub_labels to indicate which row of the sub_data belongs to increase/decrease state\n",
        "        svc_kernel : kernel for svc\n",
        "        svc_c: c value for svc\n",
        "      optionals:\n",
        "        do_cv: boolean: to decide if cross-validation gridsearch is requested: default=False\n",
        "        params: dictionary: dictionary containing params to grid search: default=empty dictionary\n",
        "    returns      : subject individual model\n",
        "  \"\"\" \n",
        "  #run cv if do_cv = True, else run individual model SVM\n",
        "  X_train = []\n",
        "  y_train = []\n",
        "  if len(runs_train)>1:\n",
        "        for run in runs_train:\n",
        "          X_train.append(data[run])\n",
        "          y_train.append(train_labels)    \n",
        "        X_train = np.concatenate(np.array(X_train))\n",
        "        y_train = np.concatenate(np.array(y_train))\n",
        "  else:\n",
        "    X_train = data[runs_train[0]]\n",
        "    y_train = train_labels\n",
        "  if do_cv:\n",
        "    #cv_params = {'C':[0.7, 1, 5, 10],'kernel':['linear', 'rbf']}\n",
        "    svc = SVC()\n",
        "    clf = GridSearchCV(svc, params)\n",
        "    clf.fit(X_train,y_train)\n",
        "    return clf\n",
        "  else:\n",
        "    clf = SVC(C=svc_c,kernel=svc_kernel,probability=True)\n",
        "    clf.fit(X_train,y_train)\n",
        "  return clf,X_train,y_train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###running model with best params across all masks\n",
        "##what runs do you want to normalize on\n",
        "runs_train=['run_02'] #runs we want to train on\n",
        "runs_test=['run_03','run_04'] #runs we want to test on\n",
        "runs_list=[2,3,4] #specify runs we want to normalize\n",
        "norm_type = 'zscore' #specify normalization\n",
        "svc_kernel='rbf' #specify kernel \n",
        "svc_c = 1 #specify c parameter\n",
        "save_data_path = f'/content/drive/My Drive/data/singlesubjectmodels/' #where we want to store our models\n",
        "#masks we want to run model on, needs to be nested list for cell to run\n",
        "mask_list = [['mask']] \n",
        "#indices of the masks we want 0 = whole brain mask and masks minus ROIs, 1 = ROIs\n",
        "mask_indices = [0] #indices of the masks we want 0 = whole brain mask and masks minus ROIs, 1 = ROIs\n",
        "#get subject information\n",
        "subjs_id, subjs_paths = get_subj_information(data_path_dict)\n",
        "#get mask labels to only retrieve time series we care about\n",
        "mask_labels_indices,binary_labels,label_type = get_labels(data_path_dict)\n",
        "subj_mask_model = {}\n",
        "subj_mask_model['data'] = {}\n",
        "for idx in range(len(subjs_id)):\n",
        "  subj_id = subjs_id[idx]\n",
        "  subj_path = subjs_paths[idx]\n",
        "  subj_data = access_load_data(subj_path,True)\n",
        "  for midx in mask_indices:\n",
        "    mask_dict = get_mask_data(data_path_dict,midx)\n",
        "    masks = mask_list[midx]\n",
        "    for mask_type in masks:\n",
        "      subj_mask_model['data'][subj_id] = {}\n",
        "      mask = make_mask(mask_dict[mask_type])\n",
        "      masked_data = mask_subject_data(subj_data,mask,mask_labels_indices)\n",
        "      scaled_data = scale_data_single_subj(masked_data,runs_train,runs_test,norm='zscore')\n",
        "      clf,X_train,y_train = run_single_subject_svm(scaled_data,runs_train,binary_labels,svc_kernel,svc_c)\n",
        "      subj_mask_model['data'][subj_id]['model'] = clf\n",
        "      subj_mask_model['data'][subj_id]['X_train'] = X_train\n",
        "      subj_mask_model['data'][subj_id]['y_train'] = y_train\n",
        "      subj_mask_model['data'][subj_id]['predicts'] = get_predicts(clf,scaled_data,runs_test)\n",
        "      full_path_name = f'{save_data_path}{mask_type}_subject_models.pkl'\n",
        "      filehandler = open(full_path_name,\"wb\")\n",
        "      pickle.dump(subj_mask_model,filehandler)\n",
        "      filehandler.close()\n",
        "  \n"
      ],
      "metadata": {
        "id": "841pE0J14LuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "###data exploration for different normalization strategies\n",
        "##what runs do you want to normalize on\n",
        "runs_train=['run_01','run_02'] #runs we want to train on\n",
        "runs_test=['run_03','run_04'] #runs we want to test on\n",
        "runs_list=[1,2,3,4]\n",
        "##for fMRI, we always want to detrend the data\n",
        "norm_list = ['psc','zscore','none'] #list of normalization strategies you want to test\n",
        "save_subject_data =  ['10047_09030','30017_09567'] #specify subjects you want to save for normalization visualizations\n",
        "svc_kernel='rbf' #specify kernel \n",
        "svc_c = 1 #specify c parameter\n",
        "mask_dict = get_mask_data(data_path_dict,0) #get mask_dict\n",
        "mask = make_mask(mask_dict['mask']) #create mask\n",
        "#get subject information\n",
        "subjs_id, subjs_paths = get_subj_information(data_path_dict)\n",
        "#get mask labels to only retrieve time series we care about\n",
        "mask_labels_indices,binary_labels,label_type = get_labels(data_path_dict)\n",
        "###initialize variables to hold results\n",
        "model_dict = {}\n",
        "results = []\n",
        "df_columns = ['subject_id','norm_type']\n",
        "#loop over norm_type to get initialize model_dict norm_type dictionary\n",
        "for norm_type in norm_list:\n",
        "  model_dict[norm_type] = {}\n",
        "#for loop to loop over subjects\n",
        "for idx in range(len(subjs_id)):\n",
        "  subj_id = subjs_id[idx] #get subj_id\n",
        "  subj_path = subjs_paths[idx] #get subj_path\n",
        "  subj_data = access_load_data(subj_path,True) #get subj_data\n",
        "  masked_data = mask_subject_data(subj_data,mask,mask_labels_indices) #mask subject data\n",
        "  #loop over normalizations to do\n",
        "  for norm_type in norm_list:\n",
        "    norm_results = [subj_id,norm_type] #store subject id and normalization strategy in results list\n",
        "    scaled_data = scale_data_single_subj(masked_data,runs_train,runs_test,norm=norm_type) #scale the data for runs listed in runs_lit\n",
        "    clf,X_train,y_train = run_single_subject_svm(scaled_data,runs_train,binary_labels,svc_kernel,svc_c) #run the model\n",
        "    if subj_id in save_subject_data:    #if subject in list of subjects to save\n",
        "      model_dict[norm_type][subj_id] = {} #initialize empty dictionary for the subject data\n",
        "      #store subject variables\n",
        "      model_dict[norm_type][subj_id]['model'] = clf\n",
        "      model_dict[norm_type][subj_id]['X_train'] = X_train\n",
        "      model_dict[norm_type][subj_id]['y_train'] = y_train\n",
        "      model_dict[norm_type][subj_id]['data'] = scaled_data\n",
        "    sub_scores,cols = get_accuracy_scores(clf,scaled_data,X_train,y_train,runs_test,binary_labels) #get accuracy scores for analysis\n",
        "    norm_results.extend(sub_scores) #append to list\n",
        "    results.append(norm_results) #append to results\n",
        "df_columns.extend(cols) #extend data frame columns\n",
        "results_df = pd.DataFrame(results,columns=df_columns) #create df\n",
        "#save results for analysis\n",
        "save_data_path = f'/content/drive/My Drive/data/dataexploration/{svc_kernel}_exploration/rtr_1_2_accuracy_results.csv'\n",
        "results_df.to_csv(save_data_path)\n",
        "#save models for visualization\n",
        "save_models_path = f'/content/drive/My Drive/data/dataexploration/{svc_kernel}_exploration/rtr_1_2_norm_models.pkl'\n",
        "filehandler = open(save_models_path,\"wb\")\n",
        "pickle.dump(model_dict,filehandler)\n",
        "filehandler.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "Uzmswf5uu5uz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "134baebf-e4a2-417e-86f0-42b56f7ab921"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [00:00<00:00,  4.14it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/nilearn/signal.py:83: RuntimeWarning: invalid value encountered in true_divide\n",
            "  signals = (signals - mean_signal) / np.absolute(mean_signal)\n",
            "/usr/local/lib/python3.7/dist-packages/nilearn/signal.py:87: UserWarning: psc standardization strategy is meaningless for features that have a mean of 0. These time series are set to 0.\n",
            "  warnings.warn('psc standardization strategy is meaningless '\n",
            "100%|██████████| 4/4 [00:00<00:00,  4.77it/s]\n",
            "100%|██████████| 4/4 [00:00<00:00,  4.71it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/nilearn/signal.py:83: RuntimeWarning: invalid value encountered in true_divide\n",
            "  signals = (signals - mean_signal) / np.absolute(mean_signal)\n",
            "/usr/local/lib/python3.7/dist-packages/nilearn/signal.py:87: UserWarning: psc standardization strategy is meaningless for features that have a mean of 0. These time series are set to 0.\n",
            "  warnings.warn('psc standardization strategy is meaningless '\n",
            "100%|██████████| 4/4 [00:00<00:00,  4.91it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/nilearn/signal.py:83: RuntimeWarning: invalid value encountered in true_divide\n",
            "  signals = (signals - mean_signal) / np.absolute(mean_signal)\n",
            "/usr/local/lib/python3.7/dist-packages/nilearn/signal.py:87: UserWarning: psc standardization strategy is meaningless for features that have a mean of 0. These time series are set to 0.\n",
            "  warnings.warn('psc standardization strategy is meaningless '\n",
            "100%|██████████| 4/4 [00:00<00:00,  5.79it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/nilearn/signal.py:83: RuntimeWarning: invalid value encountered in true_divide\n",
            "  signals = (signals - mean_signal) / np.absolute(mean_signal)\n",
            "/usr/local/lib/python3.7/dist-packages/nilearn/signal.py:87: UserWarning: psc standardization strategy is meaningless for features that have a mean of 0. These time series are set to 0.\n",
            "  warnings.warn('psc standardization strategy is meaningless '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cv_dict = {} #initialize the results dictionary\n",
        "destination_path = '/content/drive/My Drive/data/dataexploration/cross_validation_results/cv_results.pkl' #where to save data\n",
        "runs_train=['run_02'] #runs we want to train on\n",
        "runs_list=[2,3,4] #runs we want to do cv on\n",
        "runs_test=['run_02','run_03']\n",
        "cv_params = {'C':[1, 5, 10,1000],'kernel':['linear', 'rbf'],'gamma': [.0001, .01, 'auto','scale']} #params we want to test\n",
        "subject_ids,subj_paths = get_subj_information(data_path_dict) #get subject information\n",
        "mask_labels_indices,binary_labels,label_type = get_labels(data_path_dict) #get labels\n",
        "mask_dict = get_mask_data(data_path_dict,0) #get mask dictionary containing mask data\n",
        "mask = make_mask(mask_dict['mask']) #mask we want to use in cv\n",
        "norm_type = 'zscore' #which normalization we want\n",
        "#iterate over subjects and perform cv single subject svm\n",
        "for idx in range(len(subject_ids)):\n",
        "  subj_id = subjs_id[idx] #get subject id\n",
        "  subj_path = subjs_paths[idx] #get path to subject data\n",
        "  subj_data = access_load_data(subj_path,True)  #load subject data\n",
        "  masked_data = mask_subject_data(subj_data,mask,mask_labels_indices) #mask the data\n",
        "  scaled_data = scale_data_single_subj(masked_data,runs_train,runs_test,norm=norm_type) #normalize the data\n",
        "  clf,X_train,y_train = run_single_subject_svm(scaled_data,runs_train,binary_labels,do_cv=True,params=cv_params) #run cross validation\n",
        "  cv_dict[subj_id] = {} #initialize subject dictionary\n",
        "  cv_dict[subj_id]['model'] = clf #save cv model for further analysis\n",
        "  cv_dict[subj_id]['X_train'] = X_train\n",
        "  cv_dict[subj_id]['y_train'] = y_train\n",
        "  cv_dict[subj_id]['run_03'] = scaled_data['run_03']\n",
        "  cv_dict[subj_id]['run_04'] = scaled_data['run_04']\n",
        "#save data\n",
        "filehandler = open(destination_path,\"wb\")\n",
        "pickle.dump(cv_dict,filehandler)\n",
        "filehandler.close() "
      ],
      "metadata": {
        "id": "ddU2mgT1DDhQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "SingleSubjectSVM",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}