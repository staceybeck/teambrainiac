{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yecatstevir/teambrainiac/blob/main/source/DL/Group_3DCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssBXn9lhG9du"
      },
      "source": [
        "# Deep Learning with PyTorch\n",
        "## 3D Convolutional Neural Network on Group Brain fMRI\n",
        "Contributors: Ben Merrill, Stacey Rivet Beck\n",
        "\n",
        "This CNN is implemented in similar form to the fMRI CNN dicussed in the paper by [Wang et al.](https://arxiv.org/pdf/1801.09858.pdf)\n",
        "\n",
        "You can see the architecture of the CNN in the class ConvNet below. Throughout the model training, we used cross-entropy loss and back propogation to update, keeping track of both accuracy and loss at each epoch. Due to RAM limitations, we trained the model from 4 files. Each file was split into two sets of 756 images and used for training. The first run through each of the 8 datasets entailed 10 epochs. The second run-through contained one epoch for each dataset, to avoid overfitting the model.\n",
        "\n",
        "Once the training was complete, the model measured error and trained one epoch on the validation set. Finally, the model runs the test data and returns the predictions and the error. \n",
        "\n",
        "Although this script only shows the training portion of the model building, it could very easily be updated for model validation and testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjOMBc929Vpp"
      },
      "source": [
        "## Importing Dataset and Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWlULVj79acH"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFAZrJHL9a6f"
      },
      "outputs": [],
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/yecatstevir/teambrainiac.git\n",
        "\n",
        "# Change directory into cloned repo DL folder\n",
        "%cd teambrainiac/source/DL\n",
        "\n",
        "# !ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjLsWQoxMLZt"
      },
      "source": [
        "### Load path_config.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJ5q46JZMTPc"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giqBoQS0MWFi"
      },
      "source": [
        "## Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNgdNbLKxASP"
      },
      "outputs": [],
      "source": [
        "# # Possible Missing Packages\n",
        "!pip install boto3\n",
        "!pip install nilearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAhQQqpqMYNT"
      },
      "outputs": [],
      "source": [
        "# General Library Imports\n",
        "import scipy.io\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "import pandas as pd\n",
        "import boto3\n",
        "import tempfile\n",
        "import tqdm\n",
        "import random\n",
        "from path_config import mat_path\n",
        "from botocore.exceptions import ClientError\n",
        "from collections import defaultdict\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# From Local Directory\n",
        "from access_data_dl import *\n",
        "from process_dl import *\n",
        "\n",
        "# Pytroch Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "#import torchvision.transforms as transforms\n",
        "from torch.nn import ReLU, CrossEntropyLoss, Conv3d, Module, Softmax, AdaptiveAvgPool3d\n",
        "from torch.optim import Adam, SGD\n",
        "\n",
        "#from torch.optim import lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mHyNmeLg1YG"
      },
      "source": [
        "## Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDBGSQdyh3Wj"
      },
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ConvNet, self).__init__()\n",
        "    \n",
        "    #Conv1\n",
        "    self.conv1 = nn.Conv3d(in_channels = 1, \n",
        "                           out_channels = 32, \n",
        "                           kernel_size = (1,1,1), \n",
        "                           stride = (1,1,1)\n",
        "                           )\n",
        "    self.bn1 = nn.BatchNorm3d(32)\n",
        "    self.conv2 = nn.Conv3d(in_channels = 32, \n",
        "                           out_channels = 64, \n",
        "                           kernel_size = (7,7,7),\n",
        "                           stride = (2,2,2)\n",
        "                           )\n",
        "    self.bn2 = nn.BatchNorm3d(64)\n",
        "    self.conv3 = nn.Conv3d(in_channels = 64, \n",
        "                           out_channels = 64, \n",
        "                           kernel_size = (3,3,3),\n",
        "                           stride = (2,2,2)\n",
        "                           )\n",
        "    self.bn3 = nn.BatchNorm3d(64)\n",
        "    self.conv4 = nn.Conv3d(in_channels = 64, \n",
        "                           out_channels = 128, \n",
        "                           kernel_size = (3,3,3),\n",
        "                           stride = (2,2,2)\n",
        "                           )\n",
        "    self.bn4 = nn.BatchNorm3d(128) \n",
        "    self.pool1 = nn.AdaptiveAvgPool3d((1,1,1)) #Global Average Pool, takes the average over last two dimensions to flatten \n",
        "  \n",
        "                                                             \n",
        "    # Fully connected layer\n",
        "    self.fc1 = nn.Linear(128,64) # need to find out the size where AdaptiveAvgPool \n",
        "    self.fc2 = nn.Linear(64, 2) # left with 2 for the two classes                     \n",
        "\n",
        "  def forward(self, xb):\n",
        "    xb = self.bn1(F.relu(self.conv1((xb))))\n",
        "    xb = self.bn2(F.relu(self.conv2((xb)))) # Takes a long time\n",
        "    xb = self.bn3(F.relu(self.conv3((xb))))\n",
        "    xb = self.bn4(F.relu(self.conv4((xb))))\n",
        "    xb = self.pool1(xb)\n",
        "    xb = xb.view(xb.shape[:2])\n",
        "    xb = self.fc1(xb)\n",
        "    xb = self.fc2(xb)\n",
        "    return xb      \n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions to Run the Model"
      ],
      "metadata": {
        "id": "N5yPN756rDxN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTAp6i9XReq-"
      },
      "outputs": [],
      "source": [
        "def run_cnn(model, epochs, learning_rate, loss_func, opt, dl, val=False, test=False):\n",
        "  metrics_dict = {}\n",
        "\n",
        "  for epoch in range(1, 1+epochs):\n",
        "    accuracy_list = []\n",
        "    loss_list = []\n",
        "    model.train()\n",
        "    print('epoch', epoch)\n",
        "    for i,(xb, yb) in enumerate(dl):\n",
        "      print('batch', i)\n",
        "\n",
        "      xb = xb.float()\n",
        "      pred = model(xb)\n",
        "      loss_batch = loss_func(pred, yb)\n",
        "      loss_list.append(loss_batch)\n",
        "      accuracy_batch = accuracy(pred, yb)\n",
        "      \n",
        "      # Early Stopping\n",
        "      if int(accuracy_batch) == 1:\n",
        "        print('Perfect Accuracy\\nStopping early to avoid overfitting\\n\\n')\n",
        "        return model, metrics_dict\n",
        "\n",
        "      accuracy_list.append(accuracy_batch)\n",
        "\n",
        "      print('Batch Loss', loss_batch)\n",
        "      print('Batch Accuracy', accuracy_batch)\n",
        "\n",
        "      loss_batch.backward()\n",
        "      opt.step()\n",
        "      opt.zero_grad()\n",
        "      if val == True or test == True:\n",
        "        metrics_dict['preds_'+str(i)] = pred\n",
        "        metrics_dict['labels'] = yb\n",
        "\n",
        "\n",
        "    model.eval()\n",
        "    metrics_dict['epoch_'+str(epoch)] = {'accuracy':accuracy_list, 'loss':loss_list}\n",
        "\n",
        "    print('epoch', epoch, 'finished\\n')\n",
        "    \n",
        "    # Other early stopping criterion\n",
        "    try:\n",
        "      past_epoch_accuracies = [sum(metrics_dict['epoch_'+str(epoch-2)]['accuracy']), sum(metrics_dict['epoch_'+str(epoch-1)]['accuracy'])]\n",
        "      current_epoch_accuracy = sum(metrics_dict['epoch_'+str(epoch)]['accuracy'])\n",
        "      if past_epoch_accuracies[0] > current_epoch_accuracy and past_epoch_accuracies[1] > current_epoch_accuracy:\n",
        "        print('Early stop to avoid overfitting\\nModel accuracies did not decrease for two epochs')\n",
        "        return model, metrics_dict\n",
        "    except:\n",
        "      pass\n",
        "  \n",
        "  return model, metrics_dict\n",
        "  \n",
        "\n",
        "def accuracy(out, yb):\n",
        "    preds = torch.argmax(out, dim=1)\n",
        "    return (preds == yb).float().mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting Up the Model and Parameters"
      ],
      "metadata": {
        "id": "CcCqJFXrAWsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Get model\n",
        "model = ConvNet()\n",
        "model = model.to(device)\n",
        "\n",
        "# Initialize other parameters\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "loss_func = F.cross_entropy\n",
        "opt = torch.optim.Adam(model.parameters(), lr = learning_rate)"
      ],
      "metadata": {
        "id": "e0XguQ-fAdfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decide Train, Validate, or Test\n",
        "Next, decide which part of the pipeline you are running. To not overwhelm ram, please select one of the three options: training, validation, or testing."
      ],
      "metadata": {
        "id": "pLXx3jJYAeG3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train\n",
        "\n",
        "The training data is saved in 4 files. This section loads the recent parameters if needed, loads the training data of choice, and trains the model implemented above."
      ],
      "metadata": {
        "id": "VAvblXwRrJjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load older models for continued training\n",
        "load_recent_model = True\n",
        "train_path = '/content/gdrive/My Drive/cnn_train_2_round_3.pt'\n",
        "\n",
        "if load_recent_model:\n",
        "  model.load_state_dict(torch.load(train_path))"
      ],
      "metadata": {
        "id": "zvoi-dDtIGzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Pick the dataset to train on (1, 2, 3, or 4)\n",
        "train_index = 4\n",
        "\n",
        "# Load the training data\n",
        "path = 'dl/partition_train_%i.pkl'%(train_index)\n",
        "train_images = access_load_data(path, False)"
      ],
      "metadata": {
        "id": "75x9KOwirUyt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTll4mfpFWxJ"
      },
      "outputs": [],
      "source": [
        "# Run the model\n",
        "\n",
        "# Additional hyperparameters\n",
        "n_train_set_images = int(train_images['images'].shape[0])\n",
        "n_train_dataset_portions = 2\n",
        "bs = 54\n",
        "\n",
        "metrics_dict = {}\n",
        "for i,image_index in enumerate(range(0, n_train_set_images, int(n_train_set_images/n_train_dataset_portions))):\n",
        "\n",
        "  x_train = train_images['images'][image_index:image_index+n_train_dataset_portions]\n",
        "  y_train = train_images['labels'][image_index:image_index+n_train_dataset_portions]\n",
        "\n",
        "  ds = TensorDataset(x_train, y_train)\n",
        "  dl = DataLoader(ds, batch_size = bs, shuffle=True)\n",
        "\n",
        "  model, metrics = run_cnn(model, epochs, learning_rate, loss_func, opt, dl)\n",
        "  metrics_dict['round_'+str(i)] = metrics\n",
        "  \n",
        "  metrics_path = \"/content/gdrive/My Drive/metrics_dict_train_%i_%i.pkl\"%(train_index, i)\n",
        "  f = open(metrics_path,\"wb\")\n",
        "  pickle.dump(metrics_dict,f)\n",
        "\n",
        "  print('Saving model')\n",
        "  model_path = F'/content/gdrive/My Drive/cnn_train%i_%i.pt'%(train_index, i)\n",
        "  torch.save(model.state_dict(), path)\n",
        "\n",
        "  print('Finshed with set', str(i), 'of' + n_train_dataset_portions + 'images\\nStarting next set.\\n\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation\n",
        "Validation trains one epoch, and returns predictions and accuracies before back propogation."
      ],
      "metadata": {
        "id": "S96YnR2f8zYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "model_path = '/content/gdrive/My Drive/cnn_train_complete.pt'\n",
        "model.load_state_dict(torch.load(model_path))"
      ],
      "metadata": {
        "id": "A5ShwhgE81PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%time\n",
        "# Load validation data\n",
        "data_path = 'dl/partition_val.pkl'\n",
        "val_images = access_load_data(data_path, False)\n",
        "val_images['images'].shape"
      ],
      "metadata": {
        "id": "ZNYNXAjuElpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 84\n",
        "epochs = 1\n",
        "\n",
        "x_val = val_images['images']\n",
        "y_val = val_images['labels']\n",
        "\n",
        "ds = TensorDataset(x_val, y_val)\n",
        "dl = DataLoader(ds, batch_size = bs)\n",
        "\n",
        "model, val_metrics = run_cnn(model, epochs, learning_rate, loss_func, opt, dl, val=True)\n",
        "\n",
        "metrics_path = \"/content/gdrive/My Drive/metrics_dict_val.pkl\"\n",
        "f = open(metrics_path,\"wb\")\n",
        "pickle.dump(val_metrics,f)\n",
        "\n",
        "print('Saving model')\n",
        "model_path = F'/content/gdrive/My Drive/cnn_val.pt'\n",
        "torch.save(model.state_dict(), model_path)\n",
        "\n",
        "print('Finshed with validation set')"
      ],
      "metadata": {
        "id": "5Xo6N5En81Md"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test"
      ],
      "metadata": {
        "id": "AVYLO34R8zQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "model_path = '/content/gdrive/My Drive/cnn_val.pt'\n",
        "model.load_state_dict(torch.load(model_path))"
      ],
      "metadata": {
        "id": "i8xph-EL83mZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Load validation data\n",
        "test_partition_n = 2\n",
        "data_path = 'dl/partition_test_%i.pkl'%(test_partition_n)\n",
        "test_images = access_load_data(data_path, False)\n",
        "test_images['images'].shape"
      ],
      "metadata": {
        "id": "Y-i8ekaQ83I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 84\n",
        "epochs = 1\n",
        "\n",
        "x_test = test_images['images']\n",
        "y_test = test_images['labels']\n",
        "\n",
        "ds = TensorDataset(x_test, y_test)\n",
        "dl = DataLoader(ds, batch_size = bs)\n",
        "\n",
        "model, val_metrics = run_cnn(model, epochs, learning_rate, loss_func, opt, dl, test=True)\n",
        "\n",
        "metrics_path = \"/content/gdrive/My Drive/metrics_dict_test_%i.pkl\"%(test_partition_n)\n",
        "f = open(metrics_path,\"wb\")\n",
        "pickle.dump(val_metrics,f)\n",
        "\n",
        "print('Finshed with test set', test_partition_n)"
      ],
      "metadata": {
        "id": "gL3tZKKG83Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_path = \"/content/gdrive/My Drive/metrics_dict_test_%i.pkl\"%(test_partition_n)\n",
        "f = open(metrics_path,\"wb\")\n",
        "pickle.dump(val_metrics,f)"
      ],
      "metadata": {
        "id": "sWicfjWb83EX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwmpLfSLKtaZ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "name": "Group_3DCNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
