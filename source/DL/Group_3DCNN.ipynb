{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Group_3DCNN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yecatstevir/teambrainiac/blob/main/source/DL/Group_3DCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning with PyTorch\n",
        "## 3D Convolutional Neural Network on Group Brain fMRI\n",
        "Contributors: Stacey Rivet Beck, Ben Merrill\n",
        "### To Do:\n",
        "- Either:\n",
        "  - Get Raw data in 4D   \n",
        "          - OR -\n",
        "  - Reshape Raw data into 4D from 2D\n",
        "    - Apply Whole Brain Mask to data and save to AWS\n",
        "\n",
        "- Build Dataloader: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "\n",
        "- Implement 3DCNN from paper: REALLY GREAT PAPERS\n",
        "  - Nguyen et al. http://proceedings.mlr.press/v136/nguyen20a/nguyen20a.pdf\n",
        "  - Wang et al. https://arxiv.org/pdf/1801.09858.pdf (discusses more in detail the input data shapes and processing)\n",
        "  \n",
        "  - Inputs: 84@ x * y * z ; one fmri time series at a time, not concatenated\n",
        "  - Basic Architecture:\n",
        "        #First layer is generating temporal descriptors of the voxels\n",
        "        Conv1 1 x 1 x 1 filter, output = 32, stride = 1, ReLU, BatchNorm\n",
        "        Conv2 7 x 7 x 7 filter, output = 64, stride = 2, ReLU, BatchNorm\n",
        "        Conv3 3 x 3 x 3 fitler, output = 64, stride = 2, ReLU, BatchNorm\n",
        "        Conv4 3 x 3 x 3 fitler, output = 128, stride = 2, ReLU, BatchNorm\n",
        "        Global Average Pooling on final feature maps ->\n",
        "        Flattened maps size 128?\n",
        "        Fully connected layer size 64\n",
        "        Fully connected layer size 2 (2 way classification, one for each class) -> softmax\n",
        "\n",
        "        Optimized with Adam, standard parameters (β1=0.9 and β2=0.999)\n",
        "        Batched at 32, but we may need to batch smaller due to GPU compute\n",
        "        Learning Rate = 0.001, gradual decay after Val loss plateaued after 15 epochs\n",
        "        Cross entropy Loss\n",
        "        Employ early stopping\n",
        "        In Wang et al. they used data for visualization, same size as input data, though are reduced in time dimension to be mapped on fsaverage surface. \n",
        "        \n",
        "  "
      ],
      "metadata": {
        "id": "ssBXn9lhG9du"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Dataset and Labels"
      ],
      "metadata": {
        "id": "bjOMBc929Vpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWlULVj79acH",
        "outputId": "b72fc650-423b-481e-96e3-2bbc76191b60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/yecatstevir/teambrainiac.git\n",
        "# Change directory into cloned repo\n",
        "%cd teambrainiac/source/DL\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFAZrJHL9a6f",
        "outputId": "93a27c6d-1aff-44f0-a265-d642d698d504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'teambrainiac'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 909, done.\u001b[K\n",
            "remote: Counting objects: 100% (909/909), done.\u001b[K\n",
            "remote: Compressing objects: 100% (676/676), done.\u001b[K\n",
            "remote: Total 909 (delta 572), reused 428 (delta 217), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (909/909), 75.12 MiB | 24.01 MiB/s, done.\n",
            "Resolving deltas: 100% (572/572), done.\n",
            "/content/teambrainiac/source/teambrainiac/source/DL/teambrainiac/source/DL\n",
            "Group_3DCNN.ipynb  process_dl.py  utils_dl.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load path_config.py"
      ],
      "metadata": {
        "id": "YjLsWQoxMLZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "NJ5q46JZMTPc",
        "outputId": "af7b46d0-82e7-493f-f381-29a3996a667a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1a1419e0-6563-4133-9a0a-1a2e69ca1cad\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1a1419e0-6563-4133-9a0a-1a2e69ca1cad\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving path_config.py to path_config.py\n",
            "User uploaded file \"path_config.py\" with length 196 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Packages"
      ],
      "metadata": {
        "id": "giqBoQS0MWFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# General Library Imports\n",
        "import re\n",
        "import scipy.io\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "import pandas as pd\n",
        "import boto3\n",
        "import tempfile\n",
        "import tqdm\n",
        "from path_config import mat_path\n",
        "from botocore.exceptions import ClientError\n",
        "from collections import defaultdict\n",
        "\n",
        "# From Local Directory\n",
        "from utils_dl import *\n",
        "from process_dl import *\n",
        "\n",
        "# Pytroch Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "\n",
        "#import torchvision.transforms as transforms\n",
        "from torch.nn import ReLU, CrossEntropyLoss, Conv3d, Module, Softmax, AdaptiveAvgPool3d\n",
        "from torch.optim import Adam, SGD\n",
        "\n",
        "#from torch.optim import lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "lAhQQqpqMYNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Group fMRI Data, Normalize, and Create Masks"
      ],
      "metadata": {
        "id": "eYG_gfx5sDHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Mask and Open data_path_dict\n",
        "\n",
        "# def open_pickle(file_path):\n",
        "#     \"\"\"\n",
        "#     :param file path for dictionary\n",
        "#     :return dictionary:\n",
        "#     \"\"\"\n",
        "#     f = open(file_path, \"rb\")\n",
        "#     dictionary = pickle.load(f)\n",
        "#     f.close()\n",
        "\n",
        "#     return dictionary\n",
        "\n",
        "\n",
        "# def get_mask(mask_type,data_path_dict,mask_ind):\n",
        "#   \"\"\"\n",
        "#     Function to return the mask of what brain voxels we want to include in analysis\n",
        "#     Params:\n",
        "#       data_path_dict  : dictionary containing paths to all data stored on AWS\n",
        "#       mask_type: name of mask we want to use\n",
        "#       mask_ind: index of where the path to the masks are 0: full brain mask plus masks that subtract region\n",
        "#               1: Regions of interest(ROIs) mask out full brain except structure we care about\n",
        "#   \"\"\"\n",
        "#   mask_data_filepath = data_path_dict['mask_data'][mask_ind] #path to masked data     \n",
        "#   mask_type_dict = access_load_data(mask_data_filepath, True) #get the mask data dictionary\n",
        "#   np_array_mask = mask_type_dict[mask_type] #get the mask array\n",
        "#   mask = np.ma.make_mask(np_array_mask).reshape(79*95*79,order='F') #create a 1-D array for the mask. Important to use Fourier Transformation as we are working in brain space!\n",
        "\n",
        "#   return mask\n",
        "\n",
        "# # def access_aws():\n",
        "# #     \"\"\"\n",
        "# #     :return:\n",
        "# #     \"\"\"\n",
        "\n",
        "# #     # Acces AWS S3 MATLAB file\n",
        "# #     pubkey = mat_path['ACCESS_KEY']\n",
        "# #     seckey = mat_path['SECRET_KEY']\n",
        "# #     client = boto3.client('s3', aws_access_key_id = pubkey, aws_secret_access_key = seckey)\n",
        "# #     s3 = boto3.resource('s3', aws_access_key_id = pubkey, aws_secret_access_key = seckey)\n",
        "# #     bucket = s3.Bucket('teambrainiac')\n",
        "# #     bucket_ = bucket.name\n",
        "# #     obj_name = list(bucket.objects.all())\n",
        "\n",
        "# #     return obj_name, bucket_, client\n",
        "\n",
        "\n",
        "\n",
        "# # def load_mat(path):\n",
        "# #     \"\"\"\n",
        "# #     :param .mat data path\n",
        "# #     uses scipy to convert access to mat file in python\n",
        "# #     :return mat data\n",
        "# #     \"\"\"\n",
        "# #     mat_file = scipy.io.loadmat(path)\n",
        "# #     return mat_file\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def labels_mask_binary(data_path_dict, label_type='rt_labels'):\n",
        "#     \"\"\"\n",
        "#     \"\"\"\n",
        "#     label_path = data_path_dict['labels'][0]\n",
        "#     label_data_dict = access_load_data(label_path, True)\n",
        "#     labels = label_data_dict[label_type].T[0]\n",
        "#     image_label_mask = np.array([bool(x!=9999) for x in labels])\n",
        "#     image_labels = labels[image_label_mask]\n",
        "\n",
        "#     return image_label_mask, image_labels\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def access_load_data(obj, bool_mat):\n",
        "#     \"\"\"\n",
        "#     :param data_file: file path to data. For example can be pickle file containing a dictionary\n",
        "#                       or csv file path to load as a dataframe, e.g. file_names_dict['subject_data'][0]\n",
        "#                       or nifti file path to load as nifti object\n",
        "#                       or load a matlab file if bool_mat == TRUE\n",
        "#     :param bool_mat:  if true will run load_mat() and return .mat file\n",
        "#     :return        :  will open matlab data if bool_mat == True,\n",
        "#                       pickle file\n",
        "#                       csv file as a dataframe\n",
        "#                       nifti file\n",
        "#     \"\"\"\n",
        "\n",
        "#     # Connect to AWS client\n",
        "#     _, bucket_, client = access_aws()\n",
        "\n",
        "#     # Create a temporary file\n",
        "#     temp = tempfile.NamedTemporaryFile()\n",
        "\n",
        "#     # Download data in temp file\n",
        "#     client.download_file(bucket_, obj, temp.name)\n",
        "\n",
        "#     if bool_mat == True:\n",
        "#         data = load_mat(temp.name)\n",
        "#     else:\n",
        "#         if '.pkl' in obj:\n",
        "#             data = open_pickle(temp.name)\n",
        "#         elif '.csv' in obj:\n",
        "#             data = pd.read_csv(temp.name)\n",
        "#         elif '.nii' in obj:\n",
        "#             temp_f = 'data/temp.nii'\n",
        "#             client.download_file(bucket_, obj, temp_f)\n",
        "#             data = data_to_nib(temp_f)\n",
        "\n",
        "#     temp.close()\n",
        "\n",
        "#     return data\n",
        "\n",
        "\n",
        "\n",
        "# def load_subjects_chronologically(data_path_dict, n_subjects, image_label_mask, image_labels, label_type='rt_labels', runs=[2, 3]):\n",
        "#   '''\n",
        "#     Function to load subject data. This deletes images with no labels and returns only the runs of interest for each subject.\n",
        "#     Params:\n",
        "#       data_path_dict  : dictionary that has paths to data on AWS\n",
        "#       n_subjects      : the number of subjects\n",
        "#       image_label_mask: a mask indicating whether a binary label exists for each image in a run\n",
        "#       image_labels    : binary labels indicating whether a subject was up or down regulating \n",
        "#       label_type      : the type of label to return from the labels file in AWS \n",
        "#       runs            : a list of runs to return from each subject\n",
        "\n",
        "#     returns: dictionary of users and their runs\n",
        "#   '''\n",
        "#   # Load subject Ids\n",
        "#   subject_paths = data_path_dict['subject_data'][:n_subjects]\n",
        "#   subject_ids = data_path_dict['subject_ID'][:n_subjects]\n",
        "#   subjects = {}\n",
        "\n",
        "#   for path,id in tqdm.tqdm(zip(subject_paths, subject_ids)):\n",
        "#     subject_dict = {}\n",
        "#     data = access_load_data(path,True)\n",
        "#     subject_dict['image_labels'] = image_labels\n",
        "\n",
        "#     for run in runs:\n",
        "#       run_key = 'run_0' + str(run) + '_vec'\n",
        "#       run_masked = data[run_key][image_label_mask]\n",
        "#       subject_dict['run_'+str(run)] = run_masked\n",
        "    \n",
        "#     subjects[id] = subject_dict\n",
        "  \n",
        "#   return subjects"
      ],
      "metadata": {
        "id": "whTNueKLM01r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Open path dictionary file to get subject ids\n",
        "path = \"../data/data_path_dictionary.pkl\"\n",
        "data_path_dict = open_pickle(path)"
      ],
      "metadata": {
        "id": "wWYt1ci4sTgk",
        "outputId": "264a219a-a1da-4dec-ca25-8b2734cb7c7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-63d1d3387e72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Open path dictionary file to get subject ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../data/data_path_dictionary.pkl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdata_path_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/teambrainiac/source/teambrainiac/source/DL/utils_dl.py\u001b[0m in \u001b[0;36mopen_pickle\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \"\"\"\n\u001b[1;32m      8\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix hyperparameters for preprocessing\n",
        "label_type='rt_labels'\n",
        "n_subjects = 5\n",
        "runs_list = [2]\n",
        "\n",
        "\n",
        "# Run functions to import unmasked data\n",
        "image_label_mask, image_labels = labels_mask_binary(data_path_dict, label_type='rt_labels')\n",
        "\n",
        "five_subjects_run_2 = load_subjects_chronologically(data_path_dict, n_subjects, image_label_mask, image_labels, label_type, runs_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_A6VDQpeGZW",
        "outputId": "7ade4d3b-3a4d-43d3-835b-27acddd285ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "5it [00:58, 11.71s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "five_subjects_run_2.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnuQfrN2RAZa",
        "outputId": "510430f8-a907-48d8-c4c1-ba07033b8ea0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['10004_08693', '10008_09924', '10009_08848', '10016_09694', '10017_08894'])"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "five_subjects_run_2['10004_08693']\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTzFzdv6Nka2",
        "outputId": "62b4fedd-4cfc-473a-d36c-db11ff51db14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'image_labels': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0], dtype=uint16),\n",
              " 'run_2': array([[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]], dtype=int16)}"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Mask Indicies. Note that 'mask' is the string for a whole brain mask.\n",
        "whole_mask = get_mask('mask', data_path_dict, mask_ind=0)\n",
        "whole_mask\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pq1tvL5AOOKj",
        "outputId": "2bdc29f8-6b96-4c1c-fb08-daf6329fc60c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([False, False, False, ..., False, False, False])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Dataloader"
      ],
      "metadata": {
        "id": "sqW1TnYdgyYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "        image = read_image(img_path)\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "vW0vgmmVg1Fv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Model"
      ],
      "metadata": {
        "id": "7mHyNmeLg1YG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ConvNet, self).__init__()\n",
        "    \n",
        "    #Conv1\n",
        "    self.conv1 = nn.Conv3d(in_channels = 1, \n",
        "                           out_channels = 32, \n",
        "                           kernel_size = 1, \n",
        "                           stride = 1\n",
        "                           )\n",
        "    self.bn1 = nn.BatchNorm3d(32)\n",
        "    self.conv2 = nn.Conv3d(in_channels = 32, \n",
        "                           out_channels = 64, \n",
        "                           kernel_size = 7, \n",
        "                           stride = 2\n",
        "                           )\n",
        "    self.bn2 = nn.BatchNorm3d(64)\n",
        "    self.conv3 = nn.Conv3d(in_channels = 64, \n",
        "                           out_channels = 64, \n",
        "                           kernel_size = 3, \n",
        "                           stride = 2\n",
        "                           )\n",
        "    self.bn3 = nn.BatchNorm3d(64)\n",
        "    self.conv4 = nn.Conv3d(in_channels = 64, \n",
        "                           out_channels = 128, \n",
        "                           kernel_size = 3, \n",
        "                           stride = 2\n",
        "                           )\n",
        "    self.bn4 = nn.BatchNorm3d(128) \n",
        "    self.pool1 = nn.AdaptiveAvgPool3d((1,1,1)) #Global Average Pool, takes the average over last two dimensions to flatten \n",
        "  \n",
        "                                                             \n",
        "    # Fully connected layer\n",
        "    self.fc1 = nn.Linear(128*?*?,64) # need to find out the size where AdaptiveAvgPool \n",
        "    self.fc2 = nn.Linear(64, 2) # left with 2 for the two classes                     \n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.bn1(F.relu(self.conv1((x))))\n",
        "    x = self.bn2(F.relu(self.conv2((x))))\n",
        "    x = self.bn3(F.relu(self.conv3((x))))\n",
        "    x = self.pool1(self.bn4(F.relu(self.conv4((x)))))\n",
        "    x = self.fc1(x)\n",
        "    x = F.softmax(self.fc2(x))\n",
        "    return x                        \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Set to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Get model\n",
        "model = ConvNet()\n",
        "model = model.to(device)\n",
        "print(\"First model training on GPU\")\n",
        "print(model)\n",
        "\n",
        "# Initialize other parameters\n",
        "epochs = 25 #120\n",
        "learning_rate = 0.001\n",
        "criterion = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)#, momentum = 0.9) #or ADAM/ momentum"
      ],
      "metadata": {
        "id": "UDBGSQdyh3Wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "kLzXUn9re04K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_stats = {\n",
        "    'train': [],\n",
        "    'val': []\n",
        "  }\n",
        "\n",
        "print(accuracy_stats)\n",
        "\n",
        "loss_stats = {\n",
        "    'train': [],\n",
        "    'val': []\n",
        "    }\n",
        "print(loss_stats)\n",
        "\n",
        "\n",
        "\n",
        "def train_val_model(epochs):\n",
        "  for epoch in range(1, epochs + 1):\n",
        "\n",
        "    # TRAINING *****************************************************************\n",
        "\n",
        "    train_epoch_loss = 0\n",
        "    train_epoch_acc = 0\n",
        "\n",
        "    # set model in training mode \n",
        "    model.train()\n",
        "    print('\\nEpoch$ : %d'%epoch)\n",
        "    for x_train_batch, y_train_batch in tqdm(train_loader):\n",
        "      x_train_batch = x_train_batch.to(device)#(float).to(device) # for GPU support\n",
        "      y_train_batch = y_train_batch.to(device) \n",
        "\n",
        "      #print(x_train_batch.shape)\n",
        "\n",
        "      # sets gradients to 0 to prevent interference with previous epoch\n",
        "      optimizer.zero_grad()\n",
        "    \n",
        "      # Forward pass through NN\n",
        "      y_train_pred = model(x_train_batch)#.to(float)\n",
        "      train_loss = criterion(y_train_pred, y_train_batch)\n",
        "      train_acc = accuracy(y_train_pred, y_train_batch)\n",
        "\n",
        "      # Backward pass, updating weights\n",
        "      train_loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Statistics\n",
        "      train_epoch_loss += train_loss.item()\n",
        "      train_epoch_acc += train_acc.item()\n",
        "\n",
        "\n",
        "    # VALIDATION****************************************************************   \n",
        "    \n",
        "    with torch.set_grad_enabled(False):\n",
        "      val_epoch_loss = 0\n",
        "      val_epoch_acc = 0\n",
        "\n",
        "      model.eval()\n",
        "      for x_val_batch, y_val_batch in tqdm(validate_loader):\n",
        "      \n",
        "        x_val_batch =  x_val_batch.to(device)#.to(float)\n",
        "        y_val_batch = y_val_batch.to(device)\n",
        "            \n",
        "        # Forward pass\n",
        "        y_val_pred = model(x_val_batch)#.to(float)   \n",
        "        val_loss = criterion(y_val_pred, y_val_batch)\n",
        "        val_acc = accuracy(y_val_pred, y_val_batch)\n",
        "            \n",
        "        val_epoch_loss += val_loss.item()\n",
        "        val_epoch_acc += val_acc.item()\n",
        "\n",
        "    # Prevent plateauing validation loss \n",
        "    #scheduler.step(val_epoch_loss/len(validate_loader))\n",
        "\n",
        "        \n",
        "    loss_stats['train'].append(train_epoch_loss/len(train_loader))\n",
        "    loss_stats['val'].append(val_epoch_loss/len(validate_loader))\n",
        "    accuracy_stats['train'].append(train_epoch_acc/len(train_loader))\n",
        "    accuracy_stats['val'].append(val_epoch_acc/len(validate_loader))\n",
        "                              \n",
        "    \n",
        "    print(f'Epoch {epoch+0:03}: Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(validate_loader):.5f}') \n",
        "    print(f'Train Acc: {train_epoch_acc/len(train_loader):.3f} | Val Acc: {val_epoch_acc/len(validate_loader):.3f}')\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def accuracy(y_pred, y_test):\n",
        "  # Calculating model accuracy at each epoch \n",
        "  y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
        "  _, y_pred_prob = torch.max(y_pred_softmax, dim = 1)\n",
        "  correct_pred = (y_pred_prob == y_test).float()\n",
        "  acc = correct_pred.sum() / len(correct_pred)\n",
        "  acc = torch.round(acc * 100)\n",
        "\n",
        "  return acc\n",
        "\n",
        "\n",
        "\n",
        "     \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  train_val_model(epochs)"
      ],
      "metadata": {
        "id": "66UUnIhJevn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6EBUAl0ceO4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def open_pickle(file_path):\n",
        "    \"\"\"\n",
        "    :param file path for dictionary\n",
        "    :return dictionary:\n",
        "    \"\"\"\n",
        "    f = open(file_path, \"rb\")\n",
        "    dictionary = pickle.load(f)\n",
        "    f.close()\n",
        "\n",
        "    return dictionary\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def access_load_data(obj, bool_mat):\n",
        "    \"\"\"\n",
        "    :param data_file: file path to data. For example can be pickle file containing a dictionary\n",
        "                      or csv file path to load as a dataframe, e.g. file_names_dict['subject_data'][0]\n",
        "                      or nifti file path to load as nifti object\n",
        "                      or load a matlab file if bool_mat == TRUE\n",
        "    :param bool_mat:  if true will run load_mat() and return .mat file\n",
        "    :return        :  will open matlab data if bool_mat == True,\n",
        "                      pickle file\n",
        "                      csv file as a dataframe\n",
        "                      nifti file\n",
        "    \"\"\"\n",
        "\n",
        "    # Connect to AWS client\n",
        "    _, bucket_, client = access_aws()\n",
        "\n",
        "    # Create a temporary file\n",
        "    temp = tempfile.NamedTemporaryFile()\n",
        "\n",
        "    # Download data in temp file\n",
        "    client.download_file(bucket_, obj, temp.name)\n",
        "\n",
        "    if bool_mat == True:\n",
        "        data = load_mat(temp.name)\n",
        "    else:\n",
        "        if '.pkl' in obj:\n",
        "            data = open_pickle(temp.name)\n",
        "        elif '.csv' in obj:\n",
        "            data = pd.read_csv(temp.name)\n",
        "        elif '.nii' in obj:\n",
        "            temp_f = 'data/temp.nii'\n",
        "            client.download_file(bucket_, obj, temp_f)\n",
        "            data = data_to_nib(temp_f)\n",
        "\n",
        "    temp.close()\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def s3_upload(data, object_name, data_type):\n",
        "    \"\"\"Upload a file to an S3 bucket\n",
        "    :param data: our data to upload\n",
        "    :param data_type: type of data file we are creating\n",
        "    :param object_name: S3 object name. If not specified then name of temp.name is used\n",
        "    :return: True if file was uploaded, else False\n",
        "    \"\"\"\n",
        "\n",
        "    # Upload the file\n",
        "    # Connect to AWS client\n",
        "    _, bucket_name, client = access_aws()\n",
        "\n",
        "    try:\n",
        "\n",
        "        with tempfile.NamedTemporaryFile(delete=False) as temp:\n",
        "            if data_type == \"pickle\":\n",
        "                pickle.dump(data, temp, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "            elif data_type == \"numpy\":\n",
        "                np.save(temp, data)\n",
        "                _ = temp.seek(0)\n",
        "\n",
        "            elif data_type == \"csv\":\n",
        "                data.to_csv(temp, index=False)\n",
        "\n",
        "            client.upload_file(temp.name, bucket_name, object_name)\n",
        "            temp.close()\n",
        "            print(f\"upload complete for {object_name}\")\n",
        "\n",
        "        if data_type == \"nifti\":\n",
        "            tempf = 'data/upload_temp.nii'\n",
        "            nib.save(data, tempf)\n",
        "            client.upload_file(tempf, bucket_name, object_name)\n",
        "            print(f\"upload complete for {object_name}\")\n",
        "\n",
        "    except ClientError as e:\n",
        "        logging.error(e)\n",
        "        return False\n",
        "\n",
        "\n",
        "    return True\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_mask_indices(data_paths, mask_type, m_path_ind):\n",
        "    \"\"\"\n",
        "    :param data_paths:\n",
        "    :param mask_type:\n",
        "    :param m_path_ind:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    mask_data_path = data_paths['mask_data'][m_path_ind]\n",
        "    mask_type_dict = access_load_data(mask_data_path, True)\n",
        "    np_array_mask = mask_type_dict[mask_type]\n",
        "    print(\"mask shape:\", np_array_mask.shape)\n",
        "    np_compatible_mask = np.ma.make_mask(np_array_mask).reshape(79 * 95 * 79, order='F')\n",
        "    indices_mask = np.where(\n",
        "        np_compatible_mask == 1)  # gets the indices where the mask is 1, the brain region for x, y, z planes\n",
        "\n",
        "    return indices_mask"
      ],
      "metadata": {
        "id": "rJGJUdiBKtnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2FyCVpQ6KtkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HKkISUtQKthJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OCzbRyCiKteE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RwmpLfSLKtaZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}