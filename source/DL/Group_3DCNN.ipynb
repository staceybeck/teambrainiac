{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yecatstevir/teambrainiac/blob/main/source/DL/Group_3DCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssBXn9lhG9du"
      },
      "source": [
        "# Deep Learning with PyTorch\n",
        "## 3D Convolutional Neural Network on Group Brain fMRI\n",
        "Contributors: Stacey Rivet Beck, Ben Merrill\n",
        "\n",
        "- Implement 3DCNN from paper: REALLY GREAT PAPERS\n",
        "  - Nguyen et al. http://proceedings.mlr.press/v136/nguyen20a/nguyen20a.pdf\n",
        "  - Wang et al. https://arxiv.org/pdf/1801.09858.pdf (discusses more in detail the input data shapes and processing)\n",
        "  \n",
        "  - Inputs: 84@ x * y * z ; one fmri time series at a time, not concatenated\n",
        "  - Basic Architecture:\n",
        "        First layer is generating temporal descriptors of the voxels\n",
        "        Conv1 1 x 1 x 1 filter, output = 32, stride = 1, ReLU, BatchNorm\n",
        "        Conv2 7 x 7 x 7 filter, output = 64, stride = 2, ReLU, BatchNorm\n",
        "        Conv3 3 x 3 x 3 fitler, output = 64, stride = 2, ReLU, BatchNorm\n",
        "        Conv4 3 x 3 x 3 fitler, output = 128, stride = 2, ReLU, BatchNorm\n",
        "        Global Average Pooling on final feature maps ->\n",
        "        Flattened maps size 128?\n",
        "        Fully connected layer size 64\n",
        "        Fully connected layer size 2 (2 way classification, one for each class) -> softmax\n",
        "\n",
        "        Optimized with Adam, standard parameters (β1=0.9 and β2=0.999)\n",
        "        Batched at 32, but we may need to batch smaller due to GPU compute\n",
        "        Learning Rate = 0.001, gradual decay after Val loss plateaued after 15 epochs\n",
        "        Cross entropy Loss\n",
        "        Employ early stopping\n",
        "        In Wang et al. they used data for visualization, same size as input data, though are reduced in time dimension to be mapped on fsaverage surface. \n",
        "        \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjOMBc929Vpp"
      },
      "source": [
        "## Importing Dataset and Labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWlULVj79acH",
        "outputId": "a8a73842-734a-4faf-cf8b-67bfaa739c2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFAZrJHL9a6f",
        "outputId": "94064b78-92c2-45eb-9d7c-e04f0aada684"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'teambrainiac'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 1602, done.\u001b[K\n",
            "remote: Counting objects: 100% (1602/1602), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1266/1266), done.\u001b[K\n",
            "remote: Total 1602 (delta 1041), reused 627 (delta 319), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1602/1602), 88.32 MiB | 11.70 MiB/s, done.\n",
            "Resolving deltas: 100% (1041/1041), done.\n",
            "/content/teambrainiac/source/DL\n"
          ]
        }
      ],
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/yecatstevir/teambrainiac.git\n",
        "\n",
        "# Change directory into cloned repo DL folder\n",
        "%cd teambrainiac/source/DL\n",
        "\n",
        "# !ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjLsWQoxMLZt"
      },
      "source": [
        "### Load path_config.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "NJ5q46JZMTPc",
        "outputId": "e829ea74-983d-4b2b-a8fe-1c8fa003a387"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b8914495-d624-4d8d-b2ae-22ee325d8158\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b8914495-d624-4d8d-b2ae-22ee325d8158\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving path_config.py to path_config.py\n",
            "User uploaded file \"path_config.py\" with length 196 bytes\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giqBoQS0MWFi"
      },
      "source": [
        "## Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MNgdNbLKxASP",
        "outputId": "5c0e8c72-3553-4c51-d6b0-1bff4d839c37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.21.42-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.2 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting botocore<1.25.0,>=1.24.42\n",
            "  Downloading botocore-1.24.42-py3-none-any.whl (8.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.7 MB 40.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.25.0,>=1.24.42->boto3) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 59.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.42->boto3) (1.15.0)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.9 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.21.42 botocore-1.24.42 jmespath-1.0.0 s3transfer-0.5.2 urllib3-1.26.9\n",
            "Collecting nilearn\n",
            "  Downloading nilearn-0.9.1-py3-none-any.whl (9.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.6 MB 3.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.0.2)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from nilearn) (4.2.6)\n",
            "Requirement already satisfied: nibabel>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from nilearn) (3.0.2)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.3.5)\n",
            "Requirement already satisfied: requests>=2 in /usr/local/lib/python3.7/dist-packages (from nilearn) (2.23.0)\n",
            "Requirement already satisfied: joblib>=0.15 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.1.0)\n",
            "Collecting scipy>=1.5\n",
            "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.1 MB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.7/dist-packages (from nilearn) (1.21.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->nilearn) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->nilearn) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0->nilearn) (1.15.0)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 47.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2->nilearn) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22->nilearn) (3.1.0)\n",
            "Installing collected packages: urllib3, scipy, nilearn\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.9\n",
            "    Uninstalling urllib3-1.26.9:\n",
            "      Successfully uninstalled urllib3-1.26.9\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed nilearn-0.9.1 scipy-1.7.3 urllib3-1.25.11\n"
          ]
        }
      ],
      "source": [
        "# Possible Missing Packages\n",
        "!pip install boto3\n",
        "!pip install nilearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "lAhQQqpqMYNT"
      },
      "outputs": [],
      "source": [
        "# General Library Imports\n",
        "import re\n",
        "import scipy.io\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "import pandas as pd\n",
        "import boto3\n",
        "import tempfile\n",
        "import tqdm\n",
        "import random\n",
        "from path_config import mat_path\n",
        "from botocore.exceptions import ClientError\n",
        "from collections import defaultdict\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# From Local Directory\n",
        "from access_data_dl import *\n",
        "from process_dl import *\n",
        "\n",
        "# Pytroch Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "#import torchvision.transforms as transforms\n",
        "from torch.nn import ReLU, CrossEntropyLoss, Conv3d, Module, Softmax, AdaptiveAvgPool3d\n",
        "from torch.optim import Adam, SGD\n",
        "\n",
        "#from torch.optim import lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYG_gfx5sDHc"
      },
      "source": [
        "## Import Group fMRI Data from AWS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWYt1ci4sTgk"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "path = 'dl/partition_train_2.pkl'\n",
        "train_images = access_load_data(path, False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_images['images'].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Im_zt3J0qzAT",
        "outputId": "40dfaec9-cb70-404e-bc35-dcc2870d9c19"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1512, 1, 79, 95, 79])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Test Batch\n",
        "Use this code only if you are changing the model based on a subset of the data"
      ],
      "metadata": {
        "id": "ePdioNH3rrVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_batch_size = 18\n",
        "\n",
        "# x = train_images['images'][:train_batch_size]\n",
        "# y = train_images['labels'][:train_batch_size]"
      ],
      "metadata": {
        "id": "wRBfEmyZru0d"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqW1TnYdgyYn"
      },
      "source": [
        "## Build Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IZPwPzibTN00"
      },
      "outputs": [],
      "source": [
        "# May move this into the training bit\n",
        "# bs = 150\n",
        "\n",
        "# x_train = train_images['images']\n",
        "# y_train = train_images['labels']\n",
        "\n",
        "# ds = TensorDataset(x_train, y_train)\n",
        "# dl = DataLoader(ds, batch_size = bs, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mHyNmeLg1YG"
      },
      "source": [
        "## Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UDBGSQdyh3Wj"
      },
      "outputs": [],
      "source": [
        "class ConvNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ConvNet, self).__init__()\n",
        "    \n",
        "    #Conv1\n",
        "    self.conv1 = nn.Conv3d(in_channels = 1, \n",
        "                           out_channels = 32, \n",
        "                           kernel_size = (1,1,1), \n",
        "                           stride = (1,1,1)\n",
        "                           )\n",
        "    self.bn1 = nn.BatchNorm3d(32)\n",
        "    self.conv2 = nn.Conv3d(in_channels = 32, \n",
        "                           out_channels = 64, \n",
        "                           kernel_size = (7,7,7),\n",
        "                           stride = (2,2,2)\n",
        "                           )\n",
        "    self.bn2 = nn.BatchNorm3d(64)\n",
        "    self.conv3 = nn.Conv3d(in_channels = 64, \n",
        "                           out_channels = 64, \n",
        "                           kernel_size = (3,3,3),\n",
        "                           stride = (2,2,2)\n",
        "                           )\n",
        "    self.bn3 = nn.BatchNorm3d(64)\n",
        "    self.conv4 = nn.Conv3d(in_channels = 64, \n",
        "                           out_channels = 128, \n",
        "                           kernel_size = (3,3,3),\n",
        "                           stride = (2,2,2)\n",
        "                           )\n",
        "    self.bn4 = nn.BatchNorm3d(128) \n",
        "    self.pool1 = nn.AdaptiveAvgPool3d((1,1,1)) #Global Average Pool, takes the average over last two dimensions to flatten \n",
        "  \n",
        "                                                             \n",
        "    # Fully connected layer\n",
        "    self.fc1 = nn.Linear(128,64) # need to find out the size where AdaptiveAvgPool \n",
        "    self.fc2 = nn.Linear(64, 2) # left with 2 for the two classes                     \n",
        "\n",
        "  def forward(self, xb):\n",
        "    xb = self.bn1(F.relu(self.conv1((xb))))\n",
        "    xb = self.bn2(F.relu(self.conv2((xb)))) # Takes a long time\n",
        "    xb = self.bn3(F.relu(self.conv3((xb))))\n",
        "    xb = self.bn4(F.relu(self.conv4((xb))))\n",
        "    xb = self.pool1(xb)\n",
        "    xb = xb.view(xb.shape[:2])\n",
        "    xb = self.fc1(xb)\n",
        "    xb = self.fc2(xb)\n",
        "    return xb      \n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-jsQY9v-f2fO"
      },
      "outputs": [],
      "source": [
        "# Set to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Get model\n",
        "model = ConvNet()\n",
        "model = model.to(device)\n",
        "# print(\"First model training on GPU\")\n",
        "# print(model)\n",
        "\n",
        "# Initialize other parameters\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "loss_func = F.cross_entropy\n",
        "opt = torch.optim.Adam(model.parameters(), lr = learning_rate)#, momentum = 0.9) #or ADAM/ momentum"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(out, yb):\n",
        "    preds = torch.argmax(out, dim=1)\n",
        "    return (preds == yb).float().mean()\n",
        "\n",
        "\n",
        "\n",
        "# Use to load GPU model\n",
        "# model.load_state_dict(torch.load(path))\n",
        "\n"
      ],
      "metadata": {
        "id": "YXVkGhmO8PBW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "kTAp6i9XReq-"
      },
      "outputs": [],
      "source": [
        "\n",
        "def run_cnn(model, epochs, learning_rate, loss_func, opt, dl):\n",
        "  metrics_dict = {}\n",
        "  # Run Model\n",
        "  for epoch in range(1, 1+epochs):\n",
        "    i = 1\n",
        "    accuracy_list = []\n",
        "    loss_list = []\n",
        "    model.train()\n",
        "    print('epoch', epoch)\n",
        "    for xb, yb in dl:\n",
        "      print('batch', i)\n",
        "      i += 1\n",
        "\n",
        "      xb = xb.float()\n",
        "      pred = model(xb)\n",
        "      loss_batch = loss_func(pred, yb)\n",
        "      loss_list.append(loss_batch)\n",
        "      accuracy_batch = accuracy(pred, yb)\n",
        "      accuracy_list.append(accuracy_batch)\n",
        "\n",
        "      print('Batch Loss', loss_batch)\n",
        "      print('Batch Accuracy', accuracy_batch)\n",
        "\n",
        "      loss_batch.backward()\n",
        "      opt.step()\n",
        "      opt.zero_grad()\n",
        "\n",
        "    model.eval()\n",
        "    \n",
        "    print('Saving model')\n",
        "    model_name = 'cnn_fmri_initial_model.pt'\n",
        "    path = F\"/content/gdrive/My Drive/{model_name}\" \n",
        "    torch.save(model.state_dict(), path)\n",
        "\n",
        "    metrics_dict['epoch_'+str(epoch)] = {'accuracy':accuracy_list, 'loss':loss_list}\n",
        "\n",
        "    print('epoch', epoch, 'finished\\n')\n",
        "    try:\n",
        "      past_epoch_accuracies = [sum(metrics_dict['epoch_'+str(epoch-2)]['accuracy']), sum(metrics_dict['epoch_'+str(epoch-1)]['accuracy'])]\n",
        "      current_epoch_accuracy = sum(metrics_dict['epoch_'+str(epoch)]['accuracy'])\n",
        "      if past_epoch_accuracies[0] > current_epoch_accuracy and past_epoch_accuracies[1] > current_epoch_accuracy:\n",
        "        print('Early stop to avoid overfitting\\nModel accuracies did not decrease for two epochs')\n",
        "        return model, metrics_dict\n",
        "\n",
        "    except:\n",
        "      pass\n",
        "  \n",
        "  return model, metrics_dict\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Get model\n",
        "model = ConvNet()\n",
        "model = model.to(device)\n",
        "# print(\"First model training on GPU\")\n",
        "# print(model)\n",
        "\n",
        "# Initialize other parameters\n",
        "epochs = 10\n",
        "learning_rate = 0.001\n",
        "loss_func = F.cross_entropy\n",
        "opt = torch.optim.Adam(model.parameters(), lr = learning_rate)#, momentum = 0.9) #or ADAM/ momentum\n",
        "\n"
      ],
      "metadata": {
        "id": "iVzucX349ZBz"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "metrics_dict = {}\n",
        "for i,image_index in enumerate(range(0, train_images['images'].shape[0], 504)):\n",
        "\n",
        "  bs = 36\n",
        "\n",
        "  x_train = train_images['images'][image_index:image_index+504]\n",
        "  y_train = train_images['labels'][image_index:image_index+504]\n",
        "\n",
        "  ds = TensorDataset(x_train, y_train)\n",
        "  dl = DataLoader(ds, batch_size = bs, shuffle=True)\n",
        "\n",
        "  model, metrics = run_cnn(model, epochs, learning_rate, loss_func, opt, dl)\n",
        "  metrics_dict['round_'+str(i)] = metrics\n",
        "  \n",
        "  f = open(\"/content/gdrive/My Drive/metrics_dict_%s.pkl\"%('train_1'),\"wb\")\n",
        "  pickle.dump(metrics_dict,f)\n",
        "  print('Finshed with set', str(i), 'of 504 images\\nStarting next set.\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTll4mfpFWxJ",
        "outputId": "a5921f10-5e32-4e73-a3af-6554485b5e9d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1\n",
            "batch 1\n",
            "Batch Loss tensor(0.6929, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5278)\n",
            "batch 2\n",
            "Batch Loss tensor(0.6831, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4722)\n",
            "batch 3\n",
            "Batch Loss tensor(0.8528, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "batch 4\n",
            "Batch Loss tensor(0.7260, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4722)\n",
            "batch 5\n",
            "Batch Loss tensor(0.7298, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 6\n",
            "Batch Loss tensor(0.7134, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4722)\n",
            "batch 7\n",
            "Batch Loss tensor(0.6989, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4722)\n",
            "batch 8\n",
            "Batch Loss tensor(0.7069, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4722)\n",
            "batch 9\n",
            "Batch Loss tensor(0.6709, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 10\n",
            "Batch Loss tensor(0.6858, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5278)\n",
            "batch 11\n",
            "Batch Loss tensor(0.7153, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4444)\n",
            "batch 12\n",
            "Batch Loss tensor(0.6949, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4444)\n",
            "batch 13\n",
            "Batch Loss tensor(0.6686, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 14\n",
            "Batch Loss tensor(0.7069, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5000)\n",
            "Saving model\n",
            "epoch 1 finished\n",
            "\n",
            "epoch 2\n",
            "batch 1\n",
            "Batch Loss tensor(0.6209, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7222)\n",
            "batch 2\n",
            "Batch Loss tensor(0.7253, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4722)\n",
            "batch 3\n",
            "Batch Loss tensor(0.6955, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4444)\n",
            "batch 4\n",
            "Batch Loss tensor(0.6258, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6111)\n",
            "batch 5\n",
            "Batch Loss tensor(0.6825, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5278)\n",
            "batch 6\n",
            "Batch Loss tensor(0.7424, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4444)\n",
            "batch 7\n",
            "Batch Loss tensor(0.6748, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "batch 8\n",
            "Batch Loss tensor(0.6856, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "batch 9\n",
            "Batch Loss tensor(0.6711, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6389)\n",
            "batch 10\n",
            "Batch Loss tensor(0.7229, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4167)\n",
            "batch 11\n",
            "Batch Loss tensor(0.7292, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4722)\n",
            "batch 12\n",
            "Batch Loss tensor(0.6776, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "batch 13\n",
            "Batch Loss tensor(0.6912, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4444)\n",
            "batch 14\n",
            "Batch Loss tensor(0.6647, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "Saving model\n",
            "epoch 2 finished\n",
            "\n",
            "epoch 3\n",
            "batch 1\n",
            "Batch Loss tensor(0.7065, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5000)\n",
            "batch 2\n",
            "Batch Loss tensor(0.6806, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6944)\n",
            "batch 3\n",
            "Batch Loss tensor(0.7077, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "batch 4\n",
            "Batch Loss tensor(0.6888, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5000)\n",
            "batch 5\n",
            "Batch Loss tensor(0.6754, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "batch 6\n",
            "Batch Loss tensor(0.7252, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 7\n",
            "Batch Loss tensor(0.6974, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.3611)\n",
            "batch 8\n",
            "Batch Loss tensor(0.6822, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4444)\n",
            "batch 9\n",
            "Batch Loss tensor(0.6420, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6389)\n",
            "batch 10\n",
            "Batch Loss tensor(0.6751, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "batch 11\n",
            "Batch Loss tensor(0.7153, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4167)\n",
            "batch 12\n",
            "Batch Loss tensor(0.7560, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4444)\n",
            "batch 13\n",
            "Batch Loss tensor(0.6897, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5000)\n",
            "batch 14\n",
            "Batch Loss tensor(0.6410, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "Saving model\n",
            "epoch 3 finished\n",
            "\n",
            "epoch 4\n",
            "batch 1\n",
            "Batch Loss tensor(0.7162, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5278)\n",
            "batch 2\n",
            "Batch Loss tensor(0.6675, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "batch 3\n",
            "Batch Loss tensor(0.6892, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6667)\n",
            "batch 4\n",
            "Batch Loss tensor(0.6783, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6111)\n",
            "batch 5\n",
            "Batch Loss tensor(0.6538, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6667)\n",
            "batch 6\n",
            "Batch Loss tensor(0.6563, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6111)\n",
            "batch 7\n",
            "Batch Loss tensor(0.6983, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4722)\n",
            "batch 8\n",
            "Batch Loss tensor(0.7100, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 9\n",
            "Batch Loss tensor(0.6660, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5000)\n",
            "batch 10\n",
            "Batch Loss tensor(0.6626, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5000)\n",
            "batch 11\n",
            "Batch Loss tensor(0.7015, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4167)\n",
            "batch 12\n",
            "Batch Loss tensor(0.6819, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5000)\n",
            "batch 13\n",
            "Batch Loss tensor(0.6691, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.8333)\n",
            "batch 14\n",
            "Batch Loss tensor(0.7040, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5278)\n",
            "Saving model\n",
            "epoch 4 finished\n",
            "\n",
            "epoch 5\n",
            "batch 1\n",
            "Batch Loss tensor(0.6744, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6111)\n",
            "batch 2\n",
            "Batch Loss tensor(0.6774, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "batch 3\n",
            "Batch Loss tensor(0.7063, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4167)\n",
            "batch 4\n",
            "Batch Loss tensor(0.6646, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "batch 5\n",
            "Batch Loss tensor(0.6682, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "batch 6\n",
            "Batch Loss tensor(0.6754, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6111)\n",
            "batch 7\n",
            "Batch Loss tensor(0.6916, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6667)\n",
            "batch 8\n",
            "Batch Loss tensor(0.6880, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4722)\n",
            "batch 9\n",
            "Batch Loss tensor(0.6734, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 10\n",
            "Batch Loss tensor(0.6541, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "batch 11\n",
            "Batch Loss tensor(0.7100, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.3889)\n",
            "batch 12\n",
            "Batch Loss tensor(0.7102, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4722)\n",
            "batch 13\n",
            "Batch Loss tensor(0.6935, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4722)\n",
            "batch 14\n",
            "Batch Loss tensor(0.6962, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "Saving model\n",
            "epoch 5 finished\n",
            "\n",
            "epoch 6\n",
            "batch 1\n",
            "Batch Loss tensor(0.6623, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 2\n",
            "Batch Loss tensor(0.7599, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.3889)\n",
            "batch 3\n",
            "Batch Loss tensor(0.6829, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5278)\n",
            "batch 4\n",
            "Batch Loss tensor(0.6538, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 5\n",
            "Batch Loss tensor(0.6659, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6667)\n",
            "batch 6\n",
            "Batch Loss tensor(0.6866, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5278)\n",
            "batch 7\n",
            "Batch Loss tensor(0.7051, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4444)\n",
            "batch 8\n",
            "Batch Loss tensor(0.6491, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6111)\n",
            "batch 9\n",
            "Batch Loss tensor(0.6806, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "batch 10\n",
            "Batch Loss tensor(0.6694, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5000)\n",
            "batch 11\n",
            "Batch Loss tensor(0.6761, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5000)\n",
            "batch 12\n",
            "Batch Loss tensor(0.6933, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5278)\n",
            "batch 13\n",
            "Batch Loss tensor(0.6853, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6111)\n",
            "batch 14\n",
            "Batch Loss tensor(0.6711, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "Saving model\n",
            "epoch 6 finished\n",
            "\n",
            "epoch 7\n",
            "batch 1\n",
            "Batch Loss tensor(0.7042, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4722)\n",
            "batch 2\n",
            "Batch Loss tensor(0.6770, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "batch 3\n",
            "Batch Loss tensor(0.6746, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 4\n",
            "Batch Loss tensor(0.6998, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "batch 5\n",
            "Batch Loss tensor(0.6551, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "batch 6\n",
            "Batch Loss tensor(0.6503, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6389)\n",
            "batch 7\n",
            "Batch Loss tensor(0.6737, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 8\n",
            "Batch Loss tensor(0.6774, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "batch 9\n",
            "Batch Loss tensor(0.6706, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5000)\n",
            "batch 10\n",
            "Batch Loss tensor(0.6790, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5000)\n",
            "batch 11\n",
            "Batch Loss tensor(0.6799, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 12\n",
            "Batch Loss tensor(0.6790, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5000)\n",
            "batch 13\n",
            "Batch Loss tensor(0.6629, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 14\n",
            "Batch Loss tensor(0.6706, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6111)\n",
            "Saving model\n",
            "epoch 7 finished\n",
            "\n",
            "epoch 8\n",
            "batch 1\n",
            "Batch Loss tensor(0.6726, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7222)\n",
            "batch 2\n",
            "Batch Loss tensor(0.7148, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5000)\n",
            "batch 3\n",
            "Batch Loss tensor(0.7038, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5000)\n",
            "batch 4\n",
            "Batch Loss tensor(0.6122, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7778)\n",
            "batch 5\n",
            "Batch Loss tensor(0.6896, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5000)\n",
            "batch 6\n",
            "Batch Loss tensor(0.6357, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7222)\n",
            "batch 7\n",
            "Batch Loss tensor(0.6691, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6667)\n",
            "batch 8\n",
            "Batch Loss tensor(0.7056, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "batch 9\n",
            "Batch Loss tensor(0.6289, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6667)\n",
            "batch 10\n",
            "Batch Loss tensor(0.6673, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "batch 11\n",
            "Batch Loss tensor(0.6224, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7222)\n",
            "batch 12\n",
            "Batch Loss tensor(0.6526, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6389)\n",
            "batch 13\n",
            "Batch Loss tensor(0.7066, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5278)\n",
            "batch 14\n",
            "Batch Loss tensor(0.6231, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6389)\n",
            "Saving model\n",
            "epoch 8 finished\n",
            "\n",
            "epoch 9\n",
            "batch 1\n",
            "Batch Loss tensor(0.6661, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6111)\n",
            "batch 2\n",
            "Batch Loss tensor(0.6783, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6111)\n",
            "batch 3\n",
            "Batch Loss tensor(0.6074, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6667)\n",
            "batch 4\n",
            "Batch Loss tensor(0.6298, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7778)\n",
            "batch 5\n",
            "Batch Loss tensor(0.7199, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5278)\n",
            "batch 6\n",
            "Batch Loss tensor(0.6523, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "batch 7\n",
            "Batch Loss tensor(0.6256, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7778)\n",
            "batch 8\n",
            "Batch Loss tensor(0.6434, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6667)\n",
            "batch 9\n",
            "Batch Loss tensor(0.6587, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "batch 10\n",
            "Batch Loss tensor(0.6473, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6111)\n",
            "batch 11\n",
            "Batch Loss tensor(0.6963, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 12\n",
            "Batch Loss tensor(0.6385, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "batch 13\n",
            "Batch Loss tensor(0.6875, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 14\n",
            "Batch Loss tensor(0.6607, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "Saving model\n",
            "epoch 9 finished\n",
            "\n",
            "epoch 10\n",
            "batch 1\n",
            "Batch Loss tensor(0.6440, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 2\n",
            "Batch Loss tensor(0.6314, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6667)\n",
            "batch 3\n",
            "Batch Loss tensor(0.5844, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7778)\n",
            "batch 4\n",
            "Batch Loss tensor(0.6492, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6389)\n",
            "batch 5\n",
            "Batch Loss tensor(0.6628, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6944)\n",
            "batch 6\n",
            "Batch Loss tensor(0.6121, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6944)\n",
            "batch 7\n",
            "Batch Loss tensor(0.6096, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6944)\n",
            "batch 8\n",
            "Batch Loss tensor(0.6563, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 9\n",
            "Batch Loss tensor(0.6515, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7222)\n",
            "batch 10\n",
            "Batch Loss tensor(0.5758, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.8056)\n",
            "batch 11\n",
            "Batch Loss tensor(0.6688, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6667)\n",
            "batch 12\n",
            "Batch Loss tensor(0.7131, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4722)\n",
            "batch 13\n",
            "Batch Loss tensor(0.6633, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6389)\n",
            "batch 14\n",
            "Batch Loss tensor(0.6838, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "Saving model\n",
            "epoch 10 finished\n",
            "\n",
            "Finshed with set 0 of 504 images\n",
            "Starting next set.\n",
            "\n",
            "\n",
            "epoch 1\n",
            "batch 1\n",
            "Batch Loss tensor(0.8797, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.3333)\n",
            "batch 2\n",
            "Batch Loss tensor(0.6574, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5278)\n",
            "batch 3\n",
            "Batch Loss tensor(0.6459, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6389)\n",
            "batch 4\n",
            "Batch Loss tensor(0.6746, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5000)\n",
            "batch 5\n",
            "Batch Loss tensor(0.8021, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4722)\n",
            "batch 6\n",
            "Batch Loss tensor(0.8423, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4167)\n",
            "batch 7\n",
            "Batch Loss tensor(0.6199, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6389)\n",
            "batch 8\n",
            "Batch Loss tensor(0.7053, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5278)\n",
            "batch 9\n",
            "Batch Loss tensor(0.7114, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5000)\n",
            "batch 10\n",
            "Batch Loss tensor(0.7225, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4444)\n",
            "batch 11\n",
            "Batch Loss tensor(0.6979, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5000)\n",
            "batch 12\n",
            "Batch Loss tensor(0.7059, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 13\n",
            "Batch Loss tensor(0.6614, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 14\n",
            "Batch Loss tensor(0.6998, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4722)\n",
            "Saving model\n",
            "epoch 1 finished\n",
            "\n",
            "epoch 2\n",
            "batch 1\n",
            "Batch Loss tensor(0.6676, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6389)\n",
            "batch 2\n",
            "Batch Loss tensor(0.6598, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6389)\n",
            "batch 3\n",
            "Batch Loss tensor(0.7027, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4722)\n",
            "batch 4\n",
            "Batch Loss tensor(0.6919, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5278)\n",
            "batch 5\n",
            "Batch Loss tensor(0.7408, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5278)\n",
            "batch 6\n",
            "Batch Loss tensor(0.7457, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.3889)\n",
            "batch 7\n",
            "Batch Loss tensor(0.6828, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5000)\n",
            "batch 8\n",
            "Batch Loss tensor(0.6256, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6944)\n",
            "batch 9\n",
            "Batch Loss tensor(0.6124, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7500)\n",
            "batch 10\n",
            "Batch Loss tensor(0.7201, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "batch 11\n",
            "Batch Loss tensor(0.6672, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 12\n",
            "Batch Loss tensor(0.7114, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5000)\n",
            "batch 13\n",
            "Batch Loss tensor(0.6836, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "batch 14\n",
            "Batch Loss tensor(0.6810, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6111)\n",
            "Saving model\n",
            "epoch 2 finished\n",
            "\n",
            "epoch 3\n",
            "batch 1\n",
            "Batch Loss tensor(0.7086, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "batch 2\n",
            "Batch Loss tensor(0.6539, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6667)\n",
            "batch 3\n",
            "Batch Loss tensor(0.6562, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 4\n",
            "Batch Loss tensor(0.6587, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7500)\n",
            "batch 5\n",
            "Batch Loss tensor(0.6677, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "batch 6\n",
            "Batch Loss tensor(0.6770, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5000)\n",
            "batch 7\n",
            "Batch Loss tensor(0.7047, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5278)\n",
            "batch 8\n",
            "Batch Loss tensor(0.6317, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6389)\n",
            "batch 9\n",
            "Batch Loss tensor(0.6889, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "batch 10\n",
            "Batch Loss tensor(0.6764, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "batch 11\n",
            "Batch Loss tensor(0.7002, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 12\n",
            "Batch Loss tensor(0.6939, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.3889)\n",
            "batch 13\n",
            "Batch Loss tensor(0.6560, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6389)\n",
            "batch 14\n",
            "Batch Loss tensor(0.7026, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4722)\n",
            "Saving model\n",
            "epoch 3 finished\n",
            "\n",
            "epoch 4\n",
            "batch 1\n",
            "Batch Loss tensor(0.7077, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5000)\n",
            "batch 2\n",
            "Batch Loss tensor(0.6056, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6944)\n",
            "batch 3\n",
            "Batch Loss tensor(0.6370, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7222)\n",
            "batch 4\n",
            "Batch Loss tensor(0.6380, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6389)\n",
            "batch 5\n",
            "Batch Loss tensor(0.6364, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6389)\n",
            "batch 6\n",
            "Batch Loss tensor(0.6502, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6111)\n",
            "batch 7\n",
            "Batch Loss tensor(0.7201, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4167)\n",
            "batch 8\n",
            "Batch Loss tensor(0.7664, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.3889)\n",
            "batch 9\n",
            "Batch Loss tensor(0.7127, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5278)\n",
            "batch 10\n",
            "Batch Loss tensor(0.6277, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6111)\n",
            "batch 11\n",
            "Batch Loss tensor(0.7328, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5000)\n",
            "batch 12\n",
            "Batch Loss tensor(0.6989, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 13\n",
            "Batch Loss tensor(0.7020, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5000)\n",
            "batch 14\n",
            "Batch Loss tensor(0.6764, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6389)\n",
            "Saving model\n",
            "epoch 4 finished\n",
            "\n",
            "epoch 5\n",
            "batch 1\n",
            "Batch Loss tensor(0.6716, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6389)\n",
            "batch 2\n",
            "Batch Loss tensor(0.6642, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5278)\n",
            "batch 3\n",
            "Batch Loss tensor(0.6673, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "batch 4\n",
            "Batch Loss tensor(0.6955, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5278)\n",
            "batch 5\n",
            "Batch Loss tensor(0.7065, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5278)\n",
            "batch 6\n",
            "Batch Loss tensor(0.6500, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6944)\n",
            "batch 7\n",
            "Batch Loss tensor(0.6741, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "batch 8\n",
            "Batch Loss tensor(0.6576, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7500)\n",
            "batch 9\n",
            "Batch Loss tensor(0.6533, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "batch 10\n",
            "Batch Loss tensor(0.7048, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4722)\n",
            "batch 11\n",
            "Batch Loss tensor(0.6853, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 12\n",
            "Batch Loss tensor(0.6467, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6111)\n",
            "batch 13\n",
            "Batch Loss tensor(0.6806, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6389)\n",
            "batch 14\n",
            "Batch Loss tensor(0.6747, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5278)\n",
            "Saving model\n",
            "epoch 5 finished\n",
            "\n",
            "epoch 6\n",
            "batch 1\n",
            "Batch Loss tensor(0.7527, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.3889)\n",
            "batch 2\n",
            "Batch Loss tensor(0.6092, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6667)\n",
            "batch 3\n",
            "Batch Loss tensor(0.6327, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6944)\n",
            "batch 4\n",
            "Batch Loss tensor(0.6504, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 5\n",
            "Batch Loss tensor(0.7304, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5000)\n",
            "batch 6\n",
            "Batch Loss tensor(0.6806, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4722)\n",
            "batch 7\n",
            "Batch Loss tensor(0.6350, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6944)\n",
            "batch 8\n",
            "Batch Loss tensor(0.6204, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6389)\n",
            "batch 9\n",
            "Batch Loss tensor(0.6385, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6667)\n",
            "batch 10\n",
            "Batch Loss tensor(0.6261, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6667)\n",
            "batch 11\n",
            "Batch Loss tensor(0.6977, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5278)\n",
            "batch 12\n",
            "Batch Loss tensor(0.6537, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5000)\n",
            "batch 13\n",
            "Batch Loss tensor(0.6612, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6111)\n",
            "batch 14\n",
            "Batch Loss tensor(0.6835, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "Saving model\n",
            "epoch 6 finished\n",
            "\n",
            "epoch 7\n",
            "batch 1\n",
            "Batch Loss tensor(0.6604, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 2\n",
            "Batch Loss tensor(0.6702, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6111)\n",
            "batch 3\n",
            "Batch Loss tensor(0.6357, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7222)\n",
            "batch 4\n",
            "Batch Loss tensor(0.6591, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 5\n",
            "Batch Loss tensor(0.6384, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6389)\n",
            "batch 6\n",
            "Batch Loss tensor(0.6442, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6111)\n",
            "batch 7\n",
            "Batch Loss tensor(0.6920, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 8\n",
            "Batch Loss tensor(0.6863, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4722)\n",
            "batch 9\n",
            "Batch Loss tensor(0.6533, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6111)\n",
            "batch 10\n",
            "Batch Loss tensor(0.6208, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7222)\n",
            "batch 11\n",
            "Batch Loss tensor(0.7445, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6389)\n",
            "batch 12\n",
            "Batch Loss tensor(0.6801, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 13\n",
            "Batch Loss tensor(0.6788, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6111)\n",
            "batch 14\n",
            "Batch Loss tensor(0.6182, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6667)\n",
            "Saving model\n",
            "epoch 7 finished\n",
            "\n",
            "epoch 8\n",
            "batch 1\n",
            "Batch Loss tensor(0.6369, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6111)\n",
            "batch 2\n",
            "Batch Loss tensor(0.6326, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6944)\n",
            "batch 3\n",
            "Batch Loss tensor(0.5954, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7222)\n",
            "batch 4\n",
            "Batch Loss tensor(0.6321, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7500)\n",
            "batch 5\n",
            "Batch Loss tensor(0.6972, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 6\n",
            "Batch Loss tensor(0.6854, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "batch 7\n",
            "Batch Loss tensor(0.6165, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6944)\n",
            "batch 8\n",
            "Batch Loss tensor(0.6291, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6389)\n",
            "batch 9\n",
            "Batch Loss tensor(0.6309, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6667)\n",
            "batch 10\n",
            "Batch Loss tensor(0.6594, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 11\n",
            "Batch Loss tensor(0.6263, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7222)\n",
            "batch 12\n",
            "Batch Loss tensor(0.6748, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 13\n",
            "Batch Loss tensor(0.6215, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6389)\n",
            "batch 14\n",
            "Batch Loss tensor(0.5854, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7500)\n",
            "Saving model\n",
            "epoch 8 finished\n",
            "\n",
            "epoch 9\n",
            "batch 1\n",
            "Batch Loss tensor(0.6003, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6944)\n",
            "batch 2\n",
            "Batch Loss tensor(0.6787, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6111)\n",
            "batch 3\n",
            "Batch Loss tensor(0.6565, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6389)\n",
            "batch 4\n",
            "Batch Loss tensor(0.6207, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6389)\n",
            "batch 5\n",
            "Batch Loss tensor(0.5868, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7222)\n",
            "batch 6\n",
            "Batch Loss tensor(0.6415, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6944)\n",
            "batch 7\n",
            "Batch Loss tensor(0.5692, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7500)\n",
            "batch 8\n",
            "Batch Loss tensor(0.6058, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6944)\n",
            "batch 9\n",
            "Batch Loss tensor(0.6766, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5556)\n",
            "batch 10\n",
            "Batch Loss tensor(0.6312, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6389)\n",
            "batch 11\n",
            "Batch Loss tensor(0.5981, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6389)\n",
            "batch 12\n",
            "Batch Loss tensor(0.5929, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7778)\n",
            "batch 13\n",
            "Batch Loss tensor(0.7023, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4722)\n",
            "batch 14\n",
            "Batch Loss tensor(0.5313, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7778)\n",
            "Saving model\n",
            "epoch 9 finished\n",
            "\n",
            "epoch 10\n",
            "batch 1\n",
            "Batch Loss tensor(0.6147, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6944)\n",
            "batch 2\n",
            "Batch Loss tensor(0.5589, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7500)\n",
            "batch 3\n",
            "Batch Loss tensor(0.6268, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6944)\n",
            "batch 4\n",
            "Batch Loss tensor(0.6234, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7222)\n",
            "batch 5\n",
            "Batch Loss tensor(0.5746, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7500)\n",
            "batch 6\n",
            "Batch Loss tensor(0.5869, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7778)\n",
            "batch 7\n",
            "Batch Loss tensor(0.6211, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6389)\n",
            "batch 8\n",
            "Batch Loss tensor(0.5065, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.8333)\n",
            "batch 9\n",
            "Batch Loss tensor(0.5050, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7500)\n",
            "batch 10\n",
            "Batch Loss tensor(0.6045, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6944)\n",
            "batch 11\n",
            "Batch Loss tensor(0.5694, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6944)\n",
            "batch 12\n",
            "Batch Loss tensor(0.4955, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7778)\n",
            "batch 13\n",
            "Batch Loss tensor(0.5367, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.8056)\n",
            "batch 14\n",
            "Batch Loss tensor(0.6001, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6389)\n",
            "Saving model\n",
            "epoch 10 finished\n",
            "\n",
            "Finshed with set 1 of 504 images\n",
            "Starting next set.\n",
            "\n",
            "\n",
            "epoch 1\n",
            "batch 1\n",
            "Batch Loss tensor(0.7898, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6111)\n",
            "batch 2\n",
            "Batch Loss tensor(0.8857, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4722)\n",
            "batch 3\n",
            "Batch Loss tensor(0.7123, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5278)\n",
            "batch 4\n",
            "Batch Loss tensor(0.8208, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4444)\n",
            "batch 5\n",
            "Batch Loss tensor(0.6776, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4722)\n",
            "batch 6\n",
            "Batch Loss tensor(0.7123, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5278)\n",
            "batch 7\n",
            "Batch Loss tensor(0.8361, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.3889)\n",
            "batch 8\n",
            "Batch Loss tensor(0.6518, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6111)\n",
            "batch 9\n",
            "Batch Loss tensor(0.6748, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 10\n",
            "Batch Loss tensor(0.6776, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6111)\n",
            "batch 11\n",
            "Batch Loss tensor(0.7226, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4167)\n",
            "batch 12\n",
            "Batch Loss tensor(0.7192, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.4444)\n",
            "batch 13\n",
            "Batch Loss tensor(0.7120, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5000)\n",
            "batch 14\n",
            "Batch Loss tensor(0.7103, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5278)\n",
            "Saving model\n",
            "epoch 1 finished\n",
            "\n",
            "epoch 2\n",
            "batch 1\n",
            "Batch Loss tensor(0.6886, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5278)\n",
            "batch 2\n",
            "Batch Loss tensor(0.6137, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6389)\n",
            "batch 3\n",
            "Batch Loss tensor(0.6607, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6667)\n",
            "batch 4\n",
            "Batch Loss tensor(0.6507, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 5\n",
            "Batch Loss tensor(0.6624, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6389)\n",
            "batch 6\n",
            "Batch Loss tensor(0.6770, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6111)\n",
            "batch 7\n",
            "Batch Loss tensor(0.6325, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6944)\n",
            "batch 8\n",
            "Batch Loss tensor(0.6212, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.8889)\n",
            "batch 9\n",
            "Batch Loss tensor(0.6803, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.5833)\n",
            "batch 10\n",
            "Batch Loss tensor(0.6558, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6389)\n",
            "batch 11\n",
            "Batch Loss tensor(0.6500, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6944)\n",
            "batch 12\n",
            "Batch Loss tensor(0.6555, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6389)\n",
            "batch 13\n",
            "Batch Loss tensor(0.6416, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6944)\n",
            "batch 14\n",
            "Batch Loss tensor(0.6458, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6667)\n",
            "Saving model\n",
            "epoch 2 finished\n",
            "\n",
            "epoch 3\n",
            "batch 1\n",
            "Batch Loss tensor(0.5736, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7500)\n",
            "batch 2\n",
            "Batch Loss tensor(0.6101, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7500)\n",
            "batch 3\n",
            "Batch Loss tensor(0.6964, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6111)\n",
            "batch 4\n",
            "Batch Loss tensor(0.6197, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7500)\n",
            "batch 5\n",
            "Batch Loss tensor(0.6496, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6944)\n",
            "batch 6\n",
            "Batch Loss tensor(0.6510, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6667)\n",
            "batch 7\n",
            "Batch Loss tensor(0.6100, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6667)\n",
            "batch 8\n",
            "Batch Loss tensor(0.5871, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7500)\n",
            "batch 9\n",
            "Batch Loss tensor(0.5701, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.8889)\n",
            "batch 10\n",
            "Batch Loss tensor(0.5917, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6944)\n",
            "batch 11\n",
            "Batch Loss tensor(0.5791, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.8056)\n",
            "batch 12\n",
            "Batch Loss tensor(0.5902, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6944)\n",
            "batch 13\n",
            "Batch Loss tensor(0.5569, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7778)\n",
            "batch 14\n",
            "Batch Loss tensor(0.5745, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6667)\n",
            "Saving model\n",
            "epoch 3 finished\n",
            "\n",
            "epoch 4\n",
            "batch 1\n",
            "Batch Loss tensor(0.5570, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7222)\n",
            "batch 2\n",
            "Batch Loss tensor(0.5164, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7500)\n",
            "batch 3\n",
            "Batch Loss tensor(0.6023, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7222)\n",
            "batch 4\n",
            "Batch Loss tensor(0.5349, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7778)\n",
            "batch 5\n",
            "Batch Loss tensor(0.5088, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.8333)\n",
            "batch 6\n",
            "Batch Loss tensor(0.4874, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.8056)\n",
            "batch 7\n",
            "Batch Loss tensor(0.5063, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.8611)\n",
            "batch 8\n",
            "Batch Loss tensor(0.5143, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7500)\n",
            "batch 9\n",
            "Batch Loss tensor(0.4860, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.8056)\n",
            "batch 10\n",
            "Batch Loss tensor(0.5139, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7222)\n",
            "batch 11\n",
            "Batch Loss tensor(0.5590, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6667)\n",
            "batch 12\n",
            "Batch Loss tensor(0.5403, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7500)\n",
            "batch 13\n",
            "Batch Loss tensor(0.5645, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6944)\n",
            "batch 14\n",
            "Batch Loss tensor(0.4800, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.8333)\n",
            "Saving model\n",
            "epoch 4 finished\n",
            "\n",
            "epoch 5\n",
            "batch 1\n",
            "Batch Loss tensor(0.4415, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.8056)\n",
            "batch 2\n",
            "Batch Loss tensor(0.3989, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.8333)\n",
            "batch 3\n",
            "Batch Loss tensor(0.5688, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6944)\n",
            "batch 4\n",
            "Batch Loss tensor(0.5621, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.6667)\n",
            "batch 5\n",
            "Batch Loss tensor(0.4442, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.7778)\n",
            "batch 6\n",
            "Batch Loss tensor(0.3699, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.8056)\n",
            "batch 7\n",
            "Batch Loss tensor(0.3876, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.8611)\n",
            "batch 8\n",
            "Batch Loss tensor(0.3939, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.8333)\n",
            "batch 9\n",
            "Batch Loss tensor(0.4561, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.8056)\n",
            "batch 10\n",
            "Batch Loss tensor(0.3742, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.8611)\n",
            "batch 11\n",
            "Batch Loss tensor(0.4323, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.8333)\n",
            "batch 12\n",
            "Batch Loss tensor(0.3419, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.9167)\n",
            "batch 13\n",
            "Batch Loss tensor(0.3759, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.8889)\n",
            "batch 14\n",
            "Batch Loss tensor(0.4102, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.8611)\n",
            "Saving model\n",
            "epoch 5 finished\n",
            "\n",
            "epoch 6\n",
            "batch 1\n",
            "Batch Loss tensor(0.2161, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.9167)\n",
            "batch 2\n",
            "Batch Loss tensor(0.2871, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.9167)\n",
            "batch 3\n",
            "Batch Loss tensor(0.2065, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.9444)\n",
            "batch 4\n",
            "Batch Loss tensor(0.1854, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(1.)\n",
            "batch 5\n",
            "Batch Loss tensor(0.1748, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.9722)\n",
            "batch 6\n",
            "Batch Loss tensor(0.1878, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.9444)\n",
            "batch 7\n",
            "Batch Loss tensor(0.1914, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.9444)\n",
            "batch 8\n",
            "Batch Loss tensor(0.4586, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.8056)\n",
            "batch 9\n",
            "Batch Loss tensor(0.3087, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.8611)\n",
            "batch 10\n",
            "Batch Loss tensor(0.1797, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.8889)\n",
            "batch 11\n",
            "Batch Loss tensor(0.1526, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.9444)\n",
            "batch 12\n",
            "Batch Loss tensor(0.2185, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.9444)\n",
            "batch 13\n",
            "Batch Loss tensor(0.2842, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.8889)\n",
            "batch 14\n",
            "Batch Loss tensor(0.1933, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.8889)\n",
            "Saving model\n",
            "epoch 6 finished\n",
            "\n",
            "epoch 7\n",
            "batch 1\n",
            "Batch Loss tensor(0.0871, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(1.)\n",
            "batch 2\n",
            "Batch Loss tensor(0.0944, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.9722)\n",
            "batch 3\n",
            "Batch Loss tensor(0.1656, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.9722)\n",
            "batch 4\n",
            "Batch Loss tensor(0.2410, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.9444)\n",
            "batch 5\n",
            "Batch Loss tensor(0.1060, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.9722)\n",
            "batch 6\n",
            "Batch Loss tensor(0.1549, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.9444)\n",
            "batch 7\n",
            "Batch Loss tensor(0.1495, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.9722)\n",
            "batch 8\n",
            "Batch Loss tensor(0.0965, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(1.)\n",
            "batch 9\n",
            "Batch Loss tensor(0.2847, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.9167)\n",
            "batch 10\n",
            "Batch Loss tensor(0.0527, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(1.)\n",
            "batch 11\n",
            "Batch Loss tensor(0.1582, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.8889)\n",
            "batch 12\n",
            "Batch Loss tensor(0.0989, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.9722)\n",
            "batch 13\n",
            "Batch Loss tensor(0.1465, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.9444)\n",
            "batch 14\n",
            "Batch Loss tensor(0.1054, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.9722)\n",
            "Saving model\n",
            "epoch 7 finished\n",
            "\n",
            "epoch 8\n",
            "batch 1\n",
            "Batch Loss tensor(0.0364, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(1.)\n",
            "batch 2\n",
            "Batch Loss tensor(0.0187, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(1.)\n",
            "batch 3\n",
            "Batch Loss tensor(0.0351, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.9722)\n",
            "batch 4\n",
            "Batch Loss tensor(0.0750, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.9722)\n",
            "batch 5\n",
            "Batch Loss tensor(0.0786, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.9722)\n",
            "batch 6\n",
            "Batch Loss tensor(0.1294, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.9444)\n",
            "batch 7\n",
            "Batch Loss tensor(0.0297, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(1.)\n",
            "batch 8\n",
            "Batch Loss tensor(0.0468, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(1.)\n",
            "batch 9\n",
            "Batch Loss tensor(0.0667, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.9722)\n",
            "batch 10\n",
            "Batch Loss tensor(0.0488, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(1.)\n",
            "batch 11\n",
            "Batch Loss tensor(0.1174, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.9722)\n",
            "batch 12\n",
            "Batch Loss tensor(0.1316, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.9444)\n",
            "batch 13\n",
            "Batch Loss tensor(0.0238, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(1.)\n",
            "batch 14\n",
            "Batch Loss tensor(0.1046, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.9722)\n",
            "Saving model\n",
            "epoch 8 finished\n",
            "\n",
            "epoch 9\n",
            "batch 1\n",
            "Batch Loss tensor(0.0274, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(1.)\n",
            "batch 2\n",
            "Batch Loss tensor(0.0326, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(1.)\n",
            "batch 3\n",
            "Batch Loss tensor(0.0405, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(1.)\n",
            "batch 4\n",
            "Batch Loss tensor(0.0299, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(1.)\n",
            "batch 5\n",
            "Batch Loss tensor(0.0296, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(1.)\n",
            "batch 6\n",
            "Batch Loss tensor(0.0229, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(1.)\n",
            "batch 7\n",
            "Batch Loss tensor(0.0460, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.9722)\n",
            "batch 8\n",
            "Batch Loss tensor(0.0487, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.9722)\n",
            "batch 9\n",
            "Batch Loss tensor(0.0579, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(1.)\n",
            "batch 10\n",
            "Batch Loss tensor(0.0302, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(1.)\n",
            "batch 11\n",
            "Batch Loss tensor(0.0995, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.9444)\n",
            "batch 12\n",
            "Batch Loss tensor(0.0339, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(1.)\n",
            "batch 13\n",
            "Batch Loss tensor(0.0167, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(1.)\n",
            "batch 14\n",
            "Batch Loss tensor(0.0121, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(1.)\n",
            "Saving model\n",
            "epoch 9 finished\n",
            "\n",
            "epoch 10\n",
            "batch 1\n",
            "Batch Loss tensor(0.0067, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(1.)\n",
            "batch 2\n",
            "Batch Loss tensor(0.0153, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(1.)\n",
            "batch 3\n",
            "Batch Loss tensor(0.0082, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(1.)\n",
            "batch 4\n",
            "Batch Loss tensor(0.0094, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(1.)\n",
            "batch 5\n",
            "Batch Loss tensor(0.0216, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(1.)\n",
            "batch 6\n",
            "Batch Loss tensor(0.0071, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(1.)\n",
            "batch 7\n",
            "Batch Loss tensor(0.1358, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.9722)\n",
            "batch 8\n",
            "Batch Loss tensor(0.0231, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(1.)\n",
            "batch 9\n",
            "Batch Loss tensor(0.0439, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.9722)\n",
            "batch 10\n",
            "Batch Loss tensor(0.0139, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(1.)\n",
            "batch 11\n",
            "Batch Loss tensor(0.0265, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(0.9722)\n",
            "batch 12\n",
            "Batch Loss tensor(0.0317, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(1.)\n",
            "batch 13\n",
            "Batch Loss tensor(0.0090, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(1.)\n",
            "batch 14\n",
            "Batch Loss tensor(0.0163, grad_fn=<NllLossBackward0>)\n",
            "Batch Accuracy tensor(1.)\n",
            "Saving model\n",
            "epoch 10 finished\n",
            "\n",
            "Finshed with set 2 of 504 images\n",
            "Starting next set.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLzXUn9re04K"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "66UUnIhJevn9",
        "outputId": "b3a47033-0952-40ae-9e71-de9a70f52a8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'train': [], 'val': []}\n",
            "{'train': [], 'val': []}\n",
            "\n",
            "Epoch$ : 1\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-178-16282038068e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m   \u001b[0mtrain_val_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-178-16282038068e>\u001b[0m in \u001b[0;36mtrain_val_model\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nEpoch$ : %d'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx_train_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m       \u001b[0mx_train_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#(float).to(device) # for GPU support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0my_train_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
          ]
        }
      ],
      "source": [
        "accuracy_stats = {\n",
        "    'train': [],\n",
        "    'val': []\n",
        "  }\n",
        "\n",
        "print(accuracy_stats)\n",
        "\n",
        "loss_stats = {\n",
        "    'train': [],\n",
        "    'val': []\n",
        "    }\n",
        "print(loss_stats)\n",
        "\n",
        "\n",
        "\n",
        "def train_test_model(epochs, model, opt, train_dl, test_dl, device):\n",
        "  for epoch in range(1, epochs+1):\n",
        "\n",
        "    # TRAINING *****************************************************************\n",
        "\n",
        "    train_epoch_loss = 0\n",
        "    train_epoch_acc = 0\n",
        "\n",
        "    # set model in training mode \n",
        "    model.train()\n",
        "    print('\\nEpoch$ : %d'%epoch)\n",
        "    for xb, yb in tqdm(train_dl):\n",
        "      xb = xb.to(device)\n",
        "      y_train_batch = y_train_batch.to(device) \n",
        "\n",
        "      #print(x_train_batch.shape)\n",
        "\n",
        "      # sets gradients to 0 to prevent interference with previous epoch\n",
        "      opt.zero_grad()\n",
        "    \n",
        "      # Forward pass through NN\n",
        "      y_pred = model(xb)#.to(float)\n",
        "      train_loss = criterion(y_pred, yb)\n",
        "      train_acc = accuracy(y_pred, yb)\n",
        "\n",
        "      # Backward pass, updating weights\n",
        "      train_loss.backward()\n",
        "      opt.step()\n",
        "\n",
        "      # Statistics\n",
        "      train_epoch_loss += train_loss.item()\n",
        "      train_epoch_acc += train_acc.item()\n",
        "\n",
        "\n",
        "    # VALIDATION****************************************************************   \n",
        "    \n",
        "    with torch.set_grad_enabled(False):\n",
        "      val_epoch_loss = 0\n",
        "      val_epoch_acc = 0\n",
        "\n",
        "      model.eval()\n",
        "      for x_val_batch, y_val_batch in tqdm(validate_loader):\n",
        "      \n",
        "        x_val_batch =  x_val_batch.to(device)#.to(float)\n",
        "        y_val_batch = y_val_batch.to(device)\n",
        "            \n",
        "        # Forward pass\n",
        "        y_val_pred = model(x_val_batch)#.to(float)   \n",
        "        val_loss = criterion(y_val_pred, y_val_batch)\n",
        "        val_acc = accuracy(y_val_pred, y_val_batch)\n",
        "            \n",
        "        val_epoch_loss += val_loss.item()\n",
        "        val_epoch_acc += val_acc.item()\n",
        "\n",
        "    # Prevent plateauing validation loss \n",
        "    #scheduler.step(val_epoch_loss/len(validate_loader))\n",
        "\n",
        "        \n",
        "    loss_stats['train'].append(train_epoch_loss/len(train_loader))\n",
        "    loss_stats['val'].append(val_epoch_loss/len(validate_loader))\n",
        "    accuracy_stats['train'].append(train_epoch_acc/len(train_loader))\n",
        "    accuracy_stats['val'].append(val_epoch_acc/len(validate_loader))\n",
        "                              \n",
        "    \n",
        "    print(f'Epoch {epoch+0:03}: Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(validate_loader):.5f}') \n",
        "    print(f'Train Acc: {train_epoch_acc/len(train_loader):.3f} | Val Acc: {val_epoch_acc/len(validate_loader):.3f}')\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def accuracy(y_pred, y_test):\n",
        "#   # Calculating model accuracy at each epoch \n",
        "#   y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
        "#   _, y_pred_prob = torch.max(y_pred_softmax, dim = 1)\n",
        "#   correct_pred = (y_pred_prob == y_test).float()\n",
        "#   acc = correct_pred.sum() / len(correct_pred)\n",
        "#   acc = torch.round(acc * 100)\n",
        "\n",
        "#   return acc\n",
        "\n",
        "\n",
        "\n",
        "     \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#   train_val_model(epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJGJUdiBKtnl"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FyCVpQ6KtkQ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKkISUtQKthJ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCzbRyCiKteE"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwmpLfSLKtaZ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "name": "Group_3DCNN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}