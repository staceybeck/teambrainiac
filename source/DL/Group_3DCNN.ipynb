{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Group_3DCNN.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yecatstevir/teambrainiac/blob/main/source/DL/Group_3DCNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning with PyTorch\n",
        "## 3D Convolutional Neural Network on Group Brain fMRI\n",
        "Contributors: Stacey Rivet Beck, Ben Merrill\n",
        "### To Do:\n",
        "- Either:\n",
        "  - Get Raw data in 4D   \n",
        "          - OR -\n",
        "  - Reshape Raw data into 4D from 2D\n",
        "    - Apply Whole Brain Mask to data and save to AWS\n",
        "\n",
        "- Build Dataloader: https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
        "\n",
        "- Implement 3DCNN from paper: REALLY GREAT PAPERS\n",
        "  - Nguyen et al. http://proceedings.mlr.press/v136/nguyen20a/nguyen20a.pdf\n",
        "  - Wang et al. https://arxiv.org/pdf/1801.09858.pdf (discusses more in detail the input data shapes and processing)\n",
        "  \n",
        "  - Inputs: 84@ x * y * z ; one fmri time series at a time, not concatenated\n",
        "  - Basic Architecture:\n",
        "        #First layer is generating temporal descriptors of the voxels\n",
        "        Conv1 1 x 1 x 1 filter, output = 32, stride = 1, ReLU, BatchNorm\n",
        "        Conv2 7 x 7 x 7 filter, output = 64, stride = 2, ReLU, BatchNorm\n",
        "        Conv3 3 x 3 x 3 fitler, output = 64, stride = 2, ReLU, BatchNorm\n",
        "        Conv4 3 x 3 x 3 fitler, output = 128, stride = 2, ReLU, BatchNorm\n",
        "        Global Average Pooling on final feature maps ->\n",
        "        Flattened maps size 128?\n",
        "        Fully connected layer size 64\n",
        "        Fully connected layer size 2 (2 way classification, one for each class) -> softmax\n",
        "\n",
        "        Optimized with Adam, standard parameters (β1=0.9 and β2=0.999)\n",
        "        Batched at 32, but we may need to batch smaller due to GPU compute\n",
        "        Learning Rate = 0.001, gradual decay after Val loss plateaued after 15 epochs\n",
        "        Cross entropy Loss\n",
        "        Employ early stopping\n",
        "        In Wang et al. they used data for visualization, same size as input data, though are reduced in time dimension to be mapped on fsaverage surface. \n",
        "        \n",
        "  "
      ],
      "metadata": {
        "id": "ssBXn9lhG9du"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Dataset and Labels"
      ],
      "metadata": {
        "id": "bjOMBc929Vpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWlULVj79acH",
        "outputId": "d06132ca-4b01-4672-f6ab-a558a55ae7f9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/yecatstevir/teambrainiac.git\n",
        "\n",
        "# Change directory into cloned repo DL folder\n",
        "%cd teambrainiac/source/DL\n",
        "\n",
        "# !ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFAZrJHL9a6f",
        "outputId": "6917e6fd-265e-4ece-f1f8-99cb31b4ad04"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'teambrainiac'...\n",
            "warning: --local is ignored\n",
            "remote: Enumerating objects: 1109, done.\u001b[K\n",
            "remote: Counting objects: 100% (1109/1109), done.\u001b[K\n",
            "remote: Compressing objects: 100% (857/857), done.\u001b[K\n",
            "remote: Total 1109 (delta 701), reused 474 (delta 236), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (1109/1109), 82.77 MiB | 28.76 MiB/s, done.\n",
            "Resolving deltas: 100% (701/701), done.\n",
            "/content/teambrainiac/source/DL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load path_config.py"
      ],
      "metadata": {
        "id": "YjLsWQoxMLZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "NJ5q46JZMTPc",
        "outputId": "81d769a9-85aa-40ee-b6ad-0517ed938f92"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d40bb9c9-80a9-44b3-bcc3-0d376a13b7c0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d40bb9c9-80a9-44b3-bcc3-0d376a13b7c0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving path_config.py to path_config.py\n",
            "User uploaded file \"path_config.py\" with length 196 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Packages"
      ],
      "metadata": {
        "id": "giqBoQS0MWFi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Possible Missing Packages\n",
        "!pip install boto3"
      ],
      "metadata": {
        "id": "MNgdNbLKxASP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f4967de-ae15-4b63-8852-bc4016921c70"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.21.41-py3-none-any.whl (132 kB)\n",
            "\u001b[?25l\r\u001b[K     |██▌                             | 10 kB 30.6 MB/s eta 0:00:01\r\u001b[K     |█████                           | 20 kB 33.2 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 30 kB 39.7 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 40 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 51 kB 19.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 61 kB 21.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 71 kB 23.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 81 kB 24.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 92 kB 26.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 102 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 112 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 122 kB 25.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 132 kB 25.5 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 6.4 MB/s \n",
            "\u001b[?25hCollecting botocore<1.25.0,>=1.24.41\n",
            "  Downloading botocore-1.24.41-py3-none-any.whl (8.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.7 MB 70.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.25.0,>=1.24.41->boto3) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 73.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.41->boto3) (1.15.0)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.9 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.21.41 botocore-1.24.41 jmespath-1.0.0 s3transfer-0.5.2 urllib3-1.26.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# General Library Imports\n",
        "import re\n",
        "import scipy.io\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "import pandas as pd\n",
        "import boto3\n",
        "import tempfile\n",
        "import tqdm\n",
        "import random\n",
        "from path_config import mat_path\n",
        "from botocore.exceptions import ClientError\n",
        "from collections import defaultdict\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# From Local Directory\n",
        "from access_data_dl import *\n",
        "from process_dl import *\n",
        "\n",
        "# Pytroch Libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "\n",
        "#import torchvision.transforms as transforms\n",
        "from torch.nn import ReLU, CrossEntropyLoss, Conv3d, Module, Softmax, AdaptiveAvgPool3d\n",
        "from torch.optim import Adam, SGD\n",
        "\n",
        "#from torch.optim import lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# from dataloader_class import DatasetFmri\n"
      ],
      "metadata": {
        "id": "lAhQQqpqMYNT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Group fMRI Data, Normalize, and Create Masks"
      ],
      "metadata": {
        "id": "eYG_gfx5sDHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open path dictionary file to get subject ids\n",
        "path = \"../data/data_path_dictionary.pkl\"\n",
        "data_path_dict = open_pickle(path)"
      ],
      "metadata": {
        "id": "wWYt1ci4sTgk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix hyperparameters for preprocessing\n",
        "label_type='rt_labels'\n",
        "n_subjects = len(data_path_dict['subject_ID'])\n",
        "runs_list = [2,3]\n",
        "\n",
        "\n",
        "# Run functions to import unmasked data\n",
        "image_label_mask, image_labels = labels_mask_binary(data_path_dict, label_type='rt_labels')\n",
        "\n",
        "all_subjects = load_subjects_chronologically(data_path_dict, n_subjects, image_label_mask, image_labels, label_type, runs_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_A6VDQpeGZW",
        "outputId": "17ed2c18-8bf0-4249-e437-f1afa93c42eb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Subject ids loaded.\n",
            "Adding subjects to dictionary.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "52it [11:35, 13.37s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mask_normalize_runs_reshape_4d(chron_subject_dict, mask, scaler):\n",
        "    \"\"\"\n",
        "    chron_subject_dict : (subject data returned from load_subjects_chronologically function, keys are subject IDs)\n",
        "    mask               : The mask meant to be applied to each image, typically a whole brain mask\n",
        "    scaler             : Scaler from sklearn used to normalize the data\n",
        "    \n",
        "    returns            : dictionary with fully normalized subjects and labels\n",
        "                         each subject's run is in 5d - n_images x n_color_layers(i.e. 1 because images are black and white) x length x width x height\n",
        "    \"\"\"\n",
        "\n",
        "    runs_normalized_subjects = {}\n",
        "    for i,sub_id in enumerate(chron_subject_dict.keys()):\n",
        "      temp_subject = {}\n",
        "      for key in chron_subject_dict[sub_id].keys():\n",
        "        if key == 'image_labels':\n",
        "          # Labels for the images\n",
        "          temp_subject['image_labels'] = chron_subject_dict[sub_id]['image_labels']\n",
        "        \n",
        "        else:\n",
        "          # Subject Runs\n",
        "          temp_run_unmasked = []\n",
        "          temp_run = chron_subject_dict[sub_id][key]\n",
        "\n",
        "          if scaler == 'standard':\n",
        "            temp_scaler = StandardScaler()\n",
        "          else:\n",
        "            print('Please import required scaler and update function')\n",
        "            break\n",
        "          \n",
        "          temp_run_masked = temp_run[:,mask]\n",
        "          temp_run_masked_norm =  temp_scaler.fit_transform(temp_run_masked)\n",
        "\n",
        "          temp_run_masked_norm_index = 0\n",
        "          for t_or_f in mask:\n",
        "            if t_or_f == True:\n",
        "              temp_run_unmasked.append(temp_run_masked_norm[:,temp_run_masked_norm_index])\n",
        "              temp_run_masked_norm_index += 1\n",
        "            elif t_or_f == False:\n",
        "              temp_run_unmasked.append(np.zeros(len(temp_run_masked_norm[:,0])))\n",
        "            else:\n",
        "              print('Error')\n",
        "              break\n",
        "        \n",
        "          temp_run_unmasked = np.array(temp_run_unmasked).T\n",
        "          temp_run_unmasked_4d = []\n",
        "          for image in temp_run_unmasked:\n",
        "            image_3d = image.reshape((79,95,79), order='F')\n",
        "            image_4d = np.array([image_3d])\n",
        "            temp_run_unmasked_4d.append(image_4d)\n",
        "          temp_run_unmasked_4d = np.array(temp_run_unmasked_4d)\n",
        "          temp_subject[key] = temp_run_unmasked_4d\n",
        "\n",
        "      runs_normalized_subjects[sub_id] = temp_subject\n",
        "      chron_subject_dict[sub_id] = None\n",
        "      \n",
        "      print('Completed Subject', str(i+1))\n",
        "\n",
        "    return runs_normalized_subjects"
      ],
      "metadata": {
        "id": "DJilcEO_2gs2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get Mask Indicies. Note that 'mask' is the string for a whole brain mask.\n",
        "whole_brain_mask = get_mask('mask', data_path_dict, mask_ind=0)\n",
        "scaler = 'standard'\n",
        "\n",
        "all_subjects_inital_reshape = mask_normalize_runs_reshape_4d(all_subjects, whole_brain_mask, scaler)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pq1tvL5AOOKj",
        "outputId": "41c1dcea-49d4-4d33-8538-527f6bc05fd7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed Subject 1\n",
            "Completed Subject 2\n",
            "Completed Subject 3\n",
            "Completed Subject 4\n",
            "Completed Subject 5\n",
            "Completed Subject 6\n",
            "Completed Subject 7\n",
            "Completed Subject 8\n",
            "Completed Subject 9\n",
            "Completed Subject 10\n",
            "Completed Subject 11\n",
            "Completed Subject 12\n",
            "Completed Subject 13\n",
            "Completed Subject 14\n",
            "Completed Subject 15\n",
            "Completed Subject 16\n",
            "Completed Subject 17\n",
            "Completed Subject 18\n",
            "Completed Subject 19\n",
            "Completed Subject 20\n",
            "Completed Subject 21\n",
            "Completed Subject 22\n",
            "Completed Subject 23\n",
            "Completed Subject 24\n",
            "Completed Subject 25\n",
            "Completed Subject 26\n",
            "Completed Subject 27\n",
            "Completed Subject 28\n",
            "Completed Subject 29\n",
            "Completed Subject 30\n",
            "Completed Subject 31\n",
            "Completed Subject 32\n",
            "Completed Subject 33\n",
            "Completed Subject 34\n",
            "Completed Subject 35\n",
            "Completed Subject 36\n",
            "Completed Subject 37\n",
            "Completed Subject 38\n",
            "Completed Subject 39\n",
            "Completed Subject 40\n",
            "Completed Subject 41\n",
            "Completed Subject 42\n",
            "Completed Subject 43\n",
            "Completed Subject 44\n",
            "Completed Subject 45\n",
            "Completed Subject 46\n",
            "Completed Subject 47\n",
            "Completed Subject 48\n",
            "Completed Subject 49\n",
            "Completed Subject 50\n",
            "Completed Subject 51\n",
            "Completed Subject 52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure that we have all subjects\n",
        "len(all_subjects_inital_reshape.keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3uoVIGsw8op",
        "outputId": "04a693cf-7c18-496a-8f9e-74a912f60856"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "52"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Free up RAM by deleting some variables that are taking up memory\n",
        "all_subjects = None"
      ],
      "metadata": {
        "id": "pij0C8HwFhBQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Format Train and Test Data for Single Subject"
      ],
      "metadata": {
        "id": "oPuXfwTDfzGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "This is to keep hardcoded dictionary of shuffled ids to make sure train, validation, and test sets stay seperate\n",
        "\n",
        "If the size you are looking for is not here, use the function below\n",
        "\n",
        "five_ids = {'test': ['10017_08894'],\n",
        " 'train': ['10008_09924', '10016_09694', '10004_08693'],\n",
        " 'val': ['10009_08848']}\n",
        "\n",
        "\n",
        "all_ids = {'test': ['10023_09126','30036_09758','30053_10112','10004_08693','30025_09402','10022_08854','10065_09587','30011_09170',\n",
        "'30009_09227','10037_09903','10057_10124'],\n",
        " 'train': ['30026_09430','30008_08981','10046_09216','30027_09638','10042_08990','10017_08894','30020_09236',\n",
        "  '30017_09567','10039_08941','10035_08847','10069_09785','10027_09455','30004_08965','10053_09018','10043_09222',\n",
        "  '10061_09308','30044_10095','10038_09063','10045_08968','10016_09694','30024_09398','10033_08871','10047_09030',\n",
        "  '30035_09836','30012_09102','10018_08907','30045_10182','10034_08879','10009_08848','30033_09776',\n",
        "  '30014_09352','10056_09615','10050_09079','10084_10188','10080_09931','10021_08839'],\n",
        " 'val': ['10060_09359','30038_09967','10008_09924','10066_09687','10036_09800']}\n",
        "\n",
        "'''\n",
        "print()\n",
        "# generate_train_val_test_dict(list(all_subjects_inital_reshape.keys()), train_val_test_proportion=[0.7,0.8,1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "erJ1rSx-hNUg",
        "outputId": "a922d572-2763-4e5e-cfed-a54e471af603"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = {'test': ['10023_09126','30036_09758','30053_10112','10004_08693','30025_09402','10022_08854','10065_09587','30011_09170',\n",
        "'30009_09227','10037_09903','10057_10124'],\n",
        " 'train': ['30026_09430','30008_08981','10046_09216','30027_09638','10042_08990','10017_08894','30020_09236',\n",
        "  '30017_09567','10039_08941','10035_08847','10069_09785','10027_09455','30004_08965','10053_09018','10043_09222',\n",
        "  '10061_09308','30044_10095','10038_09063','10045_08968','10016_09694','30024_09398','10033_08871','10047_09030',\n",
        "  '30035_09836','30012_09102','10018_08907','30045_10182','10034_08879','10009_08848','30033_09776',\n",
        "  '30014_09352','10056_09615','10050_09079','10084_10188','10080_09931','10021_08839'],\n",
        " 'val': ['10060_09359','30038_09967','10008_09924','10066_09687','10036_09800']}"
      ],
      "metadata": {
        "id": "b4byTUunHba7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_test_aggregation_group(subjects_dict, runs, train_val_test_ids):\n",
        "  \"\"\"\n",
        "    This function aggregates a group of subjects' runs into a training and test set\n",
        "    split up by the desired proportion\n",
        "    subjects_dict       : A dictionary of subject images with ids as keys \n",
        "    runs                : List of Runs to use from subject dict\n",
        "    train_val_test_dict : dictionary of subject ids for train, validation, and test \n",
        "    \n",
        "    returns            : train and test images and their labels ready for dataloader or to save on AWS s3\n",
        "  \"\"\"\n",
        "  train_val_test_dict = {}\n",
        "\n",
        "  for key in train_val_test_ids.keys():\n",
        "    train_or_val_or_test = {}\n",
        "    images = []\n",
        "    labels = []\n",
        "    temp_ids = train_val_test_ids[key]\n",
        "    for subject_id in temp_ids:\n",
        "      for run in runs:\n",
        "        run_key = 'run_'+str(run)\n",
        "        subject_images = subjects_dict[subject_id][run_key]\n",
        "        for image in subject_images:\n",
        "          images.append(image)\n",
        "        labels.extend(list(subjects_dict[subject_id]['image_labels']))\n",
        "      train_val_test_ids[subject_id] = None\n",
        "    \n",
        "    train_or_val_or_test['images'] = torch.from_numpy(np.array(images).astype(float))\n",
        "    train_or_val_or_test['labels'] = torch.from_numpy(np.array(labels).astype(float)).long()\n",
        "    train_val_test_dict[key] = train_or_val_or_test\n",
        "\n",
        "  return train_val_test_dict"
      ],
      "metadata": {
        "id": "i5ZrcCz6lM4H"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "m0dq0qbja0Mp"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "runs = [2,3]\n",
        "\n",
        "train_val_test_dict = train_test_aggregation_group(all_subjects_inital_reshape, runs, all_ids)\n",
        "\n",
        "all_subjects_initial_reshape = None"
      ],
      "metadata": {
        "id": "UE61s17Xpccw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key in train_val_test_dict.keys():\n",
        "  print(train_val_test_dict[key]['images'].shape)\n",
        "  print(train_val_test_dict[key]['labels'].shape,'\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RC1VuWB3p1cM",
        "outputId": "4e02e74f-c0bf-4c32-ae56-25938e7a51f8"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([168, 1, 79, 95, 79])\n",
            "torch.Size([168]) \n",
            "\n",
            "torch.Size([504, 1, 79, 95, 79])\n",
            "torch.Size([504]) \n",
            "\n",
            "torch.Size([168, 1, 79, 95, 79])\n",
            "torch.Size([168]) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload Data"
      ],
      "metadata": {
        "id": "omWCoFPws53X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s3_upload(train_val_test_dict, 'dl/train_val_test_70_10_20.pkl', 'pickle')"
      ],
      "metadata": {
        "id": "krNeYQZRs8Ht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Dataloader"
      ],
      "metadata": {
        "id": "sqW1TnYdgyYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataloader(train_dict, test_dict, bs):\n",
        "  '''\n",
        "  train_dict : Dictionary of training images and their labels\n",
        "   \n",
        "  test_dict  : Dictionary of testing images and their labels\n",
        "  '''\n",
        "\n",
        "    return (\n",
        "        DataLoader(train_ds, batch_size=bs),#, shuffle=True),\n",
        "        DataLoader(test_ds, batch_size=bs * 2)\n",
        "    )"
      ],
      "metadata": {
        "id": "gqf6_gSvT7Yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "x_train = train_dict['images']\n",
        "y_train = train_dict['labels']\n",
        "\n",
        "x_test = test_dict['images']\n",
        "y_test = test_dict['labels']\n",
        "\n",
        "train_ds = TensorDataset(x_train, y_train)\n",
        "test_ds = TensorDataset(x_test, y_test)"
      ],
      "metadata": {
        "id": "IZPwPzibTN00"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bs = 6\n",
        "\n",
        "train_dl = DataLoader(train_ds, batch_size=bs)\n",
        "test_dl = DataLoader(test_ds, batch_size=bs*2)"
      ],
      "metadata": {
        "id": "ZS3OH35RYDrZ"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4lFZfwo7Am8t"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bs = 6\n",
        "\n",
        "# train_dl, test_dl = get_dataloader(train_ds, test_ds, bs)"
      ],
      "metadata": {
        "id": "RLxiW2kEDUrj"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sanity Check\n",
        "train_features, train_labels = next(iter(train_dl))\n",
        "test_features, test_labels = next(iter(test_dl))"
      ],
      "metadata": {
        "id": "aPYJlsSYadkG"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "pGYfYVWjexRW"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practice Model"
      ],
      "metadata": {
        "id": "8xerPsmwSYI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "o8FdYYc8Srgg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 814
        },
        "outputId": "3f2de8d3-1142-4a28-e1d9-f10158beb406"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 32, 79, 95, 79])\n",
            "torch.Size([6, 32, 79, 95, 79])\n",
            "torch.Size([6, 64, 37, 45, 37])\n",
            "torch.Size([6, 64, 18, 22, 18])\n",
            "before drop torch.Size([6, 128, 1, 1, 1])\n",
            "after drop torch.Size([6, 128])\n",
            "after lin layer torch.Size([6, 64])\n",
            "torch.Size([6, 2])\n",
            "torch.Size([6, 32, 79, 95, 79])\n",
            "torch.Size([6, 32, 79, 95, 79])\n",
            "torch.Size([6, 64, 37, 45, 37])\n",
            "torch.Size([6, 64, 18, 22, 18])\n",
            "before drop torch.Size([6, 128, 1, 1, 1])\n",
            "after drop torch.Size([6, 128])\n",
            "after lin layer torch.Size([6, 64])\n",
            "torch.Size([6, 2])\n",
            "torch.Size([6, 32, 79, 95, 79])\n",
            "torch.Size([6, 32, 79, 95, 79])\n",
            "torch.Size([6, 64, 37, 45, 37])\n",
            "torch.Size([6, 64, 18, 22, 18])\n",
            "before drop torch.Size([6, 128, 1, 1, 1])\n",
            "after drop torch.Size([6, 128])\n",
            "after lin layer torch.Size([6, 64])\n",
            "torch.Size([6, 2])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-f116746eb423>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0mxb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m       \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-f116746eb423>\u001b[0m in \u001b[0;36mloss_batch\u001b[0;34m(model, loss_func, xb, yb, opt)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Model"
      ],
      "metadata": {
        "id": "7mHyNmeLg1YG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ConvNet, self).__init__()\n",
        "    \n",
        "    #Conv1\n",
        "    self.conv1 = nn.Conv3d(in_channels = 1, \n",
        "                           out_channels = 32, \n",
        "                           kernel_size = (1,1,1), \n",
        "                           stride = (1,1,1)\n",
        "                           )\n",
        "    self.bn1 = nn.BatchNorm3d(32)\n",
        "    self.conv2 = nn.Conv3d(in_channels = 32, \n",
        "                           out_channels = 64, \n",
        "                           kernel_size = (7,7,7),\n",
        "                           stride = (2,2,2)\n",
        "                           )\n",
        "    self.bn2 = nn.BatchNorm3d(64)\n",
        "    self.conv3 = nn.Conv3d(in_channels = 64, \n",
        "                           out_channels = 64, \n",
        "                           kernel_size = (3,3,3),\n",
        "                           stride = (2,2,2)\n",
        "                           )\n",
        "    self.bn3 = nn.BatchNorm3d(64)\n",
        "    self.conv4 = nn.Conv3d(in_channels = 64, \n",
        "                           out_channels = 128, \n",
        "                           kernel_size = (3,3,3),\n",
        "                           stride = (2,2,2)\n",
        "                           )\n",
        "    self.bn4 = nn.BatchNorm3d(128) \n",
        "    self.pool1 = nn.AdaptiveAvgPool3d((1,1,1)) #Global Average Pool, takes the average over last two dimensions to flatten \n",
        "  \n",
        "                                                             \n",
        "    # Fully connected layer\n",
        "    self.fc1 = nn.Linear(128,64) # need to find out the size where AdaptiveAvgPool \n",
        "    self.fc2 = nn.Linear(64, 2) # left with 2 for the two classes                     \n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, xb):\n",
        "    xb = self.bn1(F.relu(self.conv1((xb))))\n",
        "    xb = self.bn2(F.relu(self.conv2((xb)))) # Takes a long time\n",
        "    xb = self.bn3(F.relu(self.conv3((xb))))\n",
        "    xb = self.bn4(F.relu(self.conv4((xb))))\n",
        "    xb = self.pool1(xb)\n",
        "    xb = xb.view(xb.shape[:2])\n",
        "    xb = self.fc1(xb)\n",
        "    xb = self.fc2(xb)\n",
        "    return xb      \n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "UDBGSQdyh3Wj"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Get model\n",
        "model = ConvNet()\n",
        "model = model.to(device)\n",
        "print(\"First model training on GPU\")\n",
        "print(model)\n",
        "\n",
        "# Initialize other parameters\n",
        "epochs = 3 #120\n",
        "learning_rate = 0.0002\n",
        "loss_func = F.cross_entropy\n",
        "opt = torch.optim.Adam(model.parameters(), lr = learning_rate)#, momentum = 0.9) #or ADAM/ momentum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jsQY9v-f2fO",
        "outputId": "20a2bb5f-d7d9-4a9d-842a-7157ec530989"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First model training on GPU\n",
            "ConvNet(\n",
            "  (conv1): Conv3d(1, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
            "  (bn1): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv2): Conv3d(32, 64, kernel_size=(7, 7, 7), stride=(2, 2, 2))\n",
            "  (bn2): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv3): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(2, 2, 2))\n",
            "  (bn3): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv4): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2))\n",
            "  (bn4): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (pool1): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
            "  (fc1): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (fc2): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Model\n",
        "for epoch in range(1, 1+epochs):\n",
        "  model.train()\n",
        "  i = 1\n",
        "  print('epoch', epoch)\n",
        "  for xb, yb in train_dl:\n",
        "    print('batch', i)\n",
        "    i = i+1\n",
        "\n",
        "    xb = xb.float()\n",
        "    pred = model(xb)\n",
        "    loss = loss_func(pred, yb)\n",
        "    print('Loss', loss)\n",
        "\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  print(epoch, valid_loss / len(valid_dl))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "id": "kTAp6i9XReq-",
        "outputId": "74c87678-47eb-48e0-83c3-a1f15685035d"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1\n",
            "batch 1\n",
            "Loss tensor(0.6779, grad_fn=<NllLossBackward0>)\n",
            "batch 2\n",
            "Loss tensor(0.6765, grad_fn=<NllLossBackward0>)\n",
            "batch 3\n",
            "Loss tensor(0.6590, grad_fn=<NllLossBackward0>)\n",
            "batch 4\n",
            "Loss tensor(0.7258, grad_fn=<NllLossBackward0>)\n",
            "batch 5\n",
            "Loss tensor(0.6836, grad_fn=<NllLossBackward0>)\n",
            "batch 6\n",
            "Loss tensor(0.7156, grad_fn=<NllLossBackward0>)\n",
            "batch 7\n",
            "Loss tensor(0.7124, grad_fn=<NllLossBackward0>)\n",
            "batch 8\n",
            "Loss tensor(0.7137, grad_fn=<NllLossBackward0>)\n",
            "batch 9\n",
            "Loss tensor(0.7218, grad_fn=<NllLossBackward0>)\n",
            "batch 10\n",
            "Loss tensor(0.6575, grad_fn=<NllLossBackward0>)\n",
            "batch 11\n",
            "Loss tensor(0.7254, grad_fn=<NllLossBackward0>)\n",
            "batch 12\n",
            "Loss tensor(0.6792, grad_fn=<NllLossBackward0>)\n",
            "batch 13\n",
            "Loss tensor(0.6600, grad_fn=<NllLossBackward0>)\n",
            "batch 14\n",
            "Loss tensor(0.7117, grad_fn=<NllLossBackward0>)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-9167bdc53f50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'valid_loss' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "kLzXUn9re04K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Get model\n",
        "model = ConvNet()\n",
        "model = model.to(device)\n",
        "print(\"First model training on GPU\")\n",
        "print(model)\n",
        "\n",
        "# Initialize other parameters\n",
        "epochs = 5 #120\n",
        "learning_rate = 0.001\n",
        "criterion = nn.CrossEntropyLoss(reduction=\"mean\")\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)#, momentum = 0.9) #or ADAM/ momentum\n",
        "\n",
        "fit(epochs, model, )"
      ],
      "metadata": {
        "id": "mmYyDPNiRr0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(out, yb):\n",
        "    preds = torch.argmax(out, dim=1)\n",
        "    return (preds == yb).float().mean()"
      ],
      "metadata": {
        "id": "ZB3v2KOBhYUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_stats = {\n",
        "    'train': [],\n",
        "    'val': []\n",
        "  }\n",
        "\n",
        "print(accuracy_stats)\n",
        "\n",
        "loss_stats = {\n",
        "    'train': [],\n",
        "    'val': []\n",
        "    }\n",
        "print(loss_stats)\n",
        "\n",
        "\n",
        "\n",
        "def train_test_model(epochs, model, opt, train_dl, test_dl, device):\n",
        "  for epoch in range(1, epochs+1):\n",
        "\n",
        "    # TRAINING *****************************************************************\n",
        "\n",
        "    train_epoch_loss = 0\n",
        "    train_epoch_acc = 0\n",
        "\n",
        "    # set model in training mode \n",
        "    model.train()\n",
        "    print('\\nEpoch$ : %d'%epoch)\n",
        "    for xb, yb in tqdm(train_dl):\n",
        "      xb = xb.to(device)\n",
        "      y_train_batch = y_train_batch.to(device) \n",
        "\n",
        "      #print(x_train_batch.shape)\n",
        "\n",
        "      # sets gradients to 0 to prevent interference with previous epoch\n",
        "      optimizer.zero_grad()\n",
        "    \n",
        "      # Forward pass through NN\n",
        "      y_pred = model(xb)#.to(float)\n",
        "      train_loss = criterion(y_pred, yb)\n",
        "      train_acc = accuracy(y_pred, yb)\n",
        "\n",
        "      # Backward pass, updating weights\n",
        "      train_loss.backward()\n",
        "      opt.step()\n",
        "\n",
        "      # Statistics\n",
        "      train_epoch_loss += train_loss.item()\n",
        "      train_epoch_acc += train_acc.item()\n",
        "\n",
        "\n",
        "    # VALIDATION****************************************************************   \n",
        "    \n",
        "    with torch.set_grad_enabled(False):\n",
        "      val_epoch_loss = 0\n",
        "      val_epoch_acc = 0\n",
        "\n",
        "      model.eval()\n",
        "      for x_val_batch, y_val_batch in tqdm(validate_loader):\n",
        "      \n",
        "        x_val_batch =  x_val_batch.to(device)#.to(float)\n",
        "        y_val_batch = y_val_batch.to(device)\n",
        "            \n",
        "        # Forward pass\n",
        "        y_val_pred = model(x_val_batch)#.to(float)   \n",
        "        val_loss = criterion(y_val_pred, y_val_batch)\n",
        "        val_acc = accuracy(y_val_pred, y_val_batch)\n",
        "            \n",
        "        val_epoch_loss += val_loss.item()\n",
        "        val_epoch_acc += val_acc.item()\n",
        "\n",
        "    # Prevent plateauing validation loss \n",
        "    #scheduler.step(val_epoch_loss/len(validate_loader))\n",
        "\n",
        "        \n",
        "    loss_stats['train'].append(train_epoch_loss/len(train_loader))\n",
        "    loss_stats['val'].append(val_epoch_loss/len(validate_loader))\n",
        "    accuracy_stats['train'].append(train_epoch_acc/len(train_loader))\n",
        "    accuracy_stats['val'].append(val_epoch_acc/len(validate_loader))\n",
        "                              \n",
        "    \n",
        "    print(f'Epoch {epoch+0:03}: Train Loss: {train_epoch_loss/len(train_loader):.5f} | Val Loss: {val_epoch_loss/len(validate_loader):.5f}') \n",
        "    print(f'Train Acc: {train_epoch_acc/len(train_loader):.3f} | Val Acc: {val_epoch_acc/len(validate_loader):.3f}')\n",
        "\n",
        "      \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def accuracy(y_pred, y_test):\n",
        "#   # Calculating model accuracy at each epoch \n",
        "#   y_pred_softmax = torch.log_softmax(y_pred, dim = 1)\n",
        "#   _, y_pred_prob = torch.max(y_pred_softmax, dim = 1)\n",
        "#   correct_pred = (y_pred_prob == y_test).float()\n",
        "#   acc = correct_pred.sum() / len(correct_pred)\n",
        "#   acc = torch.round(acc * 100)\n",
        "\n",
        "#   return acc\n",
        "\n",
        "\n",
        "\n",
        "     \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#   train_val_model(epochs)"
      ],
      "metadata": {
        "id": "66UUnIhJevn9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "b3a47033-0952-40ae-9e71-de9a70f52a8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': [], 'val': []}\n",
            "{'train': [], 'val': []}\n",
            "\n",
            "Epoch$ : 1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-178-16282038068e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m   \u001b[0mtrain_val_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-178-16282038068e>\u001b[0m in \u001b[0;36mtrain_val_model\u001b[0;34m(epochs)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nEpoch$ : %d'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx_train_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_batch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m       \u001b[0mx_train_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#(float).to(device) # for GPU support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0my_train_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = ConvNet()\n",
        "\n",
        "epochs = 5 #120\n",
        "learning_rate = 0.001\n",
        "loss_func = F.cross_entropy\n",
        "opt = torch.optim.Adam(model.parameters(), lr = learning_rate)#, momentum = 0.9) #or ADAM/ momentum\n",
        "\n",
        "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
        "    loss = loss_func(model(xb), yb)\n",
        "\n",
        "    if opt is not None:\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        opt.zero_grad()\n",
        "\n",
        "    return loss.item(), len(xb)\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    for xb, yb in train_dl:\n",
        "      xb = xb.float()\n",
        "      loss_batch(model, loss_func, xb, yb, opt)\n",
        "\n",
        "    model.eval()\n",
        "    # with torch.no_grad():\n",
        "    #     losses, nums = zip(\n",
        "    #         *[loss_batch(model, loss_func, xb, yb) for xb, yb in valid_dl]\n",
        "    #     )\n",
        "    # val_loss = np.sum(np.multiply(losses, nums)) / np.sum(nums)\n",
        "\n",
        "    # print(epoch, val_loss)"
      ],
      "metadata": {
        "id": "6EBUAl0ceO4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rJGJUdiBKtnl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2FyCVpQ6KtkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "HKkISUtQKthJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OCzbRyCiKteE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RwmpLfSLKtaZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}