{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yecatstevir/teambrainiac/blob/main/source/DL/preprocess_to_aws.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssBXn9lhG9du"
      },
      "source": [
        "# Preprocessing Pipeline\n",
        "## For 3D Convolutional Neural Network on Group Brain fMRI\n",
        "\n",
        "This notebook turns fMRI brain images from flat matlab files into 4D tensor objects for CNN training.\n",
        "\n",
        "To start:\n",
        "- Mount Google Colab, clone fMRI repository locally, and create path to AWS for saving and loading\n",
        "- Select desired brain images by subject id, splitting into train, validation, and test sets\n",
        "\n",
        "Pipeline flow for each batch of images:\n",
        "- Import desired brain images from AWS paths from data_path_dict\n",
        "- Drop brain images that are unlabeled\n",
        "- Mask out the brain, normalize the pixel values, and cast into 4D space\n",
        "- Aggregate images into tensor-compatible objects for model use\n",
        "- Upload tensor object dictionary of labels and images to AWS S3\n",
        "        \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjOMBc929Vpp"
      },
      "source": [
        "## Mount Colab in Google Drive and Import Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWlULVj79acH"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFAZrJHL9a6f"
      },
      "outputs": [],
      "source": [
        "# Clone the entire repo.\n",
        "!git clone -l -s https://github.com/yecatstevir/teambrainiac.git\n",
        "\n",
        "# Change directory into cloned repo DL folder\n",
        "%cd teambrainiac/source/DL\n",
        "\n",
        "# !ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjLsWQoxMLZt"
      },
      "source": [
        "### Load path_config.py to access AWS credentials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJ5q46JZMTPc"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giqBoQS0MWFi"
      },
      "source": [
        "## Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAhQQqpqMYNT"
      },
      "outputs": [],
      "source": [
        "# Possible Missing Packages\n",
        "!pip install boto3\n",
        "# !pip install sklearn\n",
        "!pip install nilearn\n",
        "\n",
        "# General Library Imports\n",
        "import re\n",
        "import scipy.io\n",
        "import scipy\n",
        "import sklearn\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "from nilearn.signal import clean\n",
        "import pandas as pd\n",
        "import boto3\n",
        "import tempfile\n",
        "import tqdm\n",
        "import random\n",
        "from path_config import mat_path\n",
        "from botocore.exceptions import ClientError\n",
        "from collections import defaultdict\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# From Local Directory\n",
        "from access_data_dl import *\n",
        "from process_dl import *\n",
        "\n",
        "# Pytroch Libraries\n",
        "import torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYG_gfx5sDHc"
      },
      "source": [
        "## Import Dictionary of Paths to Flat Matlab Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWYt1ci4sTgk"
      },
      "outputs": [],
      "source": [
        "# Open path dictionary file to get subject ids\n",
        "path = \"../data/data_path_dictionary.pkl\"\n",
        "data_path_dict = open_pickle(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Select Images of Choice to Run Through Pipeline"
      ],
      "metadata": {
        "id": "ALMNU3uyeY_G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "'''\n",
        "This is to keep hardcoded dictionary of shuffled ids to make sure train, validation, and test sets stay seperate\n",
        "\n",
        "If the size you are looking for is not here, use the function at the bottom of the cell.\n",
        "\n",
        "five_ids = {'test': ['10017_08894'],\n",
        " 'train': ['10008_09924', '10016_09694', '10004_08693'],\n",
        " 'val': ['10009_08848']}\n",
        "\n",
        "\n",
        "all_ids = {'test': ['10056_09615','10035_08847','10038_09063','30044_10095','10080_09931',\n",
        "  '30017_09567','10084_10188','10069_09785','10061_09308','10039_08941','10004_08693'],\n",
        " 'train': ['30012_09102','30027_09638','30033_09776','10033_08871',\n",
        "  '10057_10124','30036_09758','10036_09800','10047_09030','30025_09402','10043_09222','30024_09398','10066_09687',\n",
        "  '10023_09126','10050_09079','10022_08854','30053_10112','10016_09694','30011_09170','30035_09836','10065_09587',\n",
        "  '10045_08968','10008_09924','10060_09359','30014_09352','10018_08907','30020_09236','10027_09455','10046_09216',\n",
        "  '30045_10182','30038_09967','10037_09903','30009_09227','10021_08839','30004_08965','30008_08981','10053_09018'],\n",
        " 'val': ['10034_08879','10042_08990','10009_08848','10017_08894','30026_09430']}\n",
        "\n",
        "'''\n",
        "print()\n",
        "# generate_train_val_test_dict(subject_id_partition, train_val_test_proportion=[0.7,0.8,1])"
      ],
      "metadata": {
        "id": "0h9zKcPHxYNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = {'test': ['10056_09615','10035_08847','10038_09063','30044_10095','10080_09931',\n",
        "  '30017_09567','10084_10188','10069_09785','10061_09308','10039_08941','10004_08693'],\n",
        " 'train': ['30012_09102','30027_09638','30033_09776','10033_08871',\n",
        "  '10057_10124','30036_09758','10036_09800','10047_09030','30025_09402','10043_09222','30024_09398','10066_09687',\n",
        "  '10023_09126','10050_09079','10022_08854','30053_10112','10016_09694','30011_09170','30035_09836','10065_09587',\n",
        "  '10045_08968','10008_09924','10060_09359','30014_09352','10018_08907','30020_09236','10027_09455','10046_09216',\n",
        "  '30045_10182','30038_09967','10037_09903','30009_09227','10021_08839','30004_08965','30008_08981','10053_09018'],\n",
        " 'val': ['10034_08879','10042_08990','10009_08848','10017_08894','30026_09430']}"
      ],
      "metadata": {
        "id": "EPkooXXlV7uP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create Reasonably Sized Files \n",
        "~10 GB maximum"
      ],
      "metadata": {
        "id": "IPafdcDEeg1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To avoid an insane amount of RAM, we will take the all_ids dictionary and split it up into chunks\n",
        "# Val is reasonably sized, but train and test are not. We will split train into 4 pieces and test into 2.\n",
        "train_len = len(all_ids['train'])\n",
        "all_ids['train_1'] = all_ids['train'][:int(train_len/4)]\n",
        "all_ids['train_2'] = all_ids['train'][int(train_len/4):int(train_len/2)]\n",
        "all_ids['train_3'] = all_ids['train'][int(train_len/2):int(3*train_len/4)]\n",
        "all_ids['train_4'] = all_ids['train'][int(3*train_len/4):]\n",
        "\n",
        "del all_ids['train']\n",
        "\n",
        "test_len = len(all_ids['test'])\n",
        "all_ids['test_1'] = all_ids['test'][:int(test_len/2)]\n",
        "all_ids['test_2'] = all_ids['test'][int(test_len/2):]"
      ],
      "metadata": {
        "id": "Uahte02KyKFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Select Hyperparameters and Run Pipeline\n",
        "Note: The code for these functions is in our local directory. The filename and path is [access_data_dl.py](https://github.com/yecatstevir/teambrainiac/blob/main/source/DL/access_data_dl.py)"
      ],
      "metadata": {
        "id": "P0bkPY3ieq08"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_A6VDQpeGZW"
      },
      "outputs": [],
      "source": [
        "# labels_mask_binary hyperparameters\n",
        "label_type='rt_labels'\n",
        "\n",
        "# load_subjects_by_id parameters\n",
        "n_subjects = len(data_path_dict['subject_ID'])\n",
        "runs = [2,3]\n",
        "\n",
        "# get_mask parameters and pull mask\n",
        "image_mask_type = 'mask'\n",
        "mask_ind = 0\n",
        "brain_mask = get_mask(image_mask_type, data_path_dict, mask_ind)\n",
        "\n",
        "# mask_normalize_runs_reshape_4d parameters\n",
        "scaler = 'standard'\n",
        "\n",
        "\n",
        "for subject_partition in ['test_1', 'test_2']:#all_ids.keys():\n",
        "  subject_ids = all_ids[subject_partition] \n",
        "\n",
        "  image_label_mask, image_labels = labels_mask_binary(data_path_dict, label_type='rt_labels')\n",
        "\n",
        "  initial_subject_data = load_subjects_by_id(data_path_dict, subject_ids, image_label_mask, image_labels, label_type, runs)\n",
        "\n",
        "  subjects_reshaped = mask_normalize_runs_reshape_4d(initial_subject_data, brain_mask, scaler)\n",
        "\n",
        "  partition_dictionary = train_test_aggregation_group(subjects_reshaped, runs, subject_ids)\n",
        "\n",
        "  s3_upload(partition_dictionary, 'dl/partition_%s.pkl'%subject_partition, 'pickle')\n",
        "  print('Partition', subject_partition, 'complete.')\n",
        "  print()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hUD-Sq6cWCJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Healthy Pipeline Example Output of Uploading 11 Subjects\n",
        "\n",
        "Subject ids loaded.\n",
        "\n",
        "Adding subjects to dictionary.\n",
        "\n",
        "11it [02:13, 12.12s/it]\n",
        "\n",
        "Completed Subject 1\n",
        "\n",
        "Completed Subject 2\n",
        "\n",
        "Completed Subject 3\n",
        "\n",
        "Completed Subject 4\n",
        "\n",
        "Completed Subject 5\n",
        "\n",
        "Completed Subject 6\n",
        "\n",
        "Completed Subject 7\n",
        "\n",
        "Completed Subject 8\n",
        "\n",
        "Completed Subject 9\n",
        "\n",
        "Completed Subject 10\n",
        "\n",
        "Completed Subject 11\n",
        "\n",
        "upload complete for dl/partition_test.pkl\n",
        "\n",
        "Partition test complete."
      ],
      "metadata": {
        "id": "5cN6XD2DgTH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VJ9Az5Tvget-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "name": "preprocess_to_aws.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}